{"title":"Natural language processing","markdown":{"yaml":{"title":"Natural language processing"},"headingText":"Overview","containsRefs":false,"markdown":"\n\n\nThe purpose of this course is to develop students who can:\n\n- engage in thoughtful, ethical, critique of Natural Language Processing (NLP);\n- work productively to implement existing NLP methods; and\n- use NLP to contribute to our understanding of the world.\n\nStudents are expected to develop:\n\n- an understanding of NLP and its place in the world;\n- exceptional written and verbal communication skills; and \n- contribute in some small way to our understanding of something related to NLP.\n\n\n## Content\n\n- Week 1: *Essentials I*\n  - Silge, Julia & David Robinson, 2020, *Text Mining with R*, Chapters 1-4: https://www.tidytextmining.com.\n  - Hovy, Dirk and Shannon L. Spruit, 2016, 'The Social Impact of Natural Language Processing', *Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics*, pp. 591–598,https://aclweb.org/anthology/P16-2096.pdf.\n  - Prabhumoye, Shrimai, Elijah Mayfield, and Alan W Black, 2019, 'Principled Frameworks for Evaluating Ethics in NLP Systems', *Proceedings of the 2019 Workshop on Widening NLP*, https://aclweb.org/anthology/W19-3637/.\n- Week 2: *Essentials II*\n  - Silge, Julia & David Robinson, 2020, *Text Mining with R*, Chapters 5-7: https://www.tidytextmining.com.\n  - Bolukbasi, Tolga, Kai-Wei Chang, James Y. Zou, Venkatesh Saligrama and Adam T. Kalai, 2016, 'Man is to Computer Programmer as Woman is to Homemaker? Debiasing Word Embeddings', *Advances in Neural Information Processing Systems*, 29 (NIPS 2016), http://papers.nips.cc/paper/6228-man-is-to-computer-programmer-as-woman-is-to-homemaker-d.\n  - Chang, Kai-Wei, Vinod Prabhakaran, and Vicente Ordonez, 2019, 'Bias and Fairness in Natural Language Processing', *Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)*: Tutorial Abstracts, https://aclweb.org/anthology/D19-2004/.\n- Week 3: *Essentials III*\n  - Silge, Julia & David Robinson, 2020, *Text Mining with R*, Chapters 8-9: https://www.tidytextmining.com.\n  - Hutchinson, Ben, Vinodkumar Prabhakaran, Emily Denton, Kellie Webster, Yu Zhong, and Stephen Denuyl, 2020, 'Social Biases in NLP Models as Barriers forPersons with Disabilities', *arXiv*, https://arxiv.org/abs/2005.00813.\n- Week 4: *NLP intermediate I* \n  - Hvitfeldt, Emil & Julia Silge, 2020, *Supervised Machine Learning for Text Analysis in R*, Chapters 1-3, https://smltar.com.\n  - Jurafsky, Dan, and James H. Martin, 2020, *Speech and Language Processing*, 3rd ed., Chapter 3, https://web.stanford.edu/~jurafsky/slp3/.\n  - Solaiman, Irene, Miles Brundage, Jack Clark, Amanda Askell, ArielHerbert-Voss, Jeff Wu, Alec Radford, Gretchen Krueger, Jong Wook Kim, SarahKreps, Miles McCain, Alex Newhouse, Jason Blazakis, Kris McGuffie, Jasmine Wang, 2019, 'Release Strategies and the Social Impacts of Language Models',*arXiv*, https://arxiv.org/abs/1908.09203.\n- Week 5: *NLP intermediate II* \n  - Hvitfeldt, Emil & Julia Silge, 2020, *Supervised Machine Learning for Text Analysis in R*, Chapters 4-6, https://smltar.com.\n  - Jurafsky, Dan, and James H. Martin, 2020, *Speech and Language Processing*, 3rd ed., Chapters 4 and 5, https://web.stanford.edu/~jurafsky/slp3/.\n  - Tatman, Rachel, 2020, 'What I Won't Build', *Widening NLP Workshop 2020*, Keynote address, 5 July, https://slideslive.com/38929585/what-i-wont-build and http://www.rctatman.com/talks/what-i-wont-build.\n- Week 6: *NLP intermediate III* \n  - (Read this one first) Jurafsky, Dan, and James H. Martin, 2020, *Speech and Language Processing*, 3rd ed., Chapters 6 and 7, https://web.stanford.edu/~jurafsky/slp3/.\n  - Hvitfeldt, Emil & Julia Silge, 2020, *Supervised Machine Learning for Text Analysis in R*, Chapters 7-9, https://smltar.com.\n  - Zhao, Jieyu, Tianlu Wang, Mark Yatskar, Vicente Ordonez and Kai-Wei Chang,2017, 'Men Also Like Shopping: Reducing Gender Bias Amplification using Corpus-level Constraints', *Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing*, pp. 2979–2989, https://aclweb.org/anthology/D17-1323.pdf.\n- Week 7: *Deep learning I* \n  - François Chollet, 2021, *Deep Learning with Python*, Chapters 1-4.\n- Week 8: *Deep learning II* \n  - François Chollet, 2021, *Deep Learning with Python*, Chapters 5-7.\n  - Bender, Emily M. and Koller, Alexander, 2020, 'Climbing towards NLU: On Meaning, Form, and Understanding in the Age of Data', *Proceedings of the 58th Annual Meeting of the \n- Week 9: *Deep learning III*\n  - François Chollet, 2021, *Deep Learning with Python*, Chapters 11 and 12.\n  - Anna Rogers, Isabelle Augenstein, 2020, 'What Can We Do to Improve Peer Review in NLP?', *arXiv*, 8 October, https://arxiv.org/abs/2010.03863.\n  - Jurafsky, Dan, and James H. Martin, 2020, *Speech and Language Processing*, 3rd ed., Chapters 8 and 9, https://web.stanford.edu/~jurafsky/slp3/.\nAssociation for Computational Linguistics*, pp. 5185--5198, https://www.aclweb.org/anthology/2020.acl-main.463\n- Week 10: *Transformers I*\n  - Karpathy, Andrej, 2022, *Neural Networks: Zero to Hero*, Both of 'The spelled-out intro...' videos.\n  - Alammar, Jay, 2018, 'The Illustrated Transformer', http://jalammar.github.io/illustrated-transformer/. \n  - Boykis, Vicky, *What are embeddings?*, https://vickiboykis.com/what_are_embeddings/\n  - Manning, Vaswani and Huang, 2019, 'Stanford CS224N: NLP with Deep Learning | Winter 2019 | Lecture 14 – Transformers and Self-Attention', https://www.youtube.com/watch?v=5vcj8kSwBCY&list=PLoROMvodv4rOhcuXMZkNm7j3fVwBBY42z&index=14&ab_channel=stanfordonline.\n  - Uszkoreit, Jakob, 2017, 'Transformer: A Novel Neural Network Architecture for Language Understanding', Google AI Blog, 31 August, https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html\n- Week 11: *Transformers II*\n  - Karpathy, Andrej, 2022, *Neural Networks: Zero to Hero*, The 'Building makemore...' videos\n  - Alammar, Jay, 2020, 'How GPT3 Works - Visualizations and Animations', 27 July, https://jalammar.github.io/how-gpt3-works-visualizations-animations/\n  - Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser and Illia Polosukhin, 2017, 'Attention Is All You Need', *arXiv*, http://arxiv.org/abs/1706.03762.\n  - Jacob Devlin and Ming-Wei Chang, 2018, 'Open Sourcing BERT: State-of-the-Art Pre-training for Natural Language Processing', 2 November, *Google AI Blog*, https://ai.googleblog.com/2018/11/open-sourcing-bert-state-of-art-pre.html.\n  - Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, 2018, 'BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding', *arXiv*, https://arxiv.org/abs/1810.04805.\n  - Rush, Alexander, 2018, 'The Annotated Transformer', https://nlp.seas.harvard.edu/2018/04/03/attention.html\n- Week 12: *Transformers III*\n  - Karpathy, Andrej, 2022, *Neural Networks: Zero to Hero*, The 'Let's build GPT: from scratch, in code, spelled out.'\n  - Tom B. Brown and Benjamin Mann and Nick Ryder and Melanie Subbiah and Jared Kaplan and Prafulla Dhariwal and Arvind Neelakantan and Pranav Shyam and Girish Sastry and Amanda Askell and Sandhini Agarwal and Ariel Herbert-Voss and Gretchen Krueger and Tom Henighan and Rewon Child and Aditya Ramesh and Daniel M. Ziegler and Jeffrey Wu and Clemens Winter and Christopher Hesse and Mark Chen and Eric Sigler and Mateusz Litwin and Scott Gray and Benjamin Chess and Jack Clark and Christopher Berner and Sam McCandlish and Alec Radford and Ilya Sutskever and Dario Amodei, 2020, 'Language Models are Few-Shot Learners', *arXiv*, https://arxiv.org/abs/2005.14165\n  - (Fun/horrifying) Hao, Karen, 2020, 'The messy, secretive reality behind OpenAI's bid to save the world', *MIT Review*, 17 February, https://www.technologyreview.com/2020/02/17/844721/ai-openai-moonshot-elon-musk-sam-altman-greg-brockman-messy-secretive-reality/.\n\n\n## Assessment\n\n### Notebook\n\n- Due date: Try to keep this updated weekly, on average, over the course of the term.\n- Task: Use Quarto to keep a notebook of what you read in the style of [this one](https://bayesf22-notebook.classes.andrewheiss.com/bayes-rules/02-chapter.html) by Andrew Heiss.\n- Weight: 25 per cent.\n \n### Paper #1\n\n- Due date: Thursday, noon, Week 1.\n- Task: [Donaldson Paper](https://tellingstorieswithdata.com/23-assessment.html#sec-paper-one)\n- Weight: 10 per cent.\n\n### Paper #2\n\n- Due date: Thursday, noon, Week 6.\n- Task: Write a paper applying what you are learning. \n- Weight: 25 per cent.\n\n### Final Paper\n\n- Due date: Thursday, noon, Week 12 + two weeks.\n- Task: Write a paper that involves transformers.\n- Weight: 40 per cent.\n\n","srcMarkdownNoYaml":"\n\n## Overview\n\nThe purpose of this course is to develop students who can:\n\n- engage in thoughtful, ethical, critique of Natural Language Processing (NLP);\n- work productively to implement existing NLP methods; and\n- use NLP to contribute to our understanding of the world.\n\nStudents are expected to develop:\n\n- an understanding of NLP and its place in the world;\n- exceptional written and verbal communication skills; and \n- contribute in some small way to our understanding of something related to NLP.\n\n\n## Content\n\n- Week 1: *Essentials I*\n  - Silge, Julia & David Robinson, 2020, *Text Mining with R*, Chapters 1-4: https://www.tidytextmining.com.\n  - Hovy, Dirk and Shannon L. Spruit, 2016, 'The Social Impact of Natural Language Processing', *Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics*, pp. 591–598,https://aclweb.org/anthology/P16-2096.pdf.\n  - Prabhumoye, Shrimai, Elijah Mayfield, and Alan W Black, 2019, 'Principled Frameworks for Evaluating Ethics in NLP Systems', *Proceedings of the 2019 Workshop on Widening NLP*, https://aclweb.org/anthology/W19-3637/.\n- Week 2: *Essentials II*\n  - Silge, Julia & David Robinson, 2020, *Text Mining with R*, Chapters 5-7: https://www.tidytextmining.com.\n  - Bolukbasi, Tolga, Kai-Wei Chang, James Y. Zou, Venkatesh Saligrama and Adam T. Kalai, 2016, 'Man is to Computer Programmer as Woman is to Homemaker? Debiasing Word Embeddings', *Advances in Neural Information Processing Systems*, 29 (NIPS 2016), http://papers.nips.cc/paper/6228-man-is-to-computer-programmer-as-woman-is-to-homemaker-d.\n  - Chang, Kai-Wei, Vinod Prabhakaran, and Vicente Ordonez, 2019, 'Bias and Fairness in Natural Language Processing', *Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)*: Tutorial Abstracts, https://aclweb.org/anthology/D19-2004/.\n- Week 3: *Essentials III*\n  - Silge, Julia & David Robinson, 2020, *Text Mining with R*, Chapters 8-9: https://www.tidytextmining.com.\n  - Hutchinson, Ben, Vinodkumar Prabhakaran, Emily Denton, Kellie Webster, Yu Zhong, and Stephen Denuyl, 2020, 'Social Biases in NLP Models as Barriers forPersons with Disabilities', *arXiv*, https://arxiv.org/abs/2005.00813.\n- Week 4: *NLP intermediate I* \n  - Hvitfeldt, Emil & Julia Silge, 2020, *Supervised Machine Learning for Text Analysis in R*, Chapters 1-3, https://smltar.com.\n  - Jurafsky, Dan, and James H. Martin, 2020, *Speech and Language Processing*, 3rd ed., Chapter 3, https://web.stanford.edu/~jurafsky/slp3/.\n  - Solaiman, Irene, Miles Brundage, Jack Clark, Amanda Askell, ArielHerbert-Voss, Jeff Wu, Alec Radford, Gretchen Krueger, Jong Wook Kim, SarahKreps, Miles McCain, Alex Newhouse, Jason Blazakis, Kris McGuffie, Jasmine Wang, 2019, 'Release Strategies and the Social Impacts of Language Models',*arXiv*, https://arxiv.org/abs/1908.09203.\n- Week 5: *NLP intermediate II* \n  - Hvitfeldt, Emil & Julia Silge, 2020, *Supervised Machine Learning for Text Analysis in R*, Chapters 4-6, https://smltar.com.\n  - Jurafsky, Dan, and James H. Martin, 2020, *Speech and Language Processing*, 3rd ed., Chapters 4 and 5, https://web.stanford.edu/~jurafsky/slp3/.\n  - Tatman, Rachel, 2020, 'What I Won't Build', *Widening NLP Workshop 2020*, Keynote address, 5 July, https://slideslive.com/38929585/what-i-wont-build and http://www.rctatman.com/talks/what-i-wont-build.\n- Week 6: *NLP intermediate III* \n  - (Read this one first) Jurafsky, Dan, and James H. Martin, 2020, *Speech and Language Processing*, 3rd ed., Chapters 6 and 7, https://web.stanford.edu/~jurafsky/slp3/.\n  - Hvitfeldt, Emil & Julia Silge, 2020, *Supervised Machine Learning for Text Analysis in R*, Chapters 7-9, https://smltar.com.\n  - Zhao, Jieyu, Tianlu Wang, Mark Yatskar, Vicente Ordonez and Kai-Wei Chang,2017, 'Men Also Like Shopping: Reducing Gender Bias Amplification using Corpus-level Constraints', *Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing*, pp. 2979–2989, https://aclweb.org/anthology/D17-1323.pdf.\n- Week 7: *Deep learning I* \n  - François Chollet, 2021, *Deep Learning with Python*, Chapters 1-4.\n- Week 8: *Deep learning II* \n  - François Chollet, 2021, *Deep Learning with Python*, Chapters 5-7.\n  - Bender, Emily M. and Koller, Alexander, 2020, 'Climbing towards NLU: On Meaning, Form, and Understanding in the Age of Data', *Proceedings of the 58th Annual Meeting of the \n- Week 9: *Deep learning III*\n  - François Chollet, 2021, *Deep Learning with Python*, Chapters 11 and 12.\n  - Anna Rogers, Isabelle Augenstein, 2020, 'What Can We Do to Improve Peer Review in NLP?', *arXiv*, 8 October, https://arxiv.org/abs/2010.03863.\n  - Jurafsky, Dan, and James H. Martin, 2020, *Speech and Language Processing*, 3rd ed., Chapters 8 and 9, https://web.stanford.edu/~jurafsky/slp3/.\nAssociation for Computational Linguistics*, pp. 5185--5198, https://www.aclweb.org/anthology/2020.acl-main.463\n- Week 10: *Transformers I*\n  - Karpathy, Andrej, 2022, *Neural Networks: Zero to Hero*, Both of 'The spelled-out intro...' videos.\n  - Alammar, Jay, 2018, 'The Illustrated Transformer', http://jalammar.github.io/illustrated-transformer/. \n  - Boykis, Vicky, *What are embeddings?*, https://vickiboykis.com/what_are_embeddings/\n  - Manning, Vaswani and Huang, 2019, 'Stanford CS224N: NLP with Deep Learning | Winter 2019 | Lecture 14 – Transformers and Self-Attention', https://www.youtube.com/watch?v=5vcj8kSwBCY&list=PLoROMvodv4rOhcuXMZkNm7j3fVwBBY42z&index=14&ab_channel=stanfordonline.\n  - Uszkoreit, Jakob, 2017, 'Transformer: A Novel Neural Network Architecture for Language Understanding', Google AI Blog, 31 August, https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html\n- Week 11: *Transformers II*\n  - Karpathy, Andrej, 2022, *Neural Networks: Zero to Hero*, The 'Building makemore...' videos\n  - Alammar, Jay, 2020, 'How GPT3 Works - Visualizations and Animations', 27 July, https://jalammar.github.io/how-gpt3-works-visualizations-animations/\n  - Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser and Illia Polosukhin, 2017, 'Attention Is All You Need', *arXiv*, http://arxiv.org/abs/1706.03762.\n  - Jacob Devlin and Ming-Wei Chang, 2018, 'Open Sourcing BERT: State-of-the-Art Pre-training for Natural Language Processing', 2 November, *Google AI Blog*, https://ai.googleblog.com/2018/11/open-sourcing-bert-state-of-art-pre.html.\n  - Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, 2018, 'BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding', *arXiv*, https://arxiv.org/abs/1810.04805.\n  - Rush, Alexander, 2018, 'The Annotated Transformer', https://nlp.seas.harvard.edu/2018/04/03/attention.html\n- Week 12: *Transformers III*\n  - Karpathy, Andrej, 2022, *Neural Networks: Zero to Hero*, The 'Let's build GPT: from scratch, in code, spelled out.'\n  - Tom B. Brown and Benjamin Mann and Nick Ryder and Melanie Subbiah and Jared Kaplan and Prafulla Dhariwal and Arvind Neelakantan and Pranav Shyam and Girish Sastry and Amanda Askell and Sandhini Agarwal and Ariel Herbert-Voss and Gretchen Krueger and Tom Henighan and Rewon Child and Aditya Ramesh and Daniel M. Ziegler and Jeffrey Wu and Clemens Winter and Christopher Hesse and Mark Chen and Eric Sigler and Mateusz Litwin and Scott Gray and Benjamin Chess and Jack Clark and Christopher Berner and Sam McCandlish and Alec Radford and Ilya Sutskever and Dario Amodei, 2020, 'Language Models are Few-Shot Learners', *arXiv*, https://arxiv.org/abs/2005.14165\n  - (Fun/horrifying) Hao, Karen, 2020, 'The messy, secretive reality behind OpenAI's bid to save the world', *MIT Review*, 17 February, https://www.technologyreview.com/2020/02/17/844721/ai-openai-moonshot-elon-musk-sam-altman-greg-brockman-messy-secretive-reality/.\n\n\n## Assessment\n\n### Notebook\n\n- Due date: Try to keep this updated weekly, on average, over the course of the term.\n- Task: Use Quarto to keep a notebook of what you read in the style of [this one](https://bayesf22-notebook.classes.andrewheiss.com/bayes-rules/02-chapter.html) by Andrew Heiss.\n- Weight: 25 per cent.\n \n### Paper #1\n\n- Due date: Thursday, noon, Week 1.\n- Task: [Donaldson Paper](https://tellingstorieswithdata.com/23-assessment.html#sec-paper-one)\n- Weight: 10 per cent.\n\n### Paper #2\n\n- Due date: Thursday, noon, Week 6.\n- Task: Write a paper applying what you are learning. \n- Weight: 25 per cent.\n\n### Final Paper\n\n- Due date: Thursday, noon, Week 12 + two weeks.\n- Task: Write a paper that involves transformers.\n- Weight: 40 per cent.\n\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"markdown"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true,"format-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["styles.css"],"toc":true,"output-file":"teaching-nlp.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Danger","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.3.266","theme":"cosmo","title":"Natural language processing"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}