[
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Rohan Alexander",
    "section": "",
    "text": "What data science can learn from the James Webb Space Telescope\n\n\n\n\n\nA talk at the Statistics and MachIne LEarning (SMILE) Journal Club. \n\n\n\n\n\nDec 17, 2021\n\n\nRohan Alexander\n\n\n\n\n\n\n\n\n\n\n\n\nOn the privilege of turning our world into data\n\n\n\n\n\nA talk at the ‘Young Irish Statisticians’ group. \n\n\n\n\n\nDec 16, 2021\n\n\nRohan Alexander\n\n\n\n\n\n\n\n\n\n\n\n\nThe average age of politicians in the Australian Federal Parliament (1901-2021)\n\n\n\n\n\nI examine the average age of politicians in the Australian Federal Parliament on a daily basis. Using a publicly available dataset I find that generally the Senate is older than the House of Representatives. The average age increased from Federation in 1901 through to 1949, when an expansion of the parliament’s size likely brought many new politicians. I am unable to explain a sustained decline that occurred during the 1970s. From the 1980s onward there has been a gradual aging of both houses. \n\n\n\n\n\nDec 2, 2021\n\n\nRohan Alexander\n\n\n\n\n\n\n\n\n\n\n\n\nReview of ‘Data Science: A First Introduction’\n\n\n\n\n\nA brief review of ‘Data Science: A First Introduction’ by Tiffany-Anne Timbers, Trevor Campbell, and Melissa Lee. \n\n\n\n\n\nNov 30, 2021\n\n\nRohan Alexander\n\n\n\n\n\n\n\n\n\n\n\n\nRemarks at Trinity College\n\n\n\n\n\nIntroductory remarks delivered at Trinity College High Table panel on data science at the University of Toronto on 19 October 2021. \n\n\n\n\n\nOct 19, 2021\n\n\nRohan Alexander\n\n\n\n\n\n\n\n\n\n\n\n\nIntroduction to Andrew Gelman\n\n\n\n\n\nIntroductory remarks about Andrew Gelman delivered at the University of Toronto Data Sciences Institute (DSI) launch on 17 September 2021. \n\n\n\n\n\nSep 17, 2021\n\n\nRohan Alexander\n\n\n\n\n\n\n\n\n\n\n\n\nTurning our world into data\n\n\n\n\n\nA talk delivered at the Harvard Biostatistics Data Science in Action Summer Camp, 7 July 2021, organised by Jesse Gronsbell. \n\n\n\n\n\nJul 6, 2021\n\n\nRohan Alexander\n\n\n\n\n\n\n\n\n\n\n\n\nOpportunities Provided by Open Data and Reproducibility\n\n\n\n\n\nSome thoughts on getting started with open data and reproducibility. A talk delivered at the University of Toronto, Stellar Stats Workshop, 28 May 2021, organised by Gwen Eadie & Josh Speagle. \n\n\n\n\n\nMay 23, 2021\n\n\nRohan Alexander\n\n\n\n\n\n\n\n\n\n\n\n\nOn work-life balance\n\n\n\n\n\nSome thoughts on work-life balance as a PhD student and junior faculty and how that changes over time. A talk delivered to the University of Toronto Faculty of Information ‘How to get a PhD’ session on work-life balance. \n\n\n\n\n\nMay 4, 2021\n\n\nRohan Alexander\n\n\n\n\n\n\n\n\n\n\n\n\nIn Appreciation of Greg Wilson\n\n\n\n\n\nAs he moves on from his role at R Studio Education, a few words of appreciation for Greg Wilson. \n\n\n\n\n\nMar 14, 2021\n\n\nRohan Alexander\n\n\n\n\n\n\n\n\n\n\n\n\nSaturday morning thoughts on grad school applications\n\n\n\n\n\nThere’s a huge amount of luck involved. Worry as much, if not more, about letters and your personal statement as you do about your GPA. Have a clear reason for wanting to go to the school/program that you’re applying for and communicate that throughout your application. \n\n\n\n\n\nMar 13, 2021\n\n\nRohan Alexander\n\n\n\n\n\n\n\n\n\n\n\n\nCleaning Hansard: The pay’s not great but the work is hard\n\n\n\n\n\nCleaning the Australian Hansard is mind-numbing, annoying and time-consuming, but someone has to do it. \n\n\n\n\n\nAug 13, 2018\n\n\nRohan Alexander\n\n\n\n\n\n\n\n\n\n\n\n\nThe SQL Is Never As Good As The Original\n\n\n\n\n\nSQL is a popular way of working with data. Advanced users probably do a lot with it alone, but even just having a working knowledge of SQL has increased the number of datasets that I can get data from to then analyse with other tools such as R or Python. You can use SQL within RStudio if you want. The following are a few notes to help future-Rohan when he needs to use SQL. A worked example with a sample of the Hansard data will be included in a future post. \n\n\n\n\n\nJul 28, 2018\n\n\nRohan Alexander\n\n\n\n\n\n\n\n\n\n\n\n\nMapping the 2016 Australian Election Polling Place Results\n\n\n\n\n\nThe note that follows introduces Australia’s political system, and then details the process of downloading and merging first-preference votes by polling place, and then plotting it on an interactive map. \n\n\n\n\n\nJul 18, 2017\n\n\nRohan Alexander\n\n\n\n\n\n\n\n\n\n\n\n\nTrump, Revisited\n\n\n\n\n\nDonald Trump is an improved politician, but it’s unlikely to be enough. He has harnessed fervent anti-Clinton sentiment amongst Republicans. But he does not have time to build the coalitions usually needed to win a US presidential election. \n\n\n\n\n\nJul 17, 2016\n\n\nRohan Alexander\n\n\n\n\n\n\n\n\n\n\n\n\nNotes and Photos From Iowa\n\n\n\n\n\nBernie Sanders seems quite reasonable for a revolutionary. An energetic man of 74, he spoke for an hour in Perry, Iowa, to a room of 300 from only a few lines of handwritten notes, and then fielded half an hour of questions. He does not have the same aura that surrounded, then, Senator Obama in his own Iowa battle with, then, Senator Clinton in 2008 say those who saw both. Instead, Sanders has preternatural calm. \n\n\n\n\n\nJan 14, 2016\n\n\nRohan Alexander\n\n\n\n\n\n\n\n\n\n\n\n\nPrepare For Future Economic Crises Now\n\n\n\n\n\nFew policymakers were prepared for the financial crisis of 2007-08. Until it hit, their focus was on more obvious threats to the economy, instead of such an unexpected event. Could this be because planning for unexpected economic events is not the explicit responsibility of any particular policy-maker? If so, this has to change. \n\n\n\n\n\nAug 31, 2015\n\n\nRohan Alexander\n\n\n\n\n\n\n\n\n\n\n\n\nPlastic Policies\n\n\n\n\n\nThere is broad agreement that Australian plastic bag consumption should be reduced. To this end, recent South Australian legislation has banned certain types of plastic bags. But other states wishing to reduce their plastic bag consumption may find a tax rather than a ban the more appropriate policy instrument. \n\n\n\n\n\nMay 25, 2009\n\n\nRohan Alexander, Flavio Menezes\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "teaching-ethics_and_ds.html",
    "href": "teaching-ethics_and_ds.html",
    "title": "Ethics and data science",
    "section": "",
    "text": "This is a course focused on the intersection of ethics and data science. The purpose of this course is to develop students who can:\n\nengage in thoughtful, ethical, critique of data science, its antecedents, current state, and likely evolution; and\nwork productively to implement existing data science methods, as well as contribute to the creation of novel methods or applications.\n\nEach week students will read relevant papers and books, engage with them through discussion with each other and the instructor, learn related technical skills, and bring this together through on-going assessment. All students are expected to be prepared for each week’s discussion through completing the readings and technical requirements. A specific student will act as the lead for each week.\nThe course outline is available here.\n\n\n\n\nCan I audit this course? Sure, but the concept of auditing doesn’t make sense for this course. There are no lectures, we have weekly discussions. You’re welcome to come along to the discussions if you’d like but please do the readings first.\n\n\n\n\nThanks to the following who helped develop this course: A Mahfouz, Assel Kushkeyeva, Irene Duah-Kessie, Ke-li Chiu, Paul Hodgetts, and Thomas Rosenthal."
  },
  {
    "objectID": "teaching-ethics_and_ds.html#preamble",
    "href": "teaching-ethics_and_ds.html#preamble",
    "title": "Ethics and data science",
    "section": "",
    "text": "This is a course focused on the intersection of ethics and data science. The purpose of this course is to develop students who can:\n\nengage in thoughtful, ethical, critique of data science, its antecedents, current state, and likely evolution; and\nwork productively to implement existing data science methods, as well as contribute to the creation of novel methods or applications.\n\nEach week students will read relevant papers and books, engage with them through discussion with each other and the instructor, learn related technical skills, and bring this together through on-going assessment. All students are expected to be prepared for each week’s discussion through completing the readings and technical requirements. A specific student will act as the lead for each week.\nThe course outline is available here.\n\n\n\n\nCan I audit this course? Sure, but the concept of auditing doesn’t make sense for this course. There are no lectures, we have weekly discussions. You’re welcome to come along to the discussions if you’d like but please do the readings first.\n\n\n\n\nThanks to the following who helped develop this course: A Mahfouz, Assel Kushkeyeva, Irene Duah-Kessie, Ke-li Chiu, Paul Hodgetts, and Thomas Rosenthal."
  },
  {
    "objectID": "teaching-ethics_and_ds.html#content",
    "href": "teaching-ethics_and_ds.html#content",
    "title": "Ethics and data science",
    "section": "Content",
    "text": "Content\n\nWeek 1 - General\n\n\nEthical\nCore:\n\nCantwell Smith, Brian, 2019, The Promise of AI, MIT Press, Chapters 10-12.\nHealy, Kieran, 2020, ‘The Kitchen Counter Observatory’, 21 May, https://kieranhealy.org/blog/archives/2020/05/21/the-kitchen-counter-observatory/.\nKeyes, Os, 2019, ‘Counting the Countless’, Real Life, 8 April, https://reallifemag.com/counting-the-countless/.\nO’Neil, Cathy, 2016, Weapons of Math Destruction, Crown Books, Chapters 1, 3, and 4.\n\nAdditional (pick two):\n\nGreen, Ben, 2018, ‘Data Science as Political Action: Grounding Data Science in a Politics of Justice’, arXiv, 1811.03435, https://arxiv.org/abs/1811.03435.\nIrving, Geoffrey, and Amanda Askell, 2019, ‘AI Safety Needs Social Scientists’, Distill, 19 February, https://distill.pub/2019/safety-needs-social-scientists/.\nLeslie, David, 2020, ‘Tackling COVID-19 through Responsible AI Innovation: Five Steps in the Right Direction’, Harvard Data Science Review, 5 June, https://hdsr.mitpress.mit.edu/pub/as1p81um.\nSuresh, Harini, and John V. Guttag, 2019, ‘A Framework for Understanding Unintended Consequences of Machine Learning’, arXiv, 1901.10002, https://arxiv.org/abs/1901.10002.\nRaji, Inioluwa Deborah, 2020, ‘The Discomfort of Death Counts: Mourning through the Distorted Lens of Reported COVID-19 Death Data’, Patterns, https://doi.org/10.1016/j.patter.2020.100066\n\n\n\nTechnical\n\nReview ‘Essentials’ from Telling Stories With Data, if necessary.\n\n\n\n\nWeek 2 - Data and consent\n\n\nEthical\nCore:\n\nBoykis, Vicki, 2019, ‘Neural nets are just people all the way down’, 16 October, https://vicki.substack.com/p/neural-nets-are-just-people-all-the.\nCrawford, Kate, and Vladan Joler, 2018, ‘Anatomy of an AI System: The Amazon Echo As An Anatomical Map of Human Labor, Data and Planetary Resources’, AI Now Institute and Share Lab, 7 September, https://anatomyof.ai.\nCrawford, Kate, 2020, ‘Kate Crawford: Anatomy of AI’, Lecture, University of New South Wales, 28 January, https://youtu.be/uM7gqPnmDDc.\nKitchin, Rob, 2014, The data revolution: Big data, open data, data infrastructures and their consequences, Sage, Introduction, Chapters 8, and 10. (Access via U of T library).\n\nAdditional (pick two):\n\nBergis Jules, Ed Summers and Vernon Mitchell, 2018, ‘Documenting The Now: Ethical Considerations for Archiving Social Media Content Generated by Contemporary Social Movements: Challenges, Opportunities, and Recommendations’, White Paper, DocNow, https://www.docnow.io/docs/docnow-whitepaper-2018.pdf.\nBoyd, Danah, and Kate Crawford, 2012, ‘Critical Questions for Big Data’, Information, Communication & Society, 15(55), 662-679, https://www.microsoft.com/en-us/research/wp-content/uploads/2012/05/CriticalQuestionsForBigDataICS.pdf.\nDenton, Emily, Alex Hanna, Razvan Amironesei, Andrew Smart, Hilary Nicole, Morgan Klaus Scheuerman, 2020, ‘Bringing the People Back In: Contesting Benchmark Machine Learning Datasets’, arXiv, 14 July, https://arxiv.org/abs/2007.07399.\nEubanks, Virginia, 2019, ‘Automating Inequality: How high-tech tools profile, police and punish the poor’, Lecture, University of Toronto, 12 March, https://www.youtube.com/watch?v=g1ZZZ1QLXOI.\nLemov, Rebecca, 2016, ‘Big data is people!’, Aeon, 16 June, https://aeon.co/essays/why-big-data-is-actually-small-personal-and-very-human.\nOffice of Oversight and Investigations Majority Staff, 2013, ‘A Review of the Data Broker Industry: Collection, Use, and Sale of Consumer Data for Marketing Purposes’, Staff Report for Chairman Rockefeller, 18 December, United States Senate, Committee on Commerce, Science and Transportation, https://www.commerce.senate.gov/services/files/0d2b3642-6221-4888-a631-08f2f255b577.\nRadin, Joanna, 2017, ‘“Digital Natives”: How Medical and Indigenous Histories Matter for Big Data’, Osiris, 32 (1), 43-64, https://www.journals.uchicago.edu/doi/pdf/10.1086/693853.\nSnowberg, Erik and Leeat Yariv, 2018, ‘Testing The Waters: Behavior Across Participant Pools’, NBER Working Paper, No. 24781, http://www.nber.org/papers/w24781.\n\n\n\nTechnical\n\nReview ‘Hunt, gather and farm’ from Telling Stories With Data, if necessary.\n\n\n\n\nWeek 3 - Women and gender\n\nEthical\nCore:\n\nD’Ignazio, Catherine, and Lauren F. Klein, 2020, Data Feminism, MIT Press.\nGebru, Timnit, 2020, ‘Race and Gender’, The Oxford Handbook of Ethics of AI, Chapter 13, Oxford University Press.\n\nAdditional (pick two):\n\nBorgerson, Janet L., 2007, ‘On the Harmony of Feminist Ethics and Business Ethics’, Business and Society Review, 112 (4):477-509.\n\nD’Ignazio, Catherine, and Lauren F. Klein, ‘Feminist data visualization’, Workshop on Visualization for the Digital Humanities (VIS4DH), Baltimore. IEEE. 2016.\nHill, Kashmir, 2017, ‘What Happens When You Tell the Internet You’re Pregnant’, Jezebel, 27 July, https://jezebel.com/what-happens-when-you-tell-the-internet-youre-pregnant-1794398989.\nKeyes, Os, 2018, ‘The misgendering machines: Trans/HCI implications of automatic gender recognition’, Proceedings of the ACM on Human-Computer Interaction, 2(CSCW), 1-22, https://dl.acm.org/doi/pdf/10.1145/3274357.\nQuintin, Cooper, 2017, ‘Pregnancy Panopticon’, DEFCON 25, https://www.eff.org/files/2017/07/27/the_pregnancy_panopticon.pdf.\nWoods, Heather Suzanne, 2018, ‘Asking more of Siri and Alexa: feminine persona in service of surveillance capitalism’, Critical Studies in Media Communication, 35.4, pp. 334-349.\n\n\n\nTechnical\nReview the essentials of Bayesian models by going through McElreath, 2020, Statistical Rethinking, 2nd Edition, (at least chapters 1, 2, 4, 7, 9, 11, 12, and 13) to address any shortcomings.\n\n\n\nWeek 4 - Race\nTom Davidson, Assistant Professor, Sociology, Rutgers University: https://youtu.be/YDmxMn2Doq0.\n\nEthical\nCore:\n\nDavidson, Thomas, Debasmita Bhattacharya, and Ingmar Weber, 2019, ‘Racial bias in hate speech and abusive language detection datasets’, arXiv, https://arxiv.org/abs/1905.12516.\nNoble, Safiya Umoja, 2018, Algorithms of Oppression: How Search Engines Reinforce Racism, NYU Press, Chapter 2.\n\nAdditional (pick two):\n\nBuolamwini, Joy and Timnit Gebru, 2018, ‘Gender Shades: Intersectional Accuracy Disparities in Commercial Gender Classification’, Proceedings of Machine Learning Research Conference on Fairness, Accountability, and Transparency, 81: pp. 1–15, http://proceedings.mlr.press/v81/buolamwini18a/buolamwini18a.pdf\nKwet, Michael, 2019, ‘Digital colonialism: US empire and the new imperialism in the Global South’, Race & Class 60.4, 3-26.\nScheuerman, M. K., Wade, K., Lustig, C., and Brubaker, J. R., 2020, ‘How We’ve Taught Algorithms to See Identity: Constructing Race and Gender in Image Databases for Facial Analysis’, Proceedings of the ACM on Human-Computer Interaction, 4(CSCW1), 1-35.\nZiad Obermeyer, Brian Powers, Christine Vogeli, and Sendhil Mullainathan, 2019, ‘Dissecting racial bias in an algorithm used to manage the health of populations’, Science, Vol. 366, Issue 6464, pp. 447-453, DOI: 10.1126/science.aax2342, https://science.sciencemag.org/content/366/6464/447/tab-pdf\n\n\n\nTechnical\n\nPick a project from The Markup’s Show Your Work section (https://themarkup.org/series/show-your-work) and reproduce it, writing your own code. You may pick whatever language you are comfortable in.\n\n\n\n\nWeek 5 - Natural Language Processing\n\n\nEthical\nCore:\n\nBender, Emily M., Angelina McMillan-Major, Timnit Gebru and Shmargaret Shmitchell, 2021, ‘On the Dangers of Stochastic Parrots: Can Language Models Be Too Big?’, https://faculty.washington.edu/ebender/papers/Stochastic_Parrots.pdf\nHovy, Dirk and Shannon L. Spruit, 2016, ‘The Social Impact of Natural Language Processing’, Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pp. 591–598, https://aclweb.org/anthology/P16-2096.pdf.\nPrabhumoye, Shrimai, Elijah Mayfield, and Alan W Black, 2019, ‘Principled Frameworks for Evaluating Ethics in NLP Systems’, Proceedings of the 2019 Workshop on Widening NLP, https://aclweb.org/anthology/W19-3637/.\n\nAdditional (pick two):\n\nBolukbasi, Tolga, Kai-Wei Chang, James Y. Zou, Venkatesh Saligrama and Adam T. Kalai, 2016, ‘Man is to Computer Programmer as Woman is to Homemaker? Debiasing Word Embeddings’, Advances in Neural Information Processing Systems 29 (NIPS 2016), http://papers.nips.cc/paper/6228-man-is-to-computer-programmer-as-woman-is-to-homemaker-d. \nChang, Kai-Wei, Vinod Prabhakaran, and Vicente Ordonez, 2019, ‘Bias and Fairness in Natural Language Processing’, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP): Tutorial Abstracts, https://aclweb.org/anthology/D19-2004/.\nHutchinson, Ben, Vinodkumar Prabhakaran, Emily Denton, Kellie Webster, Yu Zhong, and Stephen Denuyl, 2020, ‘Social Biases in NLP Models as Barriers for Persons with Disabilities’, arXiv, https://arxiv.org/abs/2005.00813.\nSolaiman, Irene, Miles Brundage, Jack Clark, Amanda Askell, Ariel Herbert-Voss, Jeff Wu, Alec Radford, Gretchen Krueger, Jong Wook Kim, Sarah Kreps, Miles McCain, Alex Newhouse, Jason Blazakis, Kris McGuffie, Jasmine Wang, 2019, ‘Release Strategies and the Social Impacts of Language Models’, arXiv, https://arxiv.org/abs/1908.09203.\nTatman, Rachel, 2020, ‘What I Won’t Build’, Widening NLP Workshop 2020, Keynote address, 5 July, https://slideslive.com/38929585/what-i-wont-build and http://www.rctatman.com/talks/what-i-wont-build.\nZhao, Jieyu, Tianlu Wang, Mark Yatskar, Vicente Ordonez and Kai-Wei Chang, 2017, ‘Men Also Like Shopping: Reducing Gender Bias Amplification using Corpus-level Constraints’, Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pp. 2979–2989, https://aclweb.org/anthology/D17-1323.pdf.\n(Optional/fun/horrifying) Hao, Karen, 2020, ‘The messy, secretive reality behind OpenAI’s bid to save the world’, MIT Review, 17 February, https://www.technologyreview.com/2020/02/17/844721/ai-openai-moonshot-elon-musk-sam-altman-greg-brockman-messy-secretive-reality/.\n\n\n\nTechnical\n\nImplement a NLP model via Hugging Face or Spacy, depending on your language preference.\n\n\n\n\nWeek 6 - AI Ethics\nShion Guha, Assistant Professor, University of Toronto, will join the discussion briefly this week.\n\nEthical\nCore:\n\nBrundage, Miles, Shahar Avin, Jack Clark, Helen Toner, Peter Eckersley, Ben Garfinkel, Allan Dafoe, Paul Scharre, Thomas Zeitzoff, Bobby Filar, Hyrum Anderson, Heather Roff, Gregory C. Allen, Jacob Steinhardt, Carrick Flynn, Seán Ó hÉigeartaigh, Simon Beard, Haydn Belfield, Sebastian Farquhar, Clare Lyle, Rebecca Crootof, Owain Evans, Michael Page, Joanna Bryson, Roman Yampolskiy, Dario Amodei, 2019, ‘The Malicious Use of Artificial Intelligence: Forecasting, Prevention, and Mitigation’, arXiv, https://arxiv.org/abs/1802.07228.\nJobin, A., Ienca, M., and Vayena, E, 2019, ‘The global landscape of AI ethics guidelines’, Nature Machine Intelligence, 1(9), pp. 389-399. https://www.nature.com/articles/s42256-019-0088-2.\n\nAdditional (pick two):\n\nAustralian Human Rights Commission, 2019, ‘Human Rights and Technology Discussion Paper’, December, https://tech.humanrights.gov.au/sites/default/files/2019-12/TechRights_2019_DiscussionPaper.pdf.\nCrawford, Kate, Amba Kak and Jason Schultz, 2020, ‘Submission to the Australian Human Rights Commission Human Rights & Technology Discussion Paper’, AI Now Institute, New York University, 13 March.\nKaplan, Andreas, Michael Haenlein, 2019, ‘Siri, Siri, in my hand: Who’s the fairest in the land? On the interpretations, illustrations, and implications of artificial intelligence’, Business Horizons, Volume 62, Issue 1, pp. 15-25.\nLeslie, David, 2019, ‘Understanding Artificial Intelligence Ethics and Safety: A guide for the responsible design and implementation of AI systems in the public sector’, Alan Turing Institute.\nLuciano, Floridi, and Cowls Josh, 2019, ‘A Unified Framework of Five Principles for AI in Society’, Harvard Data Science Review, 1 July, https://hdsr.mitpress.mit.edu/pub/l0jsh9d1.\nPaglioni, Vincent, 2015, ‘The Ethics of Intelligent Machines’, Investment Management Consultants Association, https://investmentsandwealth.org/getattachment/f3614756-1e1d-49c7-a201-29dbc22d8fbf/IWM15NovDec-EthicsIntelligentMachines.pdf\nWinfield, Alan F., Katina Michael, Jeremy Pitt, Vanessa Evers, 2019, ‘Machine Ethics: the Design and Governance of Ethical AI and Autonomous Systems’, Proceeding of IEEE, Volume 107, Issue 3, pp. 509-517.\nSam Corbett-Davies and Sharad Goel, 2018, ‘The Measure and Mismeasure of Fairness: A Critical Review of Fair Machine Learning’, 14 August, https://arxiv.org/pdf/1808.00023.pdf.\nInareh Mehrabi, Fred Morstatter, Nripsuta Saxena, Kristina Lerman, And Aram Galstyan, 2019, ‘A Survey on Bias and Fairness in Machine Learning’, https://arxiv.org/pdf/1908.09635.pdf.\nIrene Y. Chen, Fredrik D. Johansson, David Sontag, 2018, ‘Why Is My Classifier Discriminatory?’, https://arxiv.org/pdf/1805.12002.pdf.\n\n\n\nTechnical\nUse RASA (https://rasa.com/) to build a chatbot, or OpenAI’s GPT-2 or GPT-3 to generate text.\n\n\n\nWeek 7 - Privacy\nJonathan A. Obar, Assistant Professor, Department of Communication Studies, York University, will be invited to join the discussion briefly this week.\n\nEthical\nCore:\n\nHyunghoon Cho, Daphne Ippolito, Yun William Yu, 2020, ‘Contact Tracing Mobile Apps for COVID-19: Privacy Considerations and Related Trade-offs’, arXiv, https://arxiv.org/abs/2003.11511.\nObar, Jonathan A. and Oeldorf-Hirsch, Anne, 2018, ‘The Biggest Lie on the Internet: Ignoring the Privacy Policies and Terms of Service Policies of Social Networking Services’ TPRC 44: The 44th Research Conference on Communication, Information and Internet Policy, http://dx.doi.org/10.2139/ssrn.2757465.\n\nAdditional (pick two):\n\nBlumberg, Andrew J. and Peter Eskersley, 2009, ‘On Locational Privacy, and How to Avoid Losing it Forever’, https://www.eff.org/wp/locational-privacy.\nde Montjoye, Yves-Alexandre, César A. Hidalgo, Michel Verleysen, and Vincent D. Blondel, 2013, ‘Unique in the Crowd: The privacy bounds of human mobility’, Scientific Reports, vol 3, https://doi.org/10.1038/srep01376.\nObar, Jonathan A., and Anne Oeldorf-Hirsch, 2018, ‘The clickwrap: A political economic mechanism for manufacturing consent on social media’, Social Media+ Society, 4.3, 2056305118784770\nSolove, Daniel J, 2007, ‘“I’ve Got Nothing to Hide” and Other Misunderstandings of Privacy’, San Diego Law Review, Vol. 44, p. 745-772.\nZimmeck, Sebastian, Story, Peter, Smullen, Daniel, Ravichander, Abhilasha, Wang, Ziqi, Reidenberg, Joel, Cameron Russell, N., & Sadeh, Norman, 2019, ‘MAPS: Scaling Privacy Compliance Analysis to a Million Apps’, Proceedings on Privacy Enhancing Technologies, Volume 3, pp. 66-86.\nZimmer, Michael, Priya Kumar, Jessica Vitak, Yuting Liao and Katie Chamberlain Kritikos, 2018, “‘There’s nothing really they can do with this information’: unpacking how users manage privacy boundaries for personal fitness information”, Information, Communication & Society, Vol 23, Issue 7, pp. 1020-1037.\n\n\n\nTechnical\n\nFind or generate a dataset, then implement differential privacy on it. Examine and discuss the results.\nOberski, Daniel, and Frauke Kreuter, 2020, ‘Differential Privacy and Social Science: An Urgent Puzzle’, Harvard Data Science Review, https://doi.org/10.1162/99608f92.63a22079.\nRubinstein, Benjamin I. P. and Francesco Alda, 2017, ‘diffpriv: An R Package for Easy Differential Privacy’, Journal of Machine Learning Research, 18, pp. 1-5.\n\n\n\n\nWeek 8 - Images/video with particular reference to facial recognition\nJeffrey Knockel, Research Associate, Citizen Lab, University of Toronto, will be invited to join the discussion briefly this week.\n\nEthical\nCore:\n\nBuolamwini, Joy, Vicente Ordóñez, Jamie Morgenstern, and Learned-Miller, Erik, 2020, ‘Facial recognition technologies: A primer’, Algorithmic Justice League, 29 May.\nInioluwa Deborah Raji, Timnit Gebru, Margaret Mitchell, Joy Buolamwini, Joonseok Lee, and Emily Denton, 2020, ‘Saving Face: Investigating the Ethical Concerns of Facial Recognition Auditing’, In Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society (AIES ’20). Association for Computing Machinery, New York, NY, USA, 145–151. DOI:https://doi.org/10.1145/3375627.3375820.\n\nAdditional (pick two):\n\nHill, Kashmir, 2020, ‘Wrongfully Accused by an Algorithm’, New York Times, 24 June, https://www.nytimes.com/2020/06/24/technology/facial-recognition-arrest.html.\nHill, Kashmir, 2020, ‘The Secretive Company That Might End Privacy as We Know It’, New York Times, 18 January, https://www.nytimes.com/2020/01/18/technology/clearview-privacy-facial-recognition.html.\nKnockel, Jeffrey, and Ruohan Xiong, 2019, ‘(Can’t) Picture This 2: An Analysis of WeChat’s Realtime Image Filtering in Chats’, Citizen Lab, 15 July, https://citizenlab.ca/2019/07/cant-picture-this-2-an-analysis-of-wechats-realtime-image-filtering-in-chats/.\nLearned-Miller, Erik, Vicente Ordóñez, Jamie Morgenstern, and Joy Buolamwini, 2020, ‘Facial recognition technologies in the wild: A call for a federal office’, Algorithmic Justice League, 29 May,\n\n\n\nTechnical\n\nChollet, Francois, and J. J. Allaire, 2018, Deep Learning with R, Chapter 5 ‘Deep learning for computer vision’.\n\n\n\n\nWeek 9 - Corporate Surveillance\n\nEthical\nCore:\n\nZuboff, Shoshana, 2019, The Age of Surveillance Capitalism, and watch related interview: https://www.youtube.com/watch?v=hIXhnWUmMvw\nZuboff, Shoshana, 2019, ‘Written Testimony Submitted to The International Grand Committee on Big Data, Privacy, and Democracy’, 28 May, Ottawa, https://www.ourcommons.ca/Content/Committee/421/ETHI/Brief/BR10573725/br-external/ZuboffShoshana-e.pdf and watch related video https://youtu.be/6N2kJNwGgUg?t=4869.\n\nAdditional (pick two):\n\nBennett Cyphers and Gennie Gebhart, “Behind One-Way Mirror: A Deep Dive Into the Technology of Corporate Surveillance”, https://www.eff.org/files/2019/12/11/behind_the_one-way_mirror-a_deep_dive_into_the_technology_of_corporate_surveillance.pdf\nMarczak, Bill and John Scott-Railton, 2020, ‘Move Fast and Roll Your Own Crypto: A Quick Look at the Confidentiality of Zoom Meetings’, Citizen Lab, 3 April, https://citizenlab.ca/2020/04/move-fast-roll-your-own-crypto-a-quick-look-at-the-confidentiality-of-zoom-meetings/.\nParsons, Christopher, Andrew Hilts, and Masashi Crete-Nishihata, 2017, ‘Approaching Access: A comparative analysis of company responses to data access requests in Canada’, Citizen Lab, Research Brief No. 106. Available at: https://citizenlab.ca/wp-content/uploads/2018/02/approaching_access.pdf.\n(Optional/fun) Duhigg, Charles, 2012, ‘How Companies Learn Your Secrets’, New York Times, 19 February, https://www.nytimes.com/2012/02/19/magazine/shopping-habits.html\n\n\n\nTechnical\n\nCreate a datasheet or model card for an open source dataset or model.\nGebru, Timnit, Jamie Morgenstern, Briana Vecchione, Jennifer Wortman Vaughan, Hanna Wallach, Hal Daumé III, and Kate Crawford, 2018, ‘Datasheets for Datasets’, arXiv, https://arxiv.org/abs/1803.09010\nMitchell, Margaret, Simone Wu, Andrew Zaldivar, Parker Barnes, Lucy Vasserman, Ben Hutchinson, Elena Spitzer, Inioluwa Deborah Raji and Timnit Gebru, 2019, ‘Model Cards for Model Reporting’, FAT ’19: Proceedings of the Conference on Fairness, Accountability, and Transparency, pp. 220–229 https://doi.org/10.1145/3287560.3287596.\n\n\n\n\nWeek 10 - Privacy and surveillance in Canada and other countries\nLisa Austin, Professor, Law, University of Toronto, will be invited to join the discussion briefly this week.\n\nEthical\nCore:\n\nKhoo, Cynthia, Kate Robertson, and Ronald Deibert, 2019, ‘Installing Fear: A Canadian Legal and Policy Analysis of Using, Developing, and Selling Smartphone Spyware and Stalkerware Applications,’ Citizen Lab, Research Report No. 120, University of Toronto, June, https://tspace.library.utoronto.ca/bitstream/1807/96321/1/stalkerware-legal.pdf.\nObar, Jonathan A., 2017, ‘Keeping Internet Users in the Know or in the Dark? The Data Privacy Transparency of Canadian Internet Carriers: A Third Report’, IXMaps, https://ixmaps.ca/docs/DataPrivacyTransparencyCanadianCarriers-2017.pdf\nRuan, Lotus, Crete-Nishihata, Masashi, Knockel, Jeffrey, Xiong, Ruohan and Dalek, Jakub, 2020, ‘The Intermingling of State and Private Companies: Analysing Censorship of the 19th National Communist Party Congress on WeChat,’ The China Quarterly, pp. 1–30. doi: 10.1017/S0305741020000491.\n\nAdditional (pick two):\n\nAustin, Lisa, and David Lie, 2019, ‘Safe Sharing Sites’, New York University Law Review, Vol. 94, No. 4, pp. 581 - 623.\nKnockel, Jeffrey, Christopher Parsons, Lotus Ruan, Ruohan Xiong, Jedidiah Crandall, and Ron Deibert, 2020, ‘We Chat, They Watch: How International Users Unwittingly Build up WeChat’s Chinese Censorship Apparatus,’ Citizen Lab, Research Report No. 127, University of Toronto, May, https://tspace.library.utoronto.ca/bitstream/1807/101395/1/Report%23127–wechattheywatch-web.pdf.\nObar, Jonathan A., and Brenda McPhail, 2018, ‘Preventing Big Data Discrimination in Canada: Addressing design, consent and sovereignty challenges’, Centre for International Governance Innovation (CIGI), https://www.cigionline.org/articles/preventing-big-data-discrimination-canada-addressing-design-consent-and-sovereignty.\nParsons, Christopher, Adam Molnar, Jakub Dalek, Jeffrey Knockel, Miles Kenyon, Bennett Haselton, Cynthia Khoo, and Ron Deibert, 2019, ‘The Predator in Your Pocket: A Multidisciplinary Assessment of the Stalkerware Application Industry,’ Citizen Lab, Research Report, No. 119, University of Toronto, June, https://tspace.library.utoronto.ca/bitstream/1807/96320/1/stalkerware-holistic.pdf.\nScott, James C., 1998, Seeing Like a State: How Certain Schemes to Improve the Human Condition Have Failed.\nVarious, ‘GDPR Checklist’, https://gdpr.eu/checklist/.\nVarious, ‘Summary of privacy laws in Canada’, Office of the Privacy Commissioner, https://www.priv.gc.ca/en/privacy-topics/privacy-laws-in-canada/02_05_d_15/.\n\n\n\nTechnical\n\nTBD based on student interest.\n\n\n\n\nWeek 11 - Algorithmic decision-making\nJamie Duncan, Junior Policy Analyst, Artificial Intelligence Hub, Innovation, Science and Economic Development Canada, will be invited to join the discussion briefly this week.\n\nEthical\nCore:\n\nSpiegelhalter, David, 2020, ‘Should We Trust Algorithms?’, Harvard Data Science Review, 31 January, https://doi.org/10.1162/99608f92.cb91a35a.\nMolnar, Petra and Lex Gill, 2018, ‘Bots at the Gate: A Human Rights Analysis of Automated Decision-Making in Canada’s Immigration and Refugee System,’ Citizen Lab and International Human Rights Program, Faculty of Law, University of Toronto, Research Report No. 114, University of Toronto, September, https://citizenlab.ca/wp-content/uploads/2018/09/IHRP-Automated-Systems-Report-Web-V2.pdf.\n\nAdditional (pick two):\n\nDe-Arteaga, Maria, Riccardo Fogliato, and Alexandra Chouldechova, ‘A Case for Humans-in-the-Loop: Decisions in the Presence of Erroneous Algorithmic Scores’. https://arxiv.org/abs/2002.08035.\nMitchell, Shira, Eric Potash, Solon Barocas, Alexander D’Amour, and Kristian Lum, 2018, ‘Prediction-Based Decisions and Fairness: A Catalogue of Choices, Assumptions, and Definitions’, arXiv, 1811.07867. https://arxiv.org/abs/1811.07867.\nRudin, Cynthia, Caroline Wang, and Beau Coker, ‘The Age of Secrecy and Unfairness in Recidivism Prediction’, Harvard Data Science Review, https://hdsr.mitpress.mit.edu/pub/7z10o269.\nSuresh, Harini, Natalie Lao, and Ilaria Liccardi, ‘Misplaced Trust: Measuring the Interference of Machine Learning in Human Decision-Making’, https://arxiv.org/pdf/2005.10960.pdf\nThe Joint Council for the Welfare of Immigrants v Secretary of State for the Home Department, 2020, ‘Grounds of Challenge’ and ‘Response’, available: https://www.foxglove.org.uk/news/c6tv7i7om2jze5pxs409k3oo3dyel0 and background here: https://www.theguardian.com/uk-news/2020/aug/04/home-office-to-scrap-racist-algorithm-for-uk-visa-applicants.\n\n\n\nTechnical\n\nMcElreath says that researchers use point estimates to describe posterior distributions, not to support particular decisions. But this isn’t always viable. Using a post from the Stan Case Study (https://mc-stan.org/users/documentation/case-studies.html) as a guide, please develop a Bayesian hierarchical model in Stan. Please post-process your model to support/recommend a decision, and justify your choices.\n\n\n\n\nWeek 12 - History of ethical concerns broadly, and domain-specific ethical practices\n\nEthical (Please pick two areas.)\nMedicine:\n\nParker, Michael, J A Muir Gray, 2001, ‘What is the role of clinical ethics support in the era of e-medicine?’, Journal of Medical Ethics, 27 suppl I:i33–i35 https://jme.bmj.com/content/medethics/27/suppl_1/i33.full.pdf\nChancellor, S., Baumer, E. P., & De Choudhury, M. (2019). Who is the” Human” in Human-Centered Machine Learning: The Case of Predicting Mental Health from Social Media. Proceedings of the ACM on Human-Computer Interaction, 3(CSCW), 1-32. https://doi.org/10.1145/3359249\nVayena, Effy, and Alessandro Blasimme, 2020, ‘The Ethics of AI in Biomedical Research, Medicine and Public Health’, The Oxford Handbook of Ethics of AI, Chapter 37, Oxford University Press.\n\nEngineering:\n\nDavis, Michael, 1991, ‘Thinking Like an Engineer: The Place of a Code of Ethics in the Practice of a Profession’, https://www.jstor.org/stable/pdf/2265293.pdf?refreqid=excelsior%3A94aaba1458bc97cf0563cf7d16861188\nMichaelson, Christopher, 2014, ‘The Competition for the Tallest Skyscraper: Implications for Global Ethics and Economics’, CTBUH Journal, Issue IV, https://www.jstor.org/stable/pdf/24192831.pdf?ab_segments=0%252Fbasic_SYC-5187%252Ftest&refreqid=excelsior%3A9bf439c8785e93d009d8e42608e6b425\nMillar, Jason, 2020, ‘Engineering’, The Oxford Handbook of Ethics of AI, Chapter 23, Oxford University Press.\n\nStatistics:\n\nWells, Martin, 2020, ‘Statistics’, The Oxford Handbook of Ethics of AI, Chapter 26, Oxford University Press.\n\nLaw:\n\nAngwin, Julia, Jeff Larson, Surya Mattu and Lauren Kirchner, 2016, ‘Machine Bias’, ProPublica, https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing\nEubanks, Virginia, 2014, ‘How Big Data Could Undo Our Civil-Rights Law’, https://prospect.org/justice/big-data-undo-civil-rights-laws/\nSurden, Harry, 2020, ‘Law: Basic Questions’, The Oxford Handbook of Ethics of AI, Chapter 38, Oxford University Press.\n\nFinances:\n\nGeslevich Packin, Nizan, Yafit Lev Aretz, 2015, ‘Big Data and Social Netbanks: Are You Ready to Replace Your Bank?’, https://papers.ssrn.com/sol3/papers.cfm?abstract_id=2567135\n\nEducation:\n\nMayfield, E., Madaio, M., Prabhumoye, S., Gerritsen, D., McLaughlin, B., Dixon-Román, E., & Black, A. W. (2019, August). Equity beyond bias in language technologies for education. In Proceedings of the Fourteenth Workshop on Innovative Use of NLP for Building Educational Applications (pp. 444-460). https://doi.org/10.1177/2053951720913064\nRubel, A., & Jones, K. M. (2016). Student privacy in learning analytics: An information ethics perspective. The information society, 32(2), 143-159. https://doi.org/10.1080/01972243.2016.1130502\nZeide, Elana, 2020, ‘Education’, The Oxford Handbook of Ethics of AI, Chapter 42, Oxford University Press.\n\nGeneral non-computational:\n\nSelbst, A. D., Boyd, D., Friedler, S. A., Venkatasubramanian, S., & Vertesi, J. (2019, January). Fairness and abstraction in sociotechnical systems. In Proceedings of the Conference on Fairness, Accountability, and Transparency (pp. 59-68). https://dl.acm.org/doi/pdf/10.1145/3287560.3287598\nEarlier calls for ethics in computing\nAgre, Philip E., 1997, ‘Towards a critical technical practice: Lessons learned from trying to reform AI’, Social science, technical systems, and cooperative work: Beyond the great divide, Ed. by Geoffrey C. Bowker, Susan Leigh Star, Will Turner, and Les Gasser. Mahwah, NJ: Lawrence Erlbaum Associates, pp. 131–158. URL: https://web.archive.org/web/20040203070641/http://polaris.gseis.ucla.edu/pagre/critical.html.\nFriedman, Batya, and Helen Nissenbaum, 1996, ‘Bias in computer systems’, ACM Trans. Inf. Syst, 14, 3 (July 1996), 330–347. DOI: https://doi.org/10.1145/230538.230561.\n\n\n\nTechnical\n\nTBD based on student interest."
  },
  {
    "objectID": "teaching-ethics_and_ds.html#assessment",
    "href": "teaching-ethics_and_ds.html#assessment",
    "title": "Ethics and data science",
    "section": "Assessment",
    "text": "Assessment\n\nFour ethical and technical blog posts (30 per cent)\nOver the course of the term, you are expected to submit four blog posts that each comprise two aspects: 1) ethical and 2) technical. These two aspects should be related to each other. You must submit all four, but only your best three blog posts will count, that is each blog post will account for 10 per cent of your overall mark.\nFor the first aspect (ethical), you are expected to write a moderate length discussion (think a paper of about two to three pages), of a reading, or set of readings, that we have covered over the past two weeks. Strong submissions will not limit themselves to reviewing a reading but will draw in larger issues and detail their own point of view.\nFor the second aspect (technical), you are expected to implement some small related technical aspect of what we have covered in the past two weeks. For instance, if we covered natural language processing then you may critically review a paper, and put together a chat bot.\nTo be clear, these two aspects should be related, tied together, and should be in the one blog post.\nYou should submit your blog post by emailing me a link to the relevant blog post on your website.\nThe proposed specific list of deadlines is:\n\nBlog post 1: midnight, Sunday 24 January, 2021.\nBlog post 2: midnight, Sunday 7 February, 2021.\nBlog post 3: midnight, Sunday 7 March, 2021.\nBlog post 4: midnight, Sunday 21 March, 2021.\n\nIn Week 1 we will discuss how these dates fit in with your other commitments and finalise them at that point.\nThe instructor will make the marking guide available at least a week before the submission deadline.\n\n\nPaper 1 (30 per cent)\n\nTask\nPlease gather and clean data on UofT salaries from the Sunshine List. Then conduct a Bayesian statistical analysis of your dataset to discuss the extent to which gender has an effect on salary. Finally please prepare a paper of around 10 pages that discusses your analysis. (Hint: gender is not explicitly part of the Sunshine List, you will need to grapple with what to do.)\n\n\nBackground\nYou should make appropriate use of appendices for additional and supporting material, and thoroughly reference your paper, but neither the appendices nor the reference list count toward your page limit. Your paper should have an appropriate title, author, date, abstract, and introduction. It should document and overview your dataset. It should clearly specify your model, and then discuss the results of your analysis and any weaknesses. Your analysis should be fully reproducible, with code and data hosted on a public GitHub repo. Additionally, you should include a thorough discussion of ethical considerations relevant to your analysis. This would likely take at least three pages, but you are welcome to write as much as is needed to make the points you would like to make. Likely the best way to do this is to include a brief overview of the ethical points that you would like to discuss, and then include the rest of the discussion in an appendix. I understand that Bayesian analysis may be new to you. I will assist you with putting together the model, but it is up to you to understand and interpret the output.\n\n\nSubmission\nTo submit your paper you should email me a link to a public GitHub repo. That repo should contain your paper in PDF format and all supporting code and data. Please send this email by midnight, Sunday, 14 February, 2021. Please do not make any changes to the repo after this. I will make the marking guide available at least a week before the submission deadline.\n\n\n\nPaper 2 (40 per cent)\n\nTask\nIn consultation with me, please identify an appropriate research question and data source that, like the requirement for Paper 1, combines both ethical and technical aspects. Please prepare a paper that represents your best attempt to answer this question and shows off your ability to engage in thoughtful, ethical, critique. The paper should be as long as necessary, although all extraneous material should be included in appendices. The expectation is that this paper should make an original contribution, that could be published in an academic journal.\n\n\nBackground\nPlease see the background provided for Paper 1, as this applies for Paper 2 as well.\n\n\nSubmission\nYou must send the email with the GitHub link to me by midnight, Sunday, 23 April, 2020. Please do not make any changes to the repo after this. I will make the marking guide available at least a week before the submission deadline. No extensions are possible because of deadlines for instructors to submit grades."
  },
  {
    "objectID": "teaching-history_of_statistics.html",
    "href": "teaching-history_of_statistics.html",
    "title": "History of statistics and data sciences",
    "section": "",
    "text": "Statistics and the data sciences have a long and robust history. Understanding that history provides students with a better appreciation for the methods that they are applying today.\nOften students are taught, say, linear regression in such a way that they come to believe that statisticians simply stumbled upon it one day. In fact, the idea of combining different observations in this way, took the work of decades and even centuries to come to terms with. Understanding the history of statistics and data sciences, more generally, provides a more solid foundation for applying those skills today. We are interested in why certain methods were developed, and became popular, and the circumstances under which this occurred because that provides us with a nuanced knowledge of when we should apply them ourselves.\nWe study history because we want to understand how our predecessors solved their problems. That means understanding, not just what they did, but the circumstances in which they did it, and the choices they faced. That knowledge allows us to better solve our own problems. At the very least, it helps us to avoid repeating mistakes; and, if fully accomplished, can even allow to improve our own approaches.\nThe history of statistics and the data sciences is one of greatness, and we will cover that extensively. But it also one in which that greatness was sometimes developed for abhorrent purposes, and there were many contributors, actual or potential, who were overlooked. We will cover these aspects too.\nThe hope is that having taken this course, you will understand what you have been studying in statistics and the data sciences with fresh eyes, and bring this deeper appreciation with you throughout the rest of your career."
  },
  {
    "objectID": "teaching-history_of_statistics.html#overview",
    "href": "teaching-history_of_statistics.html#overview",
    "title": "History of statistics and data sciences",
    "section": "",
    "text": "Statistics and the data sciences have a long and robust history. Understanding that history provides students with a better appreciation for the methods that they are applying today.\nOften students are taught, say, linear regression in such a way that they come to believe that statisticians simply stumbled upon it one day. In fact, the idea of combining different observations in this way, took the work of decades and even centuries to come to terms with. Understanding the history of statistics and data sciences, more generally, provides a more solid foundation for applying those skills today. We are interested in why certain methods were developed, and became popular, and the circumstances under which this occurred because that provides us with a nuanced knowledge of when we should apply them ourselves.\nWe study history because we want to understand how our predecessors solved their problems. That means understanding, not just what they did, but the circumstances in which they did it, and the choices they faced. That knowledge allows us to better solve our own problems. At the very least, it helps us to avoid repeating mistakes; and, if fully accomplished, can even allow to improve our own approaches.\nThe history of statistics and the data sciences is one of greatness, and we will cover that extensively. But it also one in which that greatness was sometimes developed for abhorrent purposes, and there were many contributors, actual or potential, who were overlooked. We will cover these aspects too.\nThe hope is that having taken this course, you will understand what you have been studying in statistics and the data sciences with fresh eyes, and bring this deeper appreciation with you throughout the rest of your career."
  },
  {
    "objectID": "teaching-history_of_statistics.html#learning-objectives",
    "href": "teaching-history_of_statistics.html#learning-objectives",
    "title": "History of statistics and data sciences",
    "section": "Learning objectives",
    "text": "Learning objectives\nThe purpose of the course is to develop an appreciation of history of statistics and the data sciences to such an extent so as to provide a firmer foundation for your conduct of applied statistics and data science. By the end of the course, you should be able to:\n\nEngage critically with ideas and readings in the history of statistics and data sciences.\nConduct research in the history of data science and statistics.\nWrite and present your research.\nUnderstand why the methods and approaches developed when they did, and the circumstances under which they developed.\nAppreciate that much of the statistical machinery that we use today was developed with respect to eugenics.\nRespectfully identify strengths and weaknesses in the work of others.\nReflect effectively on your own learning and professional development."
  },
  {
    "objectID": "teaching-history_of_statistics.html#prerequisites",
    "href": "teaching-history_of_statistics.html#prerequisites",
    "title": "History of statistics and data sciences",
    "section": "Prerequisites",
    "text": "Prerequisites\n\nAt least 1.0 FCE 300+ level STA courses with a minimum grade of 80 per cent in each course."
  },
  {
    "objectID": "teaching-history_of_statistics.html#content",
    "href": "teaching-history_of_statistics.html#content",
    "title": "History of statistics and data sciences",
    "section": "Content",
    "text": "Content\n\nWeek 1\nOverview. Also early astronomical and gambling underpinnings. Least squares, combining observations, and uncertainty. Legendre, Laplace, Bernoulli, De Moivre, Simpson.\n\nFienberg (1992)\nKendall (1960)\nStigler (1986, Chs 1-2)\nHacking (2006, “Introduction”)\n\n\n\nWeek 2\nThe 1700s, especially inverse probability. Gauss, Laplace, Central Limit Theorem.\n\nStigler (1986, Chs 3-4)\nSheynin (2018, Chs 1-7)\nMiller and Gelman (2020a)\n\nKahneman and Bar-Hillel (2020)\nShafer (2020)\nMiller and Gelman (2020b)\n\n\n\n\nWeek 3\nEarly 1800s, and moves to the social sciences. Quetelet, Poisson, Cournot, Lexis, binomials and Law of Large Numbers.\n\nStigler (1986, Chs 5-6)\nSheynin (2018, Chs 8-9)\n\n\n\nWeek 4\nLate 1800s and heredity. Galton, Edgeworth, and Pearson. Regression and correlation.\n\nStigler (1986, Chs 7-8)\nSheynin (2018, Chs 10-11)\nSalsburg (2002)\nPorter (2020)\n\n\n\nWeek 5\nLate 1800s and early 1900s. Edgeworth, Pearson, and Yule. Regression, and correlation.\n\nContent:\n\nStigler (1986, Chs 9-10)\nDavid Freedman, 1999, ‘From association to causation: some remarks on the history of statistics’.\nDonald MacKenzie, 1981, Statistics in Britain, 1865-1930: The Social Construction of Scientific Knowledge.\nErich Lehmann, 2011, Fisher, Neyman, and the Creation of Classical Statistics, Springer.\n\n\n\n\nWeek 6\nEarly 1900s\n\nStephen M. Stigler, 1996, ‘The History of Statistics in 1933’, Statistical Science\nSheynin (2018, Chs 12-15)\nStephen M. Stigler, 2016, The Seven Pillars of Statistical Wisdom.\nJan von Plato, 1994, Creating Modern Probability.\nErich Lehmann, 2007, Reminiscences of a Statistician.\n\n\n\nWeek 7 “Data visualization”\n\nFriendly and Wainer (2021, Chs 1-6 and 9)\n\n\n\nWeek 8 “The rise of Bayesian methods”\n\nFienberg (2003)\nLeonard (2014)\nMcGrayne (2012)\nNordhaus (2007)\nLindley (2000)\nGelman and Robert (2013a)\n\nGelman and Robert (2013b)\n\n\n\n\nWeek 9 “Causal inference”\n\nPearl and Mackenzie (2018, Ch 2)\n\n\n\nWeek 10 “Whither statistics? The rise of data science”\n\nContent:\n\nLeo Breiman, 2001, ‘Statistical Modeling: The Two Cultures’, Statistical Sciences.\nDavid J. Hand, 2015, ‘Statistics and computing: the genesis of data science’, Statistics and Computing.\nDavid Donoho, 2017, ‘50 Years of Data Science’, Journal of Computational and Graphical Statistics.\nGelman and Vehtari (2021)\n\n\n\n\nWeek 11 “Overlooked contributors”\n\nContent:\n\nMargo Anderson, 1992, ‘The History of Women and the History of Statistics’, Journal of Women’s History\nKatie Hafner, 2021, ‘Arianna Rosenbluth Dies at 93; Pioneering Figure in Data Science’, New York Times.\nKitagawa–Blinder–Oaxaca decomposition.\nMary E. Thompson, ‘Reflections on women in statistics in Canada’, in Xihong Lin, et al., eds, 2014, Past, present, and future of statistical science, CRC Press.\nNancy M. Reid, ‘The whole women thing’, in Xihong Lin, et al., eds, 2014, Past, present, and future of statistical science, CRC Press.\nLouise M. Ryan, ‘Reflections on diversity’, in Xihong Lin, et al., eds, 2014, Past, present, and future of statistical science, CRC Press.\n\n\n\n\nWeek 12 “Reckoning with the past and thinking about the future: Statistics and society”\n\nContent:\n\nHow to consider our history?\nAlain Desrosières, 2002, The Politics of Large Numbers, Harvard University Press.\nXihong Lin, et al., eds, 2014, Past, present, and future of statistical science, CRC Press."
  },
  {
    "objectID": "teaching-history_of_statistics.html#assessment",
    "href": "teaching-history_of_statistics.html#assessment",
    "title": "History of statistics and data sciences",
    "section": "Assessment",
    "text": "Assessment\n\nSummary\n\n\n\n\n\n\n\n\nItem\nWeight (%)\nDue date\n\n\n\n\nTutorial\n60\nFortnightly before the lecture\n\n\nFinal Paper\n40\nTen days after that\n\n\n\n\n\nTutorial papers\n\nDue date: Fortnightly before the lecture.\nWeight: Each is worth 10 per cent.\nTask: Write a paper of 2-6 pages on a topic covered in the preceding two weeks. These will be circulated and discussed in class.\n\n\n\nFinal Paper\n\nDue dates: Final day of exam block.\nWeight: 40 per cent.\nTask: Write an original paper on a topic covered in the class."
  },
  {
    "objectID": "teaching-inf2178.html",
    "href": "teaching-inf2178.html",
    "title": "Experimental design for data science",
    "section": "",
    "text": "INF2178 is a masters-level course at the University of Toronto’s Faculty of Information.\nExperimental design has a long and robust tradition within traditional applications such as agriculture, medicine, physics, and chemistry. It allows us to speak of causality with confidence. Typically, these are situations in which control groups can be established, randomization is appropriate, and ethical concerns can be assuaged. Unfortunately, such a set-up is rarely possible in the full extent of the modern applications where we want to understand causality.\n\n\n\n\n\n\n\n\n\nSource: https://xkcd.com/2400/\nThis course covers the traditional approaches and statistical methods, but focuses on what to do when traditional experimental design methods cannot be implemented or are not appropriate (i.e. what feels like most of the time these days). We cover experiments in their modern guise especially the concerns that we might have when we can run them; but also methods that can provide some causal understanding even when we cannot conduct traditional experiments. Importantly, these approaches do not rely on ‘big data’ or fancy statistics, but instead on thoroughly interrogating the data that are available to get understanding through as simple means as possible.\nThis is a hands-on course in which you will conduct research projects using real-world data. This means that you will: obtain and clean relevant datasets; develop your own research questions; use the statistical techniques that you are introduced to in class to answer those questions; and finally communicate your results in a meaningful way. This course is designed around approaches that are used extensively in academia, government, and industry. Furthermore, it includes many aspects, such as data cleaning and preparation, that are critical, but rarely taught.\nThis course is different to many other courses at the University of Toronto. At the end of this course, you will have a portfolio of work that you could show off to a potential employer. You will have developed the skills to work successfully as an applied statistician or data scientist. And you will know how to fill gaps in your knowledge yourself. A lot of scholarships and jobs these days ask for GitHub and blog links etc to show off a portfolio of your work. This is the class that gives you a chance to develop these. It’s very important to having something that shows off what you can do, and that needs to go beyond what is done in a normal class.\n\n\n\n\nIn this course you will work in a self-directed, open-ended manner. Identify relevant areas of interest and then learn the skills that you need to explore those areas.\nTo successfully complete this course, you should expect to spend a large portion of your time reading and writing (both code and text). Deeply engage with the materials. Find a small study group and keep each other motivated and focused. At the start of the week, read the course notes, all compulsory materials and some recommended materials based on your interest. After doing that, but before the ‘lecture’ time you should complete the weekly quiz. During ‘lectures’ I’ll live-code, discuss materials in the course notes, talk about an experiment, and you’ll have a chance to discuss the materials with me.\nYou need to be more active in your learning in this course than others - read the notes and related materials - and then go out there and teach yourself more and apply it. You will not be spoon-fed in this course. Each week try to write reproducible, understandable, R code surrounded by beautifully crafted text that motivates, backgrounds, explains, discusses and criticizes. Make steady progress toward the assessment.\nThis is not a ‘bird course’. Typically, after the term is finished, students say that the course is difficult but rewarding. The TAs and I are always available to answer any questions. Please come to office hours!\n\n\n\nThis webpage will provide almost all the guiding materials that you need and links to the relevant parts of the notes. The course notes are available here: https://www.tellingstorieswithdata.com. Those contain notes and other material that you could go over. There is a course Slack for discussion. We’ll use Quercus really only for assessment submission and grading. I expect you to work professionally, and so we’ll try to use professional tools to the extent possible.\nA rough weekly flow for the course would be something like:\n\nRead the week’s course notes.\nRead/watch/listen to the compulsory materials.\nComplete the weekly quiz.\nAttend the lecture.\nAttend the lab.\nMake progress on a paper.\n\n\n\n\nSuccessful past students have the following advice (completely unedited by me):\n\n‘Read the rubrics and treat INF2178 as a storytelling with data course rather than INF1344 part 2. Let the point distribution rubric guide what parts of the paper to focus on/expand. And pay attention to the “telling stories” part of the site URL. The point of the papers isn’t to chuck every statistical method and cool R trick you know in there, it’s to practice building data-driven arguments that different audiences might actually care about. Last, the piece of advice I wish I had followed for the class: keep a log of things that you find confusing/cool/useful.’\n‘Allocate as much time as you realistically can to projects. It all takes longer than expected (particularly debugging) so be a friend to your future self and don’t leave work to the last minute. Really.’\n‘Read the paper rubrics very carefully! The papers will always take longer than you think they will so try to start early and put enough time before submitting to make sure you sort out any issues with formatting and knitting your pdf because there will almost always be issues (especially if you’re doing it for the first time). Write the papers with an external audience in mind, not just as a school paper that only Rohan will read. Consider the papers to be personal projects that your peers and potential employers will be reading, and try to write something you feel could be published. Also, comment your code and thank yourself later :)’\n‘Do the readings even though there are a lot and don’t be afraid to ask questions no matter how stupid you think they are (unless the answer can easily be found, ie “is assignment 1 group or solo”). Also, Rohan is not as intimidating as he initially seems. Also, if you did poorly on assignment 1, don’t drop the class. There’s still so much to learn and the chance of making it up with the other two assignments.’\n‘You’re going to have to practice solving your own code problems, reviewing and keeping track of detailed assignment requirements, critical thinking about data, and editing your own work. It will take more time than you think.’’\n‘1) Try to keep up week to week as best as possible; watching the lectures is more rewarding that way because you have some understanding, especially if you watch asynchronously and can’t just ask the professor questions. Sometimes that means even skimming what readings you can if time is tight, otherwise you may feel quite lost. 2) R documentation is your best friend when you don’t wholly understand the examples in the course notes, or you’re looking for other viable solutions. Stack Overflow is also a great help, but don’t forget to cite everything you’ve taken from there so you remember where it came from when wondering, “How does this even work? It works, but how?” 3) Sometimes, your code will infuriate you. Take breaks. Step away for a half hour, do something else entirely for a while, or even come back tomorrow. Don’t beat yourself up too much over it. You can also ask for assistance in the course Slack, because odds are someone else has already encountered this issue and arrived at a solution. 4) Trust Rohan and his process. It’s going to rely on your efforts to learn on your own -and it’s going to feel like a harsh bootcamp- but you’ll learn to make friends with data and coding. 5) Practice the examples on your own before class. 6) Don’t be afraid or intimidated by other’s who seem to know more than you. 7) Attend Toronto Data Workshop whenever you can. 8) Form a study group early on’\n‘Don’t stop at just the assignment prompt. Have fun with your topic and see how you can take it further’\n\n\n\n\nThanks to the following who helped develop this course: Monica Alexander, Kelly Lyons, Sharla Gelfand, Faria Khandaker, Hidaya Ismail, A Mahfouz, Paul Hodgetts, Thomas Rosenthal.\n\n\n\n\n2020\n\nStudent evals\n\n2021\n\nSyllabus\nStudent evals"
  },
  {
    "objectID": "teaching-inf2178.html#preamble",
    "href": "teaching-inf2178.html#preamble",
    "title": "Experimental design for data science",
    "section": "",
    "text": "INF2178 is a masters-level course at the University of Toronto’s Faculty of Information.\nExperimental design has a long and robust tradition within traditional applications such as agriculture, medicine, physics, and chemistry. It allows us to speak of causality with confidence. Typically, these are situations in which control groups can be established, randomization is appropriate, and ethical concerns can be assuaged. Unfortunately, such a set-up is rarely possible in the full extent of the modern applications where we want to understand causality.\n\n\n\n\n\n\n\n\n\nSource: https://xkcd.com/2400/\nThis course covers the traditional approaches and statistical methods, but focuses on what to do when traditional experimental design methods cannot be implemented or are not appropriate (i.e. what feels like most of the time these days). We cover experiments in their modern guise especially the concerns that we might have when we can run them; but also methods that can provide some causal understanding even when we cannot conduct traditional experiments. Importantly, these approaches do not rely on ‘big data’ or fancy statistics, but instead on thoroughly interrogating the data that are available to get understanding through as simple means as possible.\nThis is a hands-on course in which you will conduct research projects using real-world data. This means that you will: obtain and clean relevant datasets; develop your own research questions; use the statistical techniques that you are introduced to in class to answer those questions; and finally communicate your results in a meaningful way. This course is designed around approaches that are used extensively in academia, government, and industry. Furthermore, it includes many aspects, such as data cleaning and preparation, that are critical, but rarely taught.\nThis course is different to many other courses at the University of Toronto. At the end of this course, you will have a portfolio of work that you could show off to a potential employer. You will have developed the skills to work successfully as an applied statistician or data scientist. And you will know how to fill gaps in your knowledge yourself. A lot of scholarships and jobs these days ask for GitHub and blog links etc to show off a portfolio of your work. This is the class that gives you a chance to develop these. It’s very important to having something that shows off what you can do, and that needs to go beyond what is done in a normal class.\n\n\n\n\nIn this course you will work in a self-directed, open-ended manner. Identify relevant areas of interest and then learn the skills that you need to explore those areas.\nTo successfully complete this course, you should expect to spend a large portion of your time reading and writing (both code and text). Deeply engage with the materials. Find a small study group and keep each other motivated and focused. At the start of the week, read the course notes, all compulsory materials and some recommended materials based on your interest. After doing that, but before the ‘lecture’ time you should complete the weekly quiz. During ‘lectures’ I’ll live-code, discuss materials in the course notes, talk about an experiment, and you’ll have a chance to discuss the materials with me.\nYou need to be more active in your learning in this course than others - read the notes and related materials - and then go out there and teach yourself more and apply it. You will not be spoon-fed in this course. Each week try to write reproducible, understandable, R code surrounded by beautifully crafted text that motivates, backgrounds, explains, discusses and criticizes. Make steady progress toward the assessment.\nThis is not a ‘bird course’. Typically, after the term is finished, students say that the course is difficult but rewarding. The TAs and I are always available to answer any questions. Please come to office hours!\n\n\n\nThis webpage will provide almost all the guiding materials that you need and links to the relevant parts of the notes. The course notes are available here: https://www.tellingstorieswithdata.com. Those contain notes and other material that you could go over. There is a course Slack for discussion. We’ll use Quercus really only for assessment submission and grading. I expect you to work professionally, and so we’ll try to use professional tools to the extent possible.\nA rough weekly flow for the course would be something like:\n\nRead the week’s course notes.\nRead/watch/listen to the compulsory materials.\nComplete the weekly quiz.\nAttend the lecture.\nAttend the lab.\nMake progress on a paper.\n\n\n\n\nSuccessful past students have the following advice (completely unedited by me):\n\n‘Read the rubrics and treat INF2178 as a storytelling with data course rather than INF1344 part 2. Let the point distribution rubric guide what parts of the paper to focus on/expand. And pay attention to the “telling stories” part of the site URL. The point of the papers isn’t to chuck every statistical method and cool R trick you know in there, it’s to practice building data-driven arguments that different audiences might actually care about. Last, the piece of advice I wish I had followed for the class: keep a log of things that you find confusing/cool/useful.’\n‘Allocate as much time as you realistically can to projects. It all takes longer than expected (particularly debugging) so be a friend to your future self and don’t leave work to the last minute. Really.’\n‘Read the paper rubrics very carefully! The papers will always take longer than you think they will so try to start early and put enough time before submitting to make sure you sort out any issues with formatting and knitting your pdf because there will almost always be issues (especially if you’re doing it for the first time). Write the papers with an external audience in mind, not just as a school paper that only Rohan will read. Consider the papers to be personal projects that your peers and potential employers will be reading, and try to write something you feel could be published. Also, comment your code and thank yourself later :)’\n‘Do the readings even though there are a lot and don’t be afraid to ask questions no matter how stupid you think they are (unless the answer can easily be found, ie “is assignment 1 group or solo”). Also, Rohan is not as intimidating as he initially seems. Also, if you did poorly on assignment 1, don’t drop the class. There’s still so much to learn and the chance of making it up with the other two assignments.’\n‘You’re going to have to practice solving your own code problems, reviewing and keeping track of detailed assignment requirements, critical thinking about data, and editing your own work. It will take more time than you think.’’\n‘1) Try to keep up week to week as best as possible; watching the lectures is more rewarding that way because you have some understanding, especially if you watch asynchronously and can’t just ask the professor questions. Sometimes that means even skimming what readings you can if time is tight, otherwise you may feel quite lost. 2) R documentation is your best friend when you don’t wholly understand the examples in the course notes, or you’re looking for other viable solutions. Stack Overflow is also a great help, but don’t forget to cite everything you’ve taken from there so you remember where it came from when wondering, “How does this even work? It works, but how?” 3) Sometimes, your code will infuriate you. Take breaks. Step away for a half hour, do something else entirely for a while, or even come back tomorrow. Don’t beat yourself up too much over it. You can also ask for assistance in the course Slack, because odds are someone else has already encountered this issue and arrived at a solution. 4) Trust Rohan and his process. It’s going to rely on your efforts to learn on your own -and it’s going to feel like a harsh bootcamp- but you’ll learn to make friends with data and coding. 5) Practice the examples on your own before class. 6) Don’t be afraid or intimidated by other’s who seem to know more than you. 7) Attend Toronto Data Workshop whenever you can. 8) Form a study group early on’\n‘Don’t stop at just the assignment prompt. Have fun with your topic and see how you can take it further’\n\n\n\n\nThanks to the following who helped develop this course: Monica Alexander, Kelly Lyons, Sharla Gelfand, Faria Khandaker, Hidaya Ismail, A Mahfouz, Paul Hodgetts, Thomas Rosenthal.\n\n\n\n\n2020\n\nStudent evals\n\n2021\n\nSyllabus\nStudent evals"
  },
  {
    "objectID": "teaching-inf2178.html#content",
    "href": "teaching-inf2178.html#content",
    "title": "Experimental design for data science",
    "section": "Content",
    "text": "Content\nEach week you should go through the course notes and all compulsory materials. During the lecture I will live-code various aspects. I will also discuss a case study, typically a paper. During the lab, a TA will either lead small group discussions or similarly lead other work. The lecture will be recorded and posted here, but again, it’s not enough to just watch that - you need to read and write yourself.\n\nWeek 1\n‘Drinking from a fire hose’.\n\nContent: Drinking from a fire hose, R Essentials.\nCase Study: Fisher’s Lady Tasting Tea.\nLab: Go through first four modules of DoSS Toolkit and discuss any issues with the TA.\n\n\n\nWeek 2\n‘Science-ing’.\n\nContent: Workflow, Static communication.\nCase Study: Tuskegee Syphilis Study.\nLab: Go through modules five to eight of DoSS Toolkit and discuss any issues with the TA.\n\n\n\nWeek 3\n‘Why, if ever I did fall off—which there’s no chance of—but if I did–’.\n\nContent: Experiments, and treatment effects.\nCase Study: The Oregon Health Insurance Experiment in the United States.\nSpecial guest: Greg Wilson on how to run a meeting.\nLab:\n\nPlease pretend that you work as a junior analyst for a large consulting firm. Further, pretend that your consulting firm has taken a contract to put together a facial recognition model for the Canada Border Services Agency’s Inland Enforcement branch. Write five or six points with regard to your thoughts on this matter. What would you do and why? Then split into small groups and compare your points with others. Do you think the model would end up being implemented?\nWith the help of the TA, please conduct ‘face-to-face’ surveys (via Zoom). For this exercise, you will be randomly split into groups of two. You have two minutes in each group and will then be swapped to another group. One person is to survey the other person asking the following questions: i) ‘What is your gender?’, ii) ‘What is your age?’, iii) ‘What is your marital status?’, iv) ‘What is your income?’, v) ‘If an election were held today who would you vote for?’. After one person is done, then switch roles. When you are the questioner you should record all responses using a small CSV (but not the person’s name please). When you are the respondent you are welcome to not respond. You will cycle through this multiple times. At the end, please write a small reflection about: 1) as a respondent, how you felt answering these questions and the implications that you think this feeling may have for how survey questions are answered more generally; and 2) as a questioner, how difficult it was to code responses and the implications this may have for the dataset that we analyse.\n\n\n\n\nWeek 4\n‘Gathering data’.\n\nContent: Gathering data.\nCase Study: Student Coaching: How Far Can Technology Go?\nLab:\n\nPlease pretend you work for Netflix and you want to know more about why people subscribe (or don’t!) when prices change. Please design an experiment, discuss its key features and how you would implement it. Please pay special attention to sampling issues. Then simulate an outcome.\nFollowing the guidance of the TA, please scrape some data and discuss some ethical considerations around the dataset that you created. You may like to write a short blog post discussing the difference between data being public but scattered, and a consolidated dataset being public with reference to Kirkegaard and Bjerrekær, 2016, and Politou, Alepis, and Patsakis, 2018 (if you do that please do email a link to me out of interest).\n\n\n\n\nWeek 5\n‘Whoops, I forgot EDA’.\n\nContent: Exploratory Data Analysis.\nCase Study: Civic honesty around the globe\nLab:\n\nPretend that you work for Loblaws as a data scientist and it is late March 2020. As part of normal monitoring, you have noticed that purchases of flour and pasta have increased substantially. You had been planning to increase the price of these items in April as part of a trial, but now your manager is not sure whether it is appropriate to conduct the trial. Please write five or six points with regard to your thoughts on this matter. What would you do and why?\nAnalyse the Toronto AirBNB dataset with guidance from the TA.\n\n\n\n\nWeek 6\n‘IJALM - It’s Just A Linear Model’.\n\nContent: Linear and logistic regression and tidymodels\nCase Study: Upworthy A/B tests of headlines.\nSpecial guest: Kathy Ge on experiments at Uber.\nLab recording:\n\nFollowing the guidance of the TA, please use Blogdown to create a simple website and then design and execute a simple A/B test for your website using Netlify.\n\n\n\n\nWeek 7\n‘Celestial Navigation’.\n\nContent: Simulation, power, RCTs, A/B testing.\nCase Study: Please pick one chapter from Catherine D’Ignazio and Lauren F. Klein, Data Feminism, that is of interest to you and read it (freely available: https://data-feminism.mitpress.mit.edu).\nLab:\n\nFollowing the guidance of the TA, please make a Shiny app that bundles a little data and some code and post it to shinyapps.com.\n\n\n\n\nWeek 8\n‘Such a shame they’ll never meet’.\n\nContent: Matching and difference in differences.\nCase Study: Funding of Clinical Trials and Reported Drug Efficacy\nSpecial guest: Emily Riederer on observational causal inference.\nSpecial guest: Tamar Oostrom on funding of clinical trials.\nLab:\n\nFollowing the guidance of the TA, please look at McClelland, Alexander, 2019, ‘“Lock This Whore Up”: Legal Violence and Flows of Information Precipitating Personal Violence against People Criminalised for HIV-Related Crimes in Canada’, European Journal of Risk Regulation, 10 (1), pp. 132-147.\nThen look at Policing the Pandemic - https://www.policingthepandemic.ca/. Look into how they gathered their dataset and what it took to put this together. What is in the dataset and why? What is missing and why? How could this affect the results? How might similar biases enter into other datasets that you have used or read about?\nPut together a brief model. You may like to write a short blog post about the biases and influences that are in this dataset (if you do that please do email a link to me out of interest).\n\n\n\n\nWeek 9\n‘Why does it always rain on me?’.\n\nContent: Regression discontinuity and instrumental variables.\nCase Study:\n\nJames H. Ware, 1989, ‘Investigating Therapies of Potentially Great Benefit: ECMO’, Statistical Science, available here.\nDonald A. Berry, 1989, ‘Comment: Ethics and ECMO’, Statistical Science, available here.\n\nLab:\n\nFollowing the guidance of the TA, please make an R package that bundles a little data and some code and add it to your GitHub. Don’t forget to include at least one test.\n\n\n\n\nWeek 10\n‘Post Hoc, Ergo Propter Hoc’.\n\nContent: DAGs, bias, and paradoxes.\nCase Study: Joshua Kalla and David Broockman, 2016, ‘Campaign Contributions Facilitate Access to Congressional Officials: A Randomized Field Experiment’\nLab:\n\nFollowing the guidance of the TA, please look back on the case studies that we’ve covered so far. Please break up into small groups and create DAGs for each. Then write some notes about the potential for confounding, selection bias and measurement bias. Pick one person in your group to make a brief 2-minute presentation about what you did.\n\n\n\n\nWeek 11\n‘But it works on my machine’.\n\nContent: Shiny, cloud, and deploying.\nCase Study: Alexander, M., Wildeman, C., Roehrkasse, A., and Rudlang-Perman, K., 2020, ‘Forecasting child welfare outcomes in the United States’, Shiny app; Technical model summary.\nLab:\n\nFollowing the guidance of the TA, and thinking about what we covered in lectures, please read, compare, and discuss:\n\nBendavid, E., Mulaney, B., Sood, N., Shah, S., Ling, E., Bromley-Dulfano, R., …, and Tversky, D, 2020, ‘COVID-19 Antibody Seroprevalence in Santa Clara County, California’, MedRxiv, https://www.medrxiv.org/content/10.1101/2020.04.14.20062463v1.full.pdf.\nGelman, Andrew, 2020, ‘Concerns with that Stanford study of coronavirus prevalence’, Statistical Modeling, Causal Inference, and Social Science, 19 April, https://statmodeling.stat.columbia.edu/2020/04/19/fatal-flaws-in-stanford-study-of-coronavirus-prevalence/.\nEisen, Michael B., and Robert Tibshirani, 2020, ‘How to Identify Flawed Research Before It Becomes Dangerous’, New York Times, 20 July, https://www.nytimes.com/2020/07/20/opinion/coronavirus-preprints.html.\nGelman, Andrew and Bob Carpenter, 2020, ‘Bayesian analysis of tests with unknown specificity and sensitivity’, 8 July, http://www.stat.columbia.edu/~gelman/research/published/specificity.pdf.\n\n\n\n\n\nWeek 12\n‘Lorem ipsum’.\n\nContent: Text-as-data.\nCase Study: Kevin Munger, Patrick Egan, Jonathan Nagler, Jonathan Ronen, and Joshua A. Tucker, 2017, ‘Political Knowledge and Misinformation in the Era of Social Media: Evidence From the 2015 UK Election’.\nLab:\n\nPlease form small groups and discuss, ‘to what extent do quantitative methods merely project forward the past, and what implications does this have for our conduct as practitioners and consumers?’"
  },
  {
    "objectID": "teaching-inf2178.html#assessment",
    "href": "teaching-inf2178.html#assessment",
    "title": "Experimental design for data science",
    "section": "Assessment",
    "text": "Assessment\n\nSummary\n\n\n\n\n\n\n\n\nItem\nWeight (%)\nDue date\n\n\n\n\nWeekly quiz\n20\nWeekly before the lecture\n\n\nProfessional conduct\n1\nAnytime during the teaching term\n\n\nPaper 1\n25\nEnd of Week 3\n\n\nPaper 2\n25\nEnd of Week 6\n\n\nPaper 3\n25\nEnd of Week 9\n\n\nFinal Paper (initial submission)\n1\nEnd of Week 12\n\n\nFinal Paper (peer review)\n3\nThree days after that\n\n\nFinal Paper\n25\nTen days after that\n\n\n\n\n\nWeekly quizzes\n\nDue date: Weekly before the lecture.\nWeight: 20 per cent (no quiz in Week 1 or Week 12 and only best eight out of ten count.)\nTask: Please complete a weekly quiz in Quercus.\nQuestions: The questions that form the quiz are drawn from those in the course notes.\n\n\n\nProfessional conduct\n\nDue date: Anytime during the teaching term.\nWeight: 1 per cent\nTask: We (optionally) use Slack to interact in this class. At some point during the teaching term, please use Slack to answer another student’s question or otherwise similarly be generally helpful in a professional manner. When you do that, please share the comment into the ‘Professional conduct’ channel and @ me (hover on the message, click share message, type in the channel ‘profession_conduct’, add a message that @‘s me, and click ’share’). You’ll get the full mark just for one helpful interaction. (If you are opting out of using Slack - which is entirely fine - then instead, at some point in the term send me an email with a link that is relevant to the course materials and that I should add to the course notes. Please be clear that this is your ‘professional conduct’ submission by stating that in the subject line.)\n\n\n\nPaper #1\n\nDue date: End of Week 3.\nWeight: 25 per cent (for Papers #1-#3 the best two of three count).\nTask: ‘Mandatory minimums’\n\n\n\nPaper #2\n\nDue date: End of Week 6.\nWeight: 25 per cent (for Papers #1-#3 the best two of three counts).\nTask: ‘These numbers mean dial it up’\n\n\n\nPaper #3\n\nDue date: End of Week 9.\nWeight: 25 per cent (for Papers #1-#3 the best two of three counts).\nTask: ‘The Short List’.\n\n\n\nFinal Paper\n\nDue dates:\n\nInitial submission: End of Week 12.\nPeer review: Three days after that.\nFinal Paper: Ten days after that.\n\nWeight: 29 per cent (4 per cent of this is for initial submission and peer review conducted a week before).\n\nInitial submission: 1 per cent\nPeer review: 3 per cent\nFinal Paper: 25 per cent\n\nTask: ‘Two Cathedrals’"
  },
  {
    "objectID": "teaching-nlp.html",
    "href": "teaching-nlp.html",
    "title": "Natural Language Processing",
    "section": "",
    "text": "The purpose of this course is to develop students who can:\n\nengage in thoughtful, ethical, critique of Natural Language Processing (NLP), especially Large Language Models (LLMs);\nwork productively to implement existing NLP methods, especially LLMs; and\nuse NLP, and especially LLMs, to contribute to our understanding of the world.\n\nStudents are expected to develop:\n\nan understanding of NLP, especially LLMs, and its place in the world;\nexceptional written and verbal communication skills; and\ncontribute in some small way to our understanding of something related to NLP, ideally LLMs."
  },
  {
    "objectID": "teaching-nlp.html#overview",
    "href": "teaching-nlp.html#overview",
    "title": "Natural Language Processing",
    "section": "",
    "text": "The purpose of this course is to develop students who can:\n\nengage in thoughtful, ethical, critique of Natural Language Processing (NLP), especially Large Language Models (LLMs);\nwork productively to implement existing NLP methods, especially LLMs; and\nuse NLP, and especially LLMs, to contribute to our understanding of the world.\n\nStudents are expected to develop:\n\nan understanding of NLP, especially LLMs, and its place in the world;\nexceptional written and verbal communication skills; and\ncontribute in some small way to our understanding of something related to NLP, ideally LLMs."
  },
  {
    "objectID": "teaching-nlp.html#pre-requisites",
    "href": "teaching-nlp.html#pre-requisites",
    "title": "Natural Language Processing",
    "section": "Pre-requisites",
    "text": "Pre-requisites\n\nComfortable coding in R and Python and using GitHub.\nGo through Silge, Julia & David Robinson, 2020, Text Mining with R, https://www.tidytextmining.com."
  },
  {
    "objectID": "teaching-nlp.html#content",
    "href": "teaching-nlp.html#content",
    "title": "Natural Language Processing",
    "section": "Content",
    "text": "Content\n\nWeek 1: Essentials I\n\n(Motivation) Karpathy, Andrej, 2013, “Intro to Large Language Models”, YouTube, 22 November, https://youtu.be/zjkBMFhNj_g.\nHvitfeldt, Emil & Julia Silge, 2020, Supervised Machine Learning for Text Analysis in R, Chapters 1-3, https://smltar.com.\nJurafsky, Dan, and James H. Martin, 2020, Speech and Language Processing, 3rd ed., Chapter 3, https://web.stanford.edu/~jurafsky/slp3/.\nHovy, Dirk and Shannon L. Spruit, 2016, ‘The Social Impact of Natural Language Processing’, Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pp. 591–598,https://aclweb.org/anthology/P16-2096.pdf.\nPrabhumoye, Shrimai, Elijah Mayfield, and Alan W Black, 2019, ‘Principled Frameworks for Evaluating Ethics in NLP Systems’, Proceedings of the 2019 Workshop on Widening NLP, https://aclweb.org/anthology/W19-3637/.\n\n\n\nWeek 2: Essentials II\n\nHvitfeldt, Emil & Julia Silge, 2020, Supervised Machine Learning for Text Analysis in R, Chapters 4-6, https://smltar.com.\nBolukbasi, Tolga, Kai-Wei Chang, James Y. Zou, Venkatesh Saligrama and Adam T. Kalai, 2016, ‘Man is to Computer Programmer as Woman is to Homemaker? Debiasing Word Embeddings’, Advances in Neural Information Processing Systems, 29 (NIPS 2016), http://papers.nips.cc/paper/6228-man-is-to-computer-programmer-as-woman-is-to-homemaker-d.\nChang, Kai-Wei, Vinod Prabhakaran, and Vicente Ordonez, 2019, ‘Bias and Fairness in Natural Language Processing’, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP): Tutorial Abstracts, https://aclweb.org/anthology/D19-2004/.\n\n\n\nWeek 3: Essentials III\n\nJurafsky, Dan, and James H. Martin, 2020, Speech and Language Processing, 3rd ed., Chapters 4 and 5, https://web.stanford.edu/~jurafsky/slp3/.\nHutchinson, Ben, Vinodkumar Prabhakaran, Emily Denton, Kellie Webster, Yu Zhong, and Stephen Denuyl, 2020, ‘Social Biases in NLP Models as Barriers forPersons with Disabilities’, arXiv, https://arxiv.org/abs/2005.00813.\n\n\n\nWeek 4: NLP intermediate I\n\nJurafsky, Dan, and James H. Martin, 2020, Speech and Language Processing, 3rd ed., Chapters 6 and 7, https://web.stanford.edu/~jurafsky/slp3/.\nSolaiman, Irene, Miles Brundage, Jack Clark, Amanda Askell, ArielHerbert-Voss, Jeff Wu, Alec Radford, Gretchen Krueger, Jong Wook Kim, SarahKreps, Miles McCain, Alex Newhouse, Jason Blazakis, Kris McGuffie, Jasmine Wang, 2019, ‘Release Strategies and the Social Impacts of Language Models’,arXiv, https://arxiv.org/abs/1908.09203.\n\n\n\nWeek 5: NLP intermediate II\n\nHvitfeldt, Emil & Julia Silge, 2020, Supervised Machine Learning for Text Analysis in R, Chapters 7-9, https://smltar.com.\nTatman, Rachel, 2020, ‘What I Won’t Build’, Widening NLP Workshop 2020, Keynote address, 5 July, https://slideslive.com/38929585/what-i-wont-build and http://www.rctatman.com/talks/what-i-wont-build.\n\n\n\nWeek 6: NLP intermediate III\n\nFrançois Chollet, 2021, Deep Learning with Python, Chapters 1-4.\nZhao, Jieyu, Tianlu Wang, Mark Yatskar, Vicente Ordonez and Kai-Wei Chang,2017, ‘Men Also Like Shopping: Reducing Gender Bias Amplification using Corpus-level Constraints’, Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pp. 2979–2989, https://aclweb.org/anthology/D17-1323.pdf.\n\n\n\nWeek 7: Deep learning I\n\nFrançois Chollet, 2021, Deep Learning with Python, Chapters 5-7.\n\n\n\nWeek 8: Deep learning II\n\nFrançois Chollet, 2021, Deep Learning with Python, Chapters 11 and 12.\nBender, Emily M. and Koller, Alexander, 2020, ‘Climbing towards NLU: On Meaning, Form, and Understanding in the Age of Data’, *Proceedings of the 58th Annual Meeting of the\n\n\n\nWeek 9: Deep learning III\n\nJurafsky, Dan, and James H. Martin, 2020, Speech and Language Processing, 3rd ed., Chapters 8 and 9, https://web.stanford.edu/~jurafsky/slp3/. Association for Computational Linguistics*, pp. 5185–5198, https://www.aclweb.org/anthology/2020.acl-main.463\nAnna Rogers, Isabelle Augenstein, 2020, ‘What Can We Do to Improve Peer Review in NLP?’, arXiv, 8 October, https://arxiv.org/abs/2010.03863.\n\n\n\nWeek 10: Transformers I\n\nKarpathy, Andrej, 2022, Neural Networks: Zero to Hero, Both of ‘The spelled-out intro…’ videos.\nAlammar, Jay, 2018, ‘The Illustrated Transformer’, http://jalammar.github.io/illustrated-transformer/.\nBoykis, Vicky, What are embeddings?, https://vickiboykis.com/what_are_embeddings/\nManning, Vaswani and Huang, 2019, ‘Stanford CS224N: NLP with Deep Learning | Winter 2019 | Lecture 14 – Transformers and Self-Attention’, https://www.youtube.com/watch?v=5vcj8kSwBCY&list=PLoROMvodv4rOhcuXMZkNm7j3fVwBBY42z&index=14&ab_channel=stanfordonline.\nUszkoreit, Jakob, 2017, ‘Transformer: A Novel Neural Network Architecture for Language Understanding’, Google AI Blog, 31 August, https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html\n\n\n\nWeek 11: Transformers II\n\nKarpathy, Andrej, 2022, Neural Networks: Zero to Hero, The ‘Building makemore…’ videos\nAlammar, Jay, 2020, ‘How GPT3 Works - Visualizations and Animations’, 27 July, https://jalammar.github.io/how-gpt3-works-visualizations-animations/\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser and Illia Polosukhin, 2017, ‘Attention Is All You Need’, arXiv, http://arxiv.org/abs/1706.03762.\nJacob Devlin and Ming-Wei Chang, 2018, ‘Open Sourcing BERT: State-of-the-Art Pre-training for Natural Language Processing’, 2 November, Google AI Blog, https://ai.googleblog.com/2018/11/open-sourcing-bert-state-of-art-pre.html.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, 2018, ‘BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding’, arXiv, https://arxiv.org/abs/1810.04805.\nRush, Alexander, 2018, ‘The Annotated Transformer’, https://nlp.seas.harvard.edu/2018/04/03/attention.html\n\n\n\nWeek 12: Transformers III\n\nKarpathy, Andrej, 2022, Neural Networks: Zero to Hero, The ‘Let’s build GPT: from scratch, in code, spelled out.’\nTom B. Brown and Benjamin Mann and Nick Ryder and Melanie Subbiah and Jared Kaplan and Prafulla Dhariwal and Arvind Neelakantan and Pranav Shyam and Girish Sastry and Amanda Askell and Sandhini Agarwal and Ariel Herbert-Voss and Gretchen Krueger and Tom Henighan and Rewon Child and Aditya Ramesh and Daniel M. Ziegler and Jeffrey Wu and Clemens Winter and Christopher Hesse and Mark Chen and Eric Sigler and Mateusz Litwin and Scott Gray and Benjamin Chess and Jack Clark and Christopher Berner and Sam McCandlish and Alec Radford and Ilya Sutskever and Dario Amodei, 2020, ‘Language Models are Few-Shot Learners’, arXiv, https://arxiv.org/abs/2005.14165\n(Fun/horrifying) Hao, Karen, 2020, ‘The messy, secretive reality behind OpenAI’s bid to save the world’, MIT Review, 17 February, https://www.technologyreview.com/2020/02/17/844721/ai-openai-moonshot-elon-musk-sam-altman-greg-brockman-messy-secretive-reality/."
  },
  {
    "objectID": "teaching-nlp.html#assessment",
    "href": "teaching-nlp.html#assessment",
    "title": "Natural Language Processing",
    "section": "Assessment",
    "text": "Assessment\n\nNotebook\n\nDue date: Try to keep this updated weekly, on average, over the course of the term.\nTask: Use Quarto to keep a notebook of what you read in the style of this one by Andrew Heiss.\nWeight: 25 per cent.\n\n\n\nPaper #1\n\nDue date: Thursday, noon, Week 1.\nTask: Donaldson Paper (although vary the dataset, so that it is something related to NLP).\nWeight: 10 per cent.\n\n\n\nPaper #2\n\nDue date: Thursday, noon, Week 6.\nTask: Write a paper applying what you are learning.\nWeight: 25 per cent.\n\n\n\nFinal Paper\n\nDue date: Thursday, noon, Week 12 + two weeks.\nTask: Write a paper that involves doing original research.\nWeight: 40 per cent."
  },
  {
    "objectID": "events-software.html",
    "href": "events-software.html",
    "title": "Conference on statistical software",
    "section": "",
    "text": "Statistical software is an increasingly important aspect of the work of statisticians and data scientists. CANSSI Ontario and the Faculty of Information, at the University of Toronto (U of T), host the CANSSI Ontario Statistical Software Conference to bring together academic and industry participants to share best practices on developing statistical software, exchange ideas for what is needed, and show off their latest software advances."
  },
  {
    "objectID": "events-software.html#section",
    "href": "events-software.html#section",
    "title": "Conference on statistical software",
    "section": "2022",
    "text": "2022\n\n10 November\n\nSameer Deshpande, Assistant Professor, University of Wisconsin, Madison.\n\nTitle: “A new BART prior for flexibly modeling with categorical inputs.”\n\nClara Risk, Ph.D. Student, University of Toronto.\n\nTitle: “An interactive operations research tool for field work site selection in forestry.”\n\nMonica Alexander, Assistant Professor, University of Toronto.\n\nTitle: “Lessons in knowing your audience: statistical software in quantitative social science.”\n\nLisa Lendway, Principal Healthcare Data Scientist, Blue Cross Blue Shield Minnesota.\n\nTitle: “Parameterized Reporting with RMarkdown.”\n\nVincent Arel-Bundock, Associate Professor, Université de Montréal,\n\nTitle: “How to interpret and report estimates from (almost) any R model? A post-estimation workflow with marginaleffects and modelsummary”\n\nAlex Stringer, Assistant Professor, University of Waterloo.\n\nTitle: “Towards Implementing Approximate Inference via Adaptive Quadrature.”\n\nDirk Eddelbuettel, Clinical Professor, University of Illinois Urbana-Champaign; Principal Software Engineer, TileDB.\n\nTitle: “r2u: CRAN as Ubuntu Binaries.”\n\nEmily Riederer, Senior Analytics Manager, Capital One.\n\nTitle: “The Data (error) Generating Process.”\n\nHannah Frick, Software Engineer and Statistician, RStudio.\n\nTitle: “Censored: A tidymodels package for survival models”\n\nZeny Feng, Professor, University of Guelph.\n\nTitle: “A real data driven simulation strategy for selecting an imputation method for mixed-type trait data.”\n\nAriel Mundo Ortiz, Postdoctoral Fellow, Université de Montréal.\n\nTitle: “Reproducible papers in the life sciences using R.”\n\nAlison Presmanes Hill, Director of Knowledge, Voltron Data.\n\nTitle: “The Happiest Notebooks on Earth.”\n\nJohn Fox, Professor Emeritus, McMaster University.\n\nTitle: “Regression Graphics: Added-Variable and Component+Residual Plots.”\n\nJens Von Bergmann, Founder, MountainMath.\n\nTitle: “An ecosystem of R packages to access and process Canadian data.”\n\nAna Trisovic, Research Associate, Harvard University. “Evidence-based practices for better research software.”\nAnjali Silva, Data Analyst and Lecturer, University of Toronto.\n\nTitle: “A Software for Clustering Three-way Count Data Using Mixtures of Matrix Variate Distributions.”\n\nOsvaldo Espin-Garcia, Assistant Professor, Western University.\n\nTitle: “Converting R code into C++, Is it worth it?”\n\nSilvia Canelón, Data Analyst, University of Pennsylvania.\n\nTitle: “Thinking Big with Maps in R: Tips on Wrangling Large Vector Data into Interactive Maps.”\n\nMatthew Watson, Developer/Programmer, Lunenfeld-Tanenbaum Research Institute.\n\nTitle: “cytosel: Interactive cytometry panel design using single-cell RNA-seq.”\n\nMichael Jongho Moon, Ph.D. Student, University of Toronto.\n\nTitle: “mverse: How the R package is designed to help students explore the multiverse.”\n\nSherry Zhang, Ph.D. Student, Monash University.\n\nTitle: “Switching between space and time: Spatio-temporal analysis with cubble.”\n\nWilliam Marshall, Assistant Professor, Brock University.\n\nTitle: “PyPhi: A toolbox for integrated information theory.”\n\nDavid Keyes, Founder, R for the Rest of Us.\n\nTitle: “No Designer Needed: How to Create Beautiful Reports Using Only R.”"
  },
  {
    "objectID": "teaching-applications_of_llms.html",
    "href": "teaching-applications_of_llms.html",
    "title": "Applications of LLMs",
    "section": "",
    "text": "The purpose of this course is to develop students who can:\n\nunderstand Large Language Models (LLMs) sufficiently to be able to use them;\nwork productively to develop and implement an LLM-based application; and\nattract users to that application, to an extent that they are able to conduct some evaluation and write a paper about it.\n\nTo borrow the YCombinator motto - this course develops students who can build something (based on LLMs that) people want.\nStudents are expected to develop:\n\nan understanding of LLMs, and their evolving place in the world;\na hacker mindset focused on building something;\nthe ability to build an application\nexceptional written and verbal communication skills; and\ncontribute in some small way to our understanding of something related to NLP, ideally LLMs."
  },
  {
    "objectID": "teaching-applications_of_llms.html#overview",
    "href": "teaching-applications_of_llms.html#overview",
    "title": "Applications of LLMs",
    "section": "",
    "text": "The purpose of this course is to develop students who can:\n\nunderstand Large Language Models (LLMs) sufficiently to be able to use them;\nwork productively to develop and implement an LLM-based application; and\nattract users to that application, to an extent that they are able to conduct some evaluation and write a paper about it.\n\nTo borrow the YCombinator motto - this course develops students who can build something (based on LLMs that) people want.\nStudents are expected to develop:\n\nan understanding of LLMs, and their evolving place in the world;\na hacker mindset focused on building something;\nthe ability to build an application\nexceptional written and verbal communication skills; and\ncontribute in some small way to our understanding of something related to NLP, ideally LLMs."
  },
  {
    "objectID": "teaching-applications_of_llms.html#pre-requisites",
    "href": "teaching-applications_of_llms.html#pre-requisites",
    "title": "Applications of LLMs",
    "section": "Pre-requisites",
    "text": "Pre-requisites\n\nComfortable with Python, GitHub, APIs, and data science fundamentals."
  },
  {
    "objectID": "teaching-applications_of_llms.html#content",
    "href": "teaching-applications_of_llms.html#content",
    "title": "Applications of LLMs",
    "section": "Content",
    "text": "Content\n\nWeek 1\n\nKarpathy, Andrej, 2013, “Intro to Large Language Models”, YouTube, 22 November, https://youtu.be/zjkBMFhNj_g.\nChollet, François, 2021, Deep Learning with Python, Chapters 1-3.\nTunstall, von Werra and Wolf, 2022, Natural Language Processing with Transformers, Chapters 1 and 2.\n\n\n\nWeek 2\n\nKarpathy, Andrej, 2022, Neural Networks: Zero to Hero, “The spelled-out intro to neural networks and backpropagation: building micrograd”, https://youtu.be/VMj-3S1tku0.\nChollet, François, 2021, Deep Learning with Python, Chapters 4 and 5.\nTunstall, von Werra and Wolf, 2022, Natural Language Processing with Transformers, Chapter 3.\nBolukbasi, Tolga, Kai-Wei Chang, James Y. Zou, Venkatesh Saligrama and Adam T. Kalai, 2016, ‘Man is to Computer Programmer as Woman is to Homemaker? Debiasing Word Embeddings’, Advances in Neural Information Processing Systems, 29 (NIPS 2016), http://papers.nips.cc/paper/6228-man-is-to-computer-programmer-as-woman-is-to-homemaker-d.\n\n\n\nWeek 3\n\nAlammar, Jay, 2020, ‘How GPT3 Works - Visualizations and Animations’, 27 July, https://jalammar.github.io/how-gpt3-works-visualizations-animations/\nKarpathy, Andrej, 2022, Neural Networks: Zero to Hero, “The spelled-out intro to language modeling: building makemore”.\nChollet, François, 2021, Deep Learning with Python, Chapters 6 and 7.\nTunstall, von Werra and Wolf, 2022, Natural Language Processing with Transformers, Chapters 4 and 5.\nHutchinson, Ben, Vinodkumar Prabhakaran, Emily Denton, Kellie Webster, Yu Zhong, and Stephen Denuyl, 2020, ‘Social Biases in NLP Models as Barriers forPersons with Disabilities’, arXiv, https://arxiv.org/abs/2005.00813.\n\n\n\nWeek 4\n\nKarpathy, Andrej, 2022, Neural Networks: Zero to Hero, “Building makemore Part 2: MLP”.\nChollet, François, 2021, Deep Learning with Python, Chapter 11.\nTunstall, von Werra and Wolf, 2022, Natural Language Processing with Transformers, Chapters 6 and 7.\nSolaiman, Irene, Miles Brundage, Jack Clark, Amanda Askell, ArielHerbert-Voss, Jeff Wu, Alec Radford, Gretchen Krueger, Jong Wook Kim, SarahKreps, Miles McCain, Alex Newhouse, Jason Blazakis, Kris McGuffie, Jasmine Wang, 2019, ‘Release Strategies and the Social Impacts of Language Models’,arXiv, https://arxiv.org/abs/1908.09203.\n\n\n\nWeek 5\n\nKarpathy, Andrej, 2022, Neural Networks: Zero to Hero, “Building makemore Part 3: Activations & Gradients, BatchNorm”.\nChollet, François, 2021, Deep Learning with Python, Chapters 12 and 13.\nTunstall, von Werra and Wolf, 2022, Natural Language Processing with Transformers, Chapter 8.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser and Illia Polosukhin, 2017, ‘Attention Is All You Need’, arXiv, http://arxiv.org/abs/1706.03762.\nTatman, Rachel, 2020, ‘What I Won’t Build’, Widening NLP Workshop 2020, Keynote address, 5 July, https://slideslive.com/38929585/what-i-wont-build and http://www.rctatman.com/talks/what-i-wont-build.\n\n\n\nWeek 6\n\nKarpathy, Andrej, 2022, Neural Networks: Zero to Hero, “Building makemore Part 4: Becoming a Backprop Ninja”.\nBoykis, Vicky, What are embeddings?, https://vickiboykis.com/what_are_embeddings/\nRush, Alexander, 2018, ‘The Annotated Transformer’, https://nlp.seas.harvard.edu/2018/04/03/attention.html\nZhao, Jieyu, Tianlu Wang, Mark Yatskar, Vicente Ordonez and Kai-Wei Chang,2017, ‘Men Also Like Shopping: Reducing Gender Bias Amplification using Corpus-level Constraints’, Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pp. 2979–2989, https://aclweb.org/anthology/D17-1323.pdf.\n\n\n\nWeek 7\n\nKarpathy, Andrej, 2022, Neural Networks: Zero to Hero, “Building makemore Part 5: Building a WaveNet”.\nAlammar, Jay, 2018, ‘The Illustrated Transformer’, http://jalammar.github.io/illustrated-transformer/.\nManning, Vaswani and Huang, 2019, ‘Stanford CS224N: NLP with Deep Learning | Winter 2019 | Lecture 14 – Transformers and Self-Attention’, https://www.youtube.com/watch?v=5vcj8kSwBCY&list=PLoROMvodv4rOhcuXMZkNm7j3fVwBBY42z&index=14&ab_channel=stanfordonline.\nUszkoreit, Jakob, 2017, ‘Transformer: A Novel Neural Network Architecture for Language Understanding’, Google AI Blog, 31 August, https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html\n\n\n\nWeek 8\n\nKarpathy, Andrej, 2022, Neural Networks: Zero to Hero, “Let’s build GPT: from scratch, in code, spelled out”.\nBender, Emily M. and Koller, Alexander, 2020, ‘Climbing towards NLU: On Meaning, Form, and Understanding in the Age of Data’, *Proceedings of the 58th Annual Meeting of the\nSeibel Michael, “How to build an MVP”, https://www.youtube.com/watch?v=QRZ_l7cVzzU.\n\n\n\nWeek 9\n\nSeibel Michael, “Building product”, https://www.youtube.com/watch?v=C27RVio2rOs.\nJacob Devlin and Ming-Wei Chang, 2018, ‘Open Sourcing BERT: State-of-the-Art Pre-training for Natural Language Processing’, 2 November, Google AI Blog, https://ai.googleblog.com/2018/11/open-sourcing-bert-state-of-art-pre.html.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, 2018, ‘BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding’, arXiv, https://arxiv.org/abs/1810.04805.\n\n\n\nWeek 10\n\nAlströmer, Gustaf, “How to get your first customers”, https://www.youtube.com/watch?v=hyYCn_kAngI\nTom B. Brown etc, 2020, ‘Language Models are Few-Shot Learners’, arXiv, https://arxiv.org/abs/2005.14165\n\n\n\nWeek 11\n\nNone.\n\n\n\nWeek 12\n\nNone."
  },
  {
    "objectID": "teaching-applications_of_llms.html#assessment",
    "href": "teaching-applications_of_llms.html#assessment",
    "title": "Applications of LLMs",
    "section": "Assessment",
    "text": "Assessment\n\nNotebook\n\nDue date: Updated this weekly.\nTask: Use Quarto to keep a notebook of what you have done the past week and what you want to do the coming week.\nWeight: 10 per cent.\n\n\n\nApplication\n\nDue date: Week 10.\nTask: Create a web application based on LLMs that has paying users.\nWeight: 60 per cent.\n\n\n\nFinal Paper\n\nDue date: Thursday, noon, Week 12 + two weeks.\nTask: Write a paper that involves evaluating some aspect of your application.\nWeight: 30 per cent."
  },
  {
    "objectID": "events-reproducibility.html#overview",
    "href": "events-reproducibility.html#overview",
    "title": "Toronto Workshop on Reproducibility",
    "section": "Overview",
    "text": "Overview\nAn annual workshop focused on reproducibility in data science and statistics. Free and hosted via Zoom. Jointly hosted by CANSSI Ontario and the Data Sciences Institute. Supported by the Faculty of Information and the Department of Statistical Sciences.\nThis conference brings together academic and industry participants on the critical issue of reproducibility in applied statistics and related areas. The conference is free and hosted online. Everyone is welcome, you don’t need to be affiliated with a university.\nThe conference has three broad areas of focus:\n\nEvaluating reproducibility: Systematically looking at the extent of reproducibility of a paper or even in a whole field is important to understand where weaknesses exist. Does, say, economics fall flat while demography shines? How should we approach these reproductions? What aspects contribute to the extent of reproducibility.\nPractices of reproducibility: We need new tools and approaches that encourage us to think more deeply about reproducibility and integrate it into everyday practice.\nTeaching reproducibility: While it is probably too late for most of us, how can we ensure that today’s students don’t repeat our mistakes? What are some case studies that show promise? How can we ensure this doesn’t happen again?"
  },
  {
    "objectID": "events-reproducibility.html#section",
    "href": "events-reproducibility.html#section",
    "title": "Toronto Workshop on Reproducibility",
    "section": "2023",
    "text": "2023\n\nWednesday, 22 February\nToronto Replication Games. Participants will be matched with other researchers working in the same field (e.g., economics, American Politics). Each team will work on replicating a recently published study in a leading econ/poli sci journal.\n\n\nThursday, 23 February\n\nMine Dogucu, University College London & University of California Irvine.\n\nTitle: “Reproducible Teaching in Statistics and Data Science Curricula.”\nAbstract: In reproducibility, we often focus on 1) reproducible research practices and 2) teaching these practices to students. In this talk, I will talk about a third dimension of reproducibility: reproducible teaching. Instructors use tools and adopt practices in preparing their teaching materials. I will discuss how reproducibility relates to these tools and practices. I will share examples from my statistics and data science courses and make recommendations based on teaching experiences.\n\nDebbie Yuster, Ramapo College of New Jersey.\n\nTitle: “Reproducible Student Project Reports with Python + Quarto.”\nAbstract: R users have long enjoyed the ability to render professional looking documents using R Markdown. Output formats include reports, blog posts, presentation slides, books, and more. These documents can contain a mixture of narrative, code, and code output, so they are ideally suited to reproducible work. Results and figures can be generated upon rendering, greatly minimizing the risk of copy/paste errors and outdated results. The benefits of R Markdown are now available to users of Python and Julia, in the form of Quarto, an R Markdown successor. Since Fall 2022, I have required my Data Science students to create their project reports using Python + Quarto. In this talk, I’ll introduce Quarto and some of its features, and report on my students’ experience learning and using it..\n\nMartin Plöderl, Paracelsus Medical University, Salzburg, Austria.\n\nTitle: “Moon and suicide - a case in point example of debunking a likely false positive finding.”\nAbstract: In my presentation, I will summarize the process of replicating a surprising finding of researchers who reported about a statistically significant increase of suicide rates during full moon in northern Finland, but only among younger women, and only in winter. We failed to replicate this finding with much larger samples based on the Austrian and Swedish suicide register. The finding from Finland was likely false positive. I will discuss problematic research and publication practices related to these findings.\n\nDaniel Nüst, CODECHECK & Reproducible AGILE | TU Dresden.\n\nTitle: “Code execution during peer review - you can do it, too!”\nAbstract: Daniel is a research software engineer and postdoc with the Chair of Geoinformatics, TU Dresden, Germany. He develops tools for open and reproducible geoscientific research and is a proponent for open scholarship and reproducibility in the projects NFDI4Earth, o2r, and OPTIMETA and in the CODECHECK initiative.\n\nSam Jordan, TrovBase.\n\nTitle: “Towards greater standardization of reproducibility: TrovBase approach.”\nAbstract: Research code is difficult to understand and build upon because it isn’t standardized; research pipelines are artisan. TrovBase is a data management platform that standardizes the process from dataset configuration to analysis, and does so in a way that makes sharing analysis (and building upon it) easy and fast. The TrovBase team will discuss how to make graphs and analysis maximally reproducible using TrovBase.\n\nRima-Maria Rahal, Max Planck Institute for Research on Collective Goods.\n\nTitle: “Sharing the Recipe: Reproducibility and Replicability in Research Across Disciplines.”\nAbstract: The open and transparent documentation of scientific processes has been established as a core antecedent of free knowledge. This also holds for generating robust insights in the scope of research projects. To convince academic peers and the public, the research process must be understandable and retraceable (reproducible), and repeatable (replicable) by others, precluding the inclusion of fluke findings into the canon of insights. In this contribution, we outline what reproducibility and replicability (R&R) could mean in the scope of different disciplines and traditions of research and which significance R&R has for generating insights in these fields. We draw on projects conducted in the scope of the Wikimedia “Open Science Fellows Program” (Fellowship Freies Wissen), an interdisciplinary, long-running funding scheme for projects contributing to open research practices. We identify twelve implemented projects from different disciplines which primarily focused on R&R, and multiple additional projects also touching on R&R. From these projects, we identify patterns and synthesize them into a roadmap of how research projects can achieve R&R across different disciplines. We further outline the ground covered by these projects and propose ways forward.\n\nLars Vilhuber, Cornell University.\n\nTitle: “Certifiying reproducibility.”\nAbstract: One of the goals of reproducibility - the basis for all subsequent inquiries - is to assure users of a research compendium that it is complete. How do we do that? We re-run code. But what if the data underlying the compendium is confidential (sensitive)? What if it is transient (Twitter)? What if it is so big that it takes weeks to run? All of the above? I will talk about efforts in designing a way to credibly convey that the compendium has run at least once, and the many questions that might arise around that.\n\nClaudia Solis-Lemus, University of Wisconsin-Madison.\n\nTitle: “Accessible reproducibility for biological researchers.”\nAbstract: Reproduciblity is challenging for everyone, but for biological researchers that have not been trained in good computing practices, maintaining a reproducible practice might appear impossible on first glance. We will go over specific strategies for researchers that do not come from computational backgrounds..\n\nSophia Crüwell, University of Cambridge / Charité Medical University Berlin.\n\nTitle: “A Computational Reproducibility Investigation of the Open Data Badge Policy in one Issue of Psychological Science.”\nAbstract: I will present a study that looked at the Open Data badge policy at the journal Psychological Science. We attempted to reproduce 14 articles (at least 3 independent reproduction attempts each) that received the Open Data badge, and found that only 1 was exactly reproducible and 3 further articles were essentially reproducible. I will discuss our results and recommendations for the implementation of Open Data badges as incentives for increasing reproducibility and transparency.\n\nRob Reynolds, KBR / NASA.\n\nTitle: “How to meld open science and reproducibility today to live on Mars tomorrow.”\nAbstract: Rob is a Data Scientist with NASA’s Johnson Space Center in Houston, TX. Originally trained as an epidemiologist and biostatistsician, he helps NASA formalize the process of explaining and quantifying the risks to humans from spaceflight.\n\nJue Hou, University of Minnesota and Jesse Gronsbell, University of Toronto.\n\nTitle: “A common pipeline for curating electronic health records data to enhance reproducibility of real-world evidence studies.”\nAbstract: Electronic health records (EHRs) are becoming a central source of data for biomedical research and have potential to improve our understanding of healthcare delivery and disease processes. However, the analysis of EHR data remains both practically and methodologically challenging as it is recorded as a byproduct of clinical care and not generated for research purposes. In this talk, we will describe the reproducibility challenge in EHR-based research and introduce our ongoing work developing a pipeline for real-world evidence with EHRs.\n\nYanina Bellini Saibene, rOpenSci.\n\nTitle: “Reproducible Open Science for All.”\nAbstract: Open Source and Open Science are global movements, but there is a dismaying lack of diversity in these communities. Non-English speakers and researchers working from the Global South face a significant barrier to being part of these movements. rOpenSci is carrying out a series of activities and projects to ensure our research software serves everyone in our communities, which means it needs to be sustainable, open, and built by and for all groups.\n\nFernando Mayer, Maynooth University.\n\nTitle: “A reproducible workflow and software tools for working with the Global Extreme Sea Level Analysis (GESLA) dataset.”\nAbstract: In this talk, we are going to show a general reproducible workflow, in the context of the project entitled “Estimating sea levels and sea-level extremes for Ireland”. We will demonstrate a set of software tools used to deal with a large, worldwide, sea level dataset, called GESLA (Global Extreme Sea Level Analysis). This workflow and set of tools can hopefully help other researchers in adopting the practice of reproducibility.\n\nMarielle Kirstein, Guttmacher Institute.\n\nTitle: “Qualitative Transparency Tools and Practice in Sexual and Reproductive Health Research.”\nAbstract: Reproducibility is fundamental to the open science movement to ensure science is transparent and accessible, but much of the work on reproducibility has come from quantitative research and data. However, the principles of transparency are equally relevant to qualitative researchers despite some unique challenges implementing transparent practices, given the nature of qualitative data. In this presentation, we will introduce the principles and concepts that underpin qualitative transparency and describe how we at the Guttmacher Institute have been developing and implementing qualitative transparency practices in our work. Guttmacher conducts policy-relevant research on sexual and reproductive health, and our qualitative data often includes sensitive content, underlining the ethical imperative to protect participant confidentiality. We will describe how we have embedded transparency into our qualitative projects through the use of transparency launch meetings and checklists, among other practices, and we will highlight previous and current projects at Guttmacher that are making some aspects of their projects publicly available.\n\nGrace Yu and Emily So, University of Toronto.\n\nTitle: “Evaluating the Reproducibility and Reusability of Transfer Drug Response Workflows.”\nAbstract: With recent advances in molecular profiling and computational technologies, there has been growing interest in developing and using machine learning (ML) and artificial intelligence (AI) techniques in personalized medicine and precision oncology. An active area of research in this domain is focused on the development of computational models capable of predicting therapy response for cancer patients. In a recent publication in Nature Cancer, Ma et al. presented a novel approach, named “Transfer of Cell Line Response Prediction” (TCRP), which utilizes few-shot learning to transfer drug response prediction from immortalized cancer cell line data to more complex in vitro patient-derived cell cultures and in vivo patient-derived xenografts. The authors demonstrated the effectiveness of their method in enabling the development of computational models that can accurately predict drug response in various contexts. Given the impressive results, we aim to address two main issues: (1) validating the performance of the TCRP model in its published context (reproducibility) and (2) extending its applicability to a broader range of preclinical pharmacogenomic and clinical trial data (reusability). The deployment of models such as TCRP will significantly contribute to improving personalized medicine by facilitating the selection of optimal therapy for individual patients based on their molecular profile.\n\nLindsay Katz, University of Toronto.\n\nTitle: “Reproducibility and Dataset Construction: Digitizing the Australian Hansard.”\nAbstract: While approaches to reproducibility in code are well-established, there is less focus on reproducibility in the context of datasets. In this talk, I will introduce an approach to enhancing the reproducibility of dataset construction through automated data testing. My joint work with Dr. Rohan Alexander on digitizing the Australian Hansard will be discussed as a case study, with specific examples of data validation and reproducible practices from our work.\n\nAya Mitani, University of Toronto.\n\nTitle: “Bridging the gap between data availability and reproducibility.”\nAbstract: Journals claim that one of the reasons authors are required to make data available is to facilitate reproducibility in research. However, most of the time, when data are publicly available, they are not in the right format to perform the analysis. In this talk, I share the challenges I have experienced in trying to reproduce the results of some published papers using data sources presented in their data availability statements. I also suggest some ways to improve the reproducibility pipeline, especially when the analytical data set is created from multiple data sources.\n\nNick Radcliffe, Stochastic Solutions.\n\nTitle: “Errors of Interpretation.”\nAbstract: “If our results are to have any useful impact in the world, they not only have to be (broadly) correct they also have to be interpreted correctly, and it’s our responsibility, as data scientists, to maximize the chances that this will be the case. What can we do to increase the likelihood of correct interpretations, and can software help?”.\n\nRob Moss, University of Melbourne.\n\nTitle: “Git is my lab book:”baking in” reproducibility.”\nAbstract: I am part of an infectious diseases modelling group that has informed Australia’s national pandemic preparedness and response plans for the past ~15 years. In collaboration with public health colleagues since 2015, we have developed and deployed near-real-time seasonal influenza forecasts. We rapidly adapted these methods to COVID-19 and, since April 2020, near-real-time COVID-19 forecasts have informed public health responses in Australian states and territories. Ensuring that our results are valid and reproducible is a key aspect of our research. We are also part of a broader consortium whose remit includes building sustainable quantitative research capacity in the Asia-Pacific region. In this talk I will discuss how we are trying to embed reproducible research practices into our EMCR cohort, beginning with version control workflows and normalising peer code review as an integral part of academic research.\n\nMandhri Abeysooriya, Deakin University.\n\nTitle: “The consequences of excel autocorrection on genomic data.”\nAbstract: Erroneous conversions of gene names into other types of data, such as dates and numbers, have been a long-standing issue in computational biology. This issue can have a significant impact on data reproducibility and the advancement of science and technology in the field. While this problem was first identified in 2004 and has been studied extensively in 2016 and 2021, it continues to occur. Through observation, it has been found that gene names can not only be converted to dates and floating numbers, but also to an internal date format of five-digit numbers. This highlights the limitations of using spreadsheets to manage and analyze large genomics data. Spreadsheets can misinterpret gene names as other types of data, leading to inaccuracies and inconsistencies in the data. This can make it challenging to reproduce results and hinder progress in the field. To improve data reproducibility and support the development of science and technology, it may be necessary to consider alternative methods for managing and analyzing large genomics data. In summary, it is crucial to use the appropriate software tools to handle large genomics data to avoid inaccuracies and inconsistencies in the data that can impede the progress of science and technology."
  },
  {
    "objectID": "events-reproducibility.html#section-1",
    "href": "events-reproducibility.html#section-1",
    "title": "Toronto Workshop on Reproducibility",
    "section": "2022",
    "text": "2022\n\nTimetable\n\nWednesday, 23 February 2022\n\n\n\nTime\nSpeaker\nTalk\n\n\n\n\n08:40-09:00\nLisa Strug, University of Toronto\nIntroduction and welcome\n\n\n09:00-09:30\nBenjamin Haibe-Kains, University Health Network\nThe (Not-So-)Hard Path To Transparency and Reproducibility in AI Research\n\n\n09:30-10:00\nColm-cille Caulfield, University of Cambridge\nReproducibility in an Uncertain World: How should academic data science researchers give advice?\n\n\n10:00-10:30\nStephen Eglen, University of Cambridge\nEvaluating the reproducibility of computational results reported in scientific journals\n\n\n10:30-11:00\nValentin Danchev, University of Essex\nReproducibility and Replicability of Large Pre-trained Language Models\n\n\n11:00-11:30\nMonica Alexander, University of Toronto\nReproducibility in Demography: where are we at and where can we go?\n\n\n11:30-12:00\nBreak\n\n\n\n12:00-12:30\nAriel Mundo, University of Arkansas\nStatistics and reproducibility in biomedical research: Why we need both\n\n\n12:30-13:00\nShilaan Alzahawi, Stanford University\nLay perceptions of scientific findings: Swayed by the crowd?\n\n\n13:00-13:30\nBreak\n\n\n\n13:30-14:00\nFernando Hoces de la Guardia, University of California, Berkeley\nSocial Sciences Reproducibility Platform\n\n\n14:00-15:30\nBreak\n\n\n\n15:30-16:00\nCarl Laflamme, YCharOS\nAntibody Characterization through Open Science (YCharOS)\n\n\n16:00-16:30\nRobert Hanisch, National Institute of Standards and Technology and Research Data Alliance\nReproducibility: A Metrology Perspective\n\n\n16:30-17:00\nYann Joly, McGill University\nIncentivizing open data sharing - what’s in it for me!?\n\n\n\n\n\nThursday, 24 February 2022\n\n\n\nTime\nSpeaker\nTalk\n\n\n\n\n08:30-09:00\nJulien Chiquet, Université Paris-Saclay\nComputo: a journal of the French Statistical Society promoting reproductibility\n\n\n09:00-09:30\nNick Radcliffe, Global Open Finance Centre at the University of Edinburgh\nGentest: Automatic Test Generation for Data Science\n\n\n09:30-10:00\nMarkus Fritsch, University of Passau\nTowards reproducible GMM estimation\n\n\n10:00-10:30\nBreak\n\n\n\n10:30-11:00\nAneta Piekut, Sheffield Methods Institute, University of Sheffield\nIntegrating reproducibility into the curriculum of an undergraduate social sciences degree\n\n\n11:00-12:30\nBreak\n\n\n\n12:30-13:00\nJason Hattrick,-Simpers, University of Toronto\nTowards Trust and Reproducibility in Materials AI\n\n\n13:00-13:30\nAya Mitani, University of Toronto\nReproducible, reliable, replicable? In-class exercise using peer-reviewed studies\n\n\n13:30-14:00\nShannon Ellis, UC San Diego\nStructuring & Managing Group Projects in Large-Enrollment Undergraduate Data Science Courses\n\n\n14:00-14:30\nMaria Tackett, Duke University\nKnit, Commit, and Push: Teaching version control in undergraduate statistics courses\n\n\n14:30-15:00\nBreak\n\n\n\n15:00-15:30\nLars Vilhuber, Cornell University\nTeaching for large-scale Reproducibility Verification\n\n\n15:30-16:00\nMichael Geuenich, Lunenfeld Tanenbaum Research Institute and University of Toronto\nWith great data come great pipelines: creating flexible standardized pipelines for common biomedical analysis tasks using Snakemake\n\n\n16:00-16:30\nParaskevi Massara, University of Toronto\nMOSS4Research: A maturity model to evaluate and improve reproducibility in research projects\n\n\n16:30-17:00\nChris Kenny, Harvard University\nReproducible Redistricting\n\n\n17:00-17:30\nDewi Amaliah, Monash University\nReproducible Practice in Taming the Wild Data\n\n\n\n\n\nFriday, 25 February 2022\n\n\n\nTime\nSpeaker\nTalk\n\n\n\n\n09:00-09:30\nMarco Prado, University of Western Ontario\nReproducibility for Behavior Experiments in Basic Science\n\n\n09:30-10:00\nDavid Grubbs and Lara Spieker, CRC Press\nOn book publishing\n\n\n10:00-11:00\nJoelle Pineau, McGill University & Meta (Facebook) AI Research\nImproving Reproducibility in Machine Learning Research\n\n\n11:00-11:30\nDebbie Yuster, Ramapo College of New Jersey\nInfusing Reproducibility into Introductory Data Science\n\n\n11:30-12:00\nColin Rundel, Duke University\nTeaching Statistical computing with Git and GitHub\n\n\n12:00-12:30\nMine Çetinkaya,-Rundel, Duke University and RStudio\nReproducible authoring with Quarto\n\n\n12:30-13:00\nErin Heerey, Western University\nThe Experimenter in the Room\n\n\n13:00-13:30\nJohn McLevey, University of Waterloo\nReproducibility and Principled Data Processing in Python\n\n\n13:30-14:00\nBreak\n\n\n\n14:00-14:30\nKevin Wilson, Brown University and Jake Bowers, University of Illinois at Urbana-Champaign\nSix Tips for Reproducible Field Experiments\n\n\n14:30-15:00\nAbel Brodeur, University of Ottawa\nIntroducing the Institute for Replication\n\n\n15:00-15:30\nAllison Koenecke, Cornell University and Microsoft Research\nReproducible Retrospective Analysis\n\n\n15:30-16:30\nMichael Hoffman, University Health Network and University of Toronto\nReproducibility standards for machine learning in the life sciences\n\n\n\n\n\n\nPresenter biographies and abstracts\n\nKeynotes\n\nJoelle Pineau\n\nTitle: Improving Reproducibility in Machine Learning Research Findings from the NeurIPS Reproduciblity Program and the ML Reproducibility Challenge\nBiography: Joelle Pineau is an Associate Professor and William Dawson Scholar at the School of Computer Science at McGill University, where she co-directs the Reasoning and Learning Lab. She is a core academic member of Mila and a Canada CIFAR AI chairholder. She is also co-Managing Director of Facebook AI Research. She holds a BASc in Engineering from the University of Waterloo, and an MSc and PhD in Robotics from Carnegie Mellon University. Dr. Pineau’s research focuses on developing new models and algorithms for planning and learning in complex partially-observable domains. She also works on applying these algorithms to complex problems in robotics, health care, games and conversational agents. She serves on the editorial board of the Journal of Machine Learning Research and is Past-President of the International Machine Learning Society. She is a recipient of NSERC’s E.W.R. Steacie Memorial Fellowship (2018), a Fellow of the Association for the Advancement of Artificial Intelligence (AAAI), a Senior Fellow of the Canadian Institute for Advanced Research (CIFAR), a member of the College of New Scholars, Artists and Scientists by the Royal Society of Canada, and a 2019 recipient of the Governor General’s Innovation Awards.\n\nMichael Hoffman\n\nTitle: Reproducibility standards for machine learning in the life sciences\nAbstract: To make machine-learning analyses in the life sciences more computationally reproducible, we propose standards based on data, model and code publication, programming best practices and workflow automation. By meeting these standards, the community of researchers applying machine-learning methods in the life sciences can ensure that their analyses are worthy of trust.\nBiography: Michael Hoffman creates predictive computational models to understand interactions between genome, epigenome, and phenotype in human cancers. His influential machine learning approaches have reshaped researchers’ analysis of gene regulation. These approaches include the genome annotation method Segway, which enables simple interpretation of multivariate genomic data. He is a Senior Scientist at Princess Margaret Cancer Centre and Associate Professor in the Departments of Medical Biophysics and Computer Science, University of Toronto. He was named a CIHR New Investigator and has received several awards for his academic work, including the NIH K99/R00 Pathway to Independence Award, and the Ontario Early Researcher Award.\n\n\n\n\nInvited talks\n\nAbel Brodeur\n\nTitle: Introducing the Institute for Replication\nBiography: Abel Brodeur is an associate professor in the department of economics at the University of Ottawa. He is the chair of the Institute for Replication (I4R), which he founded in January 2022. I4R works to improve the credibility of science by systematically reproducing and replicating research findings in leading academic journals.\n\nAllison Koenecke\n\nTitle: Reproducible Retrospective Analysis\nBiography: Allison Koenecke is a postdoc at Microsoft Research in the Machine Learning and Statistics group, and starting Summer 2022 will be an Assistant Professor of Information Science at Cornell University. Her research primarily spans two domains: algorithmic fairness in online services, and causal inference in public health. Previously, she received her PhD from Stanford’s Institute for Computational & Mathematical Engineering.\n\nAneta Piekut\n\nTitle: Integrating reproducibility into the curriculum of an undergraduate social sciences degree\nAbstract: While appreciation for reproducibility and research transparency in social sciences research has grown substantially recently, teaching research reproducibility is still less common, especially at the undergraduate level. Crucially, teaching reproducible research to undergraduate students requires sequencing various open science skills across the curriculum and normalising reproducible research for students. In the talk I will discuss a reproducibility assignment implemented in an undergraduate-level advanced Quantitative Social Sciences course. As part of the assignment, students reproduced a model in a paper published in a high-impact social science journal, added a small extension, and published it as a reproducible report online. I will reflect on the lessons learnt from teaching several interactions of the module and whether one stand-alone ‘replication project’ module is enough to change students’ practice.\nBiography: Sociologist specialising in migration and ethnic studies, including measurement of attitudes, migrant integration and segregation. At Sheffield Methods Institute, University of Sheffield, Aneta provides training to undergraduate and postgraduate students in advanced quantitative methods, survey methodology and mixed-method methodology. Aneta is committed to teaching reproducible research methods; in 2020 she was Project TIER Fellow (https://www.projecttier.org/), and in 2021 joined its Executive Committee.\n\nAriel Mundo\n\nTitle: Statistics and reproducibility in biomedical research: Why we need both\n\nAbstract: The biomedical field still struggles at large to make research reproducible. In this talk, I argue that part of this problem is that most of us in biomedical research do not seem to realize the importance of choosing appropriate Statistical models for our data, and how this in turn enables reproducibility. Moreover, I also argue that we need a “statistical rethinking” in biomedical research in order to establish reproducibility as a core aspect of our work.\nBiography: Ariel Mundo is a Fulbright alum and PhD Candidate in the Department of Biomedical Engineering at the University of Arkansas. His work focuses on the longitudinal study of changes in cancer metabolism using optical and molecular tools, and the use of semi-parametric methods to analyze such data. He is also an R enthusiast and avid reader.\n\nAya Mitani\n\nTitle: Reproducible, reliable, replicable? In-class exercise using peer-reviewed studies\n\nAbstract: I will share my experience in preparing and implementing an in-class exercise to reproduce the results from peer-reviewed publications in health science journals. The course, titled Analysis of Correlated Data, enrolls 20 students mostly pursuing a Master of Science degree in biostatistics. Challenges include finding a suitable clustered or longitudinal study that provides original data and translating the information given (and not given) in the “Methods” section into actual code. Through this exercise, students learn whether the results are not only reproducible but reliable, and whether the analysis can be replicated on a different set of data. The goal through this exercise is to teach the students how to write an applied manuscript or report as modern biostatisticians.\n\nBiography: I am an Assistant Professor in the Division of Biostatistics at the Dalla Lana School of Public Health (DLSPH) of the University of Toronto. I obtained my Ph.D. in Biostatistics from Boston University and did my postdoctoral research fellowship at Harvard T. H. Chan School of Public Health. My research includes the development of statistical methods for complex oral health data, multiple imputation for missing data, modelling agreement in cancer screening studies, and biased sampling designs in surveys and observational studies. At DLSPH, I teach Analysis of Correlated Data and Introduction to Joint Modeling in Health Research. I am passionate about incorporating good reproducible research practices into my teaching. In 2021, I co-founded the Health Data Working Group at DLSPH to provide an accessible space for students and researchers to learn about data and coding outside of the classroom. I live in Etobicoke with my husband and two children.\n\nBenjamin Haibe-Kains\n\nTitle: The (Not-So-)Hard Path To Transparency and Reproducibility in AI Research\n\nAbstract: As artificial intelligence (AI) becomes a method of choice to analyze biomedical data, the field is facing multiple challenges around research reproducibility and transparency. Given the proliferation of studies investigating the applications of AI in research and clinical studies, it is essential for independent researchers to be able to scrutinize and reproduce the results of a study using its materials, and build upon them in future studies. Computational reproducibility is achievable when the data can easily be shared and the required computational resources are relatively common. However, the complexity of AI algorithms and their implementation, the need for specific computer hardware and the use of sensitive biomedical data represent major obstacles in healthy-related AI research. In this talk, I will describe the various aspects of an AI biomedical study that are necessary for reproducibility and the platforms that exist for sharing these materials with the scientific community.\n\nBiography: Dr. Benjamin Haibe-Kains is a Senior Scientist at the Princess Margaret Cancer Centre (PM), University Health Network, and Associate Professor in the Medical Biophysics department of the University of Toronto. Dr. Haibe-Kains earned his PhD in Bioinformatics at the Université Libre de Bruxelles (Belgium). Supported by a Fulbright Award, he did his postdoctoral fellowship at the Dana-Farber Cancer Institute and Harvard School of Public Health (USA). Dr. Haibe-Kains’ research focuses on the integration of high-throughput data from various sources to simultaneously analyze multiple facets of carcinogenesis. Dr. Haibe- Kains’ team is analyzing large-scale radiological and (pharmaco)genomic datasets to develop new prognostic and predictive models to improve cancer care.\n\nCarl Laflamme\n\nTitle: Antibody Characterization through Open Science (YCharOS)\nAbstract: Global sales of commercial antibodies are estimated at $2 billion per year with approximately half that money wasted on underperforming reagents. Both public and private sectors agree that a robust, independent, and scalable process to characterize commercial antibodies is required, but all attempts to find a solution have failed due to the tangle of conflicting interests in both academia and industry. YCharOS (Antibody Characterization using Open Science), in collaboration with the Structural Genomics Consortium (SGC) and the Montreal Neurological Institute (The Neuro, McGill University) has created an open science ecosystem in which antibody manufacturers, knockout cell line providers, academics, pharma and granting agencies contribute resources and knowledge to solve the antibody liability crisis. We have already publicly shared the identification of hundreds of high-performing antibodies for dozens of neuroscience targets. We have scaled up our platform, developed automation and expanded our team. We now aim to characterize antibodies for the human proteome.\n\nChris Kenny\n\nTitle: Reproducible Redistricting\n\nAbstract: Modern redistricting is known for occurring behind closed doors where incumbent politicians can work to advance their co-partisan’s interests. Recent advancements in political science and statistical research have developed the tools to help resolve these problems. I overview the R-package-based workflow that the ALARM Project and its members use for research, advocacy, and testimony to courts. Key packages developed for these purposes include redist, redistmetrics, and geomander.\nBiography: Chris Kenny is a Ph.D. candidate in the Department of Government at Harvard University, studying American Politics and Political Methodology. He is currently the Political Science Pre-Doctoral Fellow at the Harvard Election Law Clinic. His substantive focus is on redistricting and gerrymandering. He primarily develops open-source R tools for analyzing redistricting and voting rights in geographic and contemporary contexts. He is an affiliate with the Center for American Political Studies at Harvard University, The Institute for Quantitative Social Science, and the Algorithm-Assisted Redistricting Methodology (ALARM) Project.\n\nColin Rundel\n\nTitle: Teaching Statistical computing with Git and GitHub\n\n\nColm-cille Caulfield\n\nTitle: Reproducibility in an Uncertain World: How should academic data science researchers give advice? open science-type initiatives\n\n\nDavid Grubbs and Lara Spieker\n\nTitle: On book publishing\n\nAbstract: In this very practical and interactive workshop, four book editors from Chapman and Hall/CRC will discuss why you should consider publishing an R or Data Science book and why you should work with CRC. The editors will go over the publishing process and provide best practices for shaping your ideas and submitting a book proposal; discuss their bestsellers and popular series as well as emerging topics and trends. The lively discussion will provide plenty of opportunities for the attendees to ask questions and discuss ideas.\n\nDebbie Yuster\n\nTitle: Infusing Reproducibility into Introductory Data Science\nAbstract: In this talk, I will discuss the role of reproducibility in my Introduction to Data Science course. The course has no prerequisites, so many students are coding and analyzing data for the first time. They develop habits of reproducibility from the start: their analyses are done within R Markdown documents, and GitHub is used to facilitate both version control and collaboration among teammates. Through scaffolded coding exercises, gradual onboarding to GitHub, and focusing on a small subset of GitHub functionality, even beginner students can become adept at using these technologies. I will also discuss tips learned from teaching the course in a fully remote format, and will provide pointers to training resources for instructors who want to use similar tools and workflows in their own courses.\nBiography: Debbie Yuster is an Assistant Professor of Data Science and Mathematics at Ramapo College of New Jersey. She holds a Ph.D. in Mathematics from Columbia University. Prior to joining Ramapo, Debbie served as a math professor at SUNY Maritime College, earning the SUNY Chancellor’s Award for Excellence in Teaching. Debbie served as a Visiting Data Science Scholar at the Wall Street Journal, and has cultivated industry partnerships leading to undergraduate research projects. She also has an interest in K-12 STEM outreach, having worked with secondary school teachers and students for many years.\n\nDewi Amaliah\n\nTitle: Reproducible Practice in Taming the Wild Data\nAbstract: I will talk about my experience in refreshing the wages data from the prominent survey, NLSY79, which is used as an example of longitudinal data in a textbook (Singer and Willet, 2003). The motivation of this study is to demonstrate the steps (extracting, tidying, cleaning, and exploring) and clearly articulate the decision made when data is refreshed from the raw (wild) to the textbook (tame) data. All of those steps are documented to ensure reproducibility.\n\nErin Heerey\n\nTitle: The Experimenter in the Room\n\nFernando Hoces de la Guardia\n\nTitle: Social Sciences Reproducibility Platform Social Sciences Reproducibility Platform\n\n\nKevin Wilson and Jake Bowers\n\nTitle: Six Tips for Reproducible Field Experiments\n\nJason Hattrick-Simpers\n\nTitle: Towards Trust and Reproducibility in Materials AI\nBiography: Jason Hattrick-Simpers is a Professor at the Department of Materials Science and Engineering, University of Toronto, and a Research Scientist at CanmetMATERIALS. He graduated with a B.S. in Mathematics and a B.S. in Physics from Rowan University and a Ph.D. in Materials Science and Engineering from the University of Maryland. Prior to joining UofT Prof. Hattrick-Simpers was a staff scientist at the National Institute of Standards and Technology (NIST) in Gaithersburg, MD where he co-developed tools for discovering novel corrosion resistance of alloys, developed active learning approaches to guide thin film and additive manufacturing alloy studies, and developed tools and best practices to enable trust in AI within the materials science community.\n\nJohn McLevey\n\nTitle: Reproducibility and Principled Data Processing in Python\n\nJulien Chiquet\n\nTitle: Computo: a journal of the French Statistical Society promoting reproductibility\n\nAbstract: This talk will present Computo (https://computo.sfds.asso.fr/), an academic journal that has just been born, which calls for higher standards in the publication of scientific results. In order to achieve this goal, Computo goes beyond classical static publications by leveraging technical advances in literate programming and scientific reporting. Computo focuses on computational and algorithmic methodological contributions to the field of statistics and machine learning. The journal is designed to allow authors to demonstrate the usefulness of their methods for data analysis, but also to promote the numerical illustration of theoretical properties. In the era of the reproducibility crisis, Computo differs from other journals in the centrality given to the issues of replicability and open science: - Computo is distributed solely online, free for authors and readers; - It systematically makes available the exchanges between authors and reviewers, the latter being able to choose to remain anonymous; - Computo uses an original publication format that guarantees the reproducibility of results: articles are submitted and published in the form of interactive documents (“notebook” integrating text, code, equations and bibliographic references), associated with a github repository configured to demonstrate, dynamically and durably, the reproducibility of the contribution. On the Computo submission page, we offer various templates to prepare your submissions, as well as an example of a finalized article and the associated repository.\nBiography: Julien Chiquet, editor of Computo, is a senior researcher in statistical learning. He is supported for this project by co-editors Chloé Azencott, Pierre Neuvial and Nelle Varoquaux, all researchers in machine learning and statistics\n\nLars Vilhuber\n\nTitle: Teaching for large-scale Reproducibility Verification\n\nAbstract: We describe a unique environment in which undergraduate students from various STEM and social science disciplines are trained in data provenance and reproducible methods, and then apply that knowledge to real, conditionally accepted manuscripts and associated replication packages. We describe in detail the recruitment, training, and regular activities. While the activity is not part of a regular curriculum, the skills and knowledge taught through explicit training of reproducible methods and principles, and reinforced through repeated application in a real-life workflow, contribute to the education of these undergraduate students, and prepare them for post-graduation jobs and further studies.\nBiography: Lars Vilhuber holds a Ph.D. in Economics from Université de Montréal, Canada, and is currently on the faculty of the Cornell University Economics Department. He has interests in labor economics, statistical disclosure limitation and data dissemination, and reproducibility and replicability in the social sciences. He is the Data Editor of the American Economic Association, and Managing Editor of the Journal of Privacy and Confidentiality.\n\nLisa Strug\n\nTitle: Introduction and overview\nBiography: Dr. Strug is Professor in the Departments of Statistical Sciences, Computer Science and cross-appointed in Biostatistics at the University of Toronto and is a Senior Scientist in the Program in Genetics and Genome Biology at the Hospital for Sick Children. Dr. Strug is the inaugural Director of the Data Sciences Institute (DSI), a tri-campus, multi-divisional, multi-institutional, multi-disciplinary hub for data science activity at the University of Toronto and affiliated Research Institutes. The DSI’s goal is to accelerate the impact of data sciences across the disciplines to address pressing societal questions and drive positive social change. Dr. Strug holds several other leadership positions at the University of Toronto including the Director of the Canadian Statistical Sciences Institute Ontario Region (CANSSI Ontario), and at the Hospital for Sick Children as Associate Director of the Centre for Applied Genomics and the Lead of the Canadian Cystic Fibrosis Gene Modifier Consortium and the Biology of Juvenile Myoclonic Epilepsy International Consortium. She is a statistical geneticist and her research focuses on the development of novel statistical approaches to analyze and integrate multi-omics data to identify genetic contributors to complex human disease. She has received several honours including the Tier 1 Canada Research Chair in Genome Data Science.\n\nMarco Prado\n\nTitle: Reproducibility for Behavior Experiments in Basic Science\nBiography: Marco Prado is scientist at the Robarts Research Institute and a full professor at the University of Western Ontario, where he holds a Canada Research Chair in Neurochemistry of Dementia. He is interested in understanding how neurochemical alterations in neurodegenerative diseases contribute to protein misfolding and cognitive dysfunction. He has made contributions to understanding maladaptive signaling in Alzheimer’s and Prion diseases by investigating physiological functions of the prion protein and in how molecular chaperones affect signaling and protein misfolding in neurodegenerative diseases. He has developed multiple genetic mouse models of neurochemical dysfunction in dementia. Marco’s group combines the use of sophisticated touchscreen tests of high-level cognition and detailed biochemical analysis to reveal several mechanisms regulating executive function and mechanisms of pathological changes in mouse models. He is currently spearheading with several colleagues an Open Science Repository (www.mousebytes.ca) for high-level cognitive data in mouse models of neurodegenerative disease. This effort will support a community of more than 300 laboratories to increase reproducibility and replicability of cognitive datasets in pre-clinical research. Marco Prado received several awards, including the Guggenheim Fellowship, for his work and has published over 200 manuscripts.\n\nMaria Tackett\n\nTitle: Knit, Commit, and Push: Teaching version control in undergraduate statistics courses\nAbstract: In recent years there has been increased focus on incorporating the skills required to conduct well-documented and reproducible analyses in the undergraduate statistics curriculum. Because data analysis is an iterative process, version control, a record of changes to a set of files over time, is a foundational part of a reproducible workflow. In this talk, I will describe how version control with Git can be included as a learning objective in the first and second statistics courses. I’ll discuss strategies for introducing version control to students, incorporating it in individual and team-based assignments, and assessing students’ understanding. I’ll also share lessons learned and an example of how this can be implemented using RStudio and GitHub.\nBiography: Maria Tackett is an Assistant Professor of the Practice in the Department of Statistical Science at Duke University. Prior to joining the faculty at Duke, Maria earned a Ph.D. in Statistics from the University of Virginia and worked as a statistician at Capital One. Her work focuses on using active learning strategies to increase engagement in large undergraduate statistics courses, and understanding how classroom practices impact students’ sense of community in these courses. Maria is active in the statistics education community, including serving as the current Communications Officer for the ASA Section on Statistics and Data Science Education.\n\nMarkus Fritsch\n\nTitle: Towards reproducible GMM estimation\n\nAbstract: Generalized method of moments (GMM) estimation is a way forward in regression setups where endogeneity is present. A practically relevant area of application is the estimation of linear dynamic panel data models. This context forces the researcher to make many decisions that seem marginal at first, but which often affect the estimation and inference dramatically. The decisions comprise the number and type of employed moment conditions, their weighting scheme, how covariates and/or dummy variables are included, whether we iterate the estimation procedure and/or bias-correct, etc. Due to the many possible choices, clear documentation and reproducibilty are vital for the communication of GMM estimation results. We provide guidelines for reproducible GMM estimation and demonstrate their relevance by replicating and extending several empirical applications.\n\nBiography: Markus Fritsch is Assistant Professor at the Chair of Statistics and Data Analytics of the University of Passau. He is the creator of the CRAN package pdynmc. His research interest include Data Science & Statistical Learning, GMM estimation, Quantile regression, and reproducible Applied Statistics.\n\nMichael Geuenich\n\nTitle: With great data come great pipelines: creating flexible standardized pipelines for common biomedical analysis tasks using Snakemake\n\nAbstract: Biomedical data analysis pipelines are becoming increasingly complex as projects frequently involve the analysis of raw data from distinct batches and experimental modalities. Work frequently starts with processing and normalizing several large datasets in a variety of ways, often requiring custom filtering approaches for each individual dataset. Existing and novel analysis methods are then frequently applied to the processed data using a variety of parameters prior to subsequent benchmarking, resulting in many individual analysis steps that need to be tied together. Importantly, some data processing steps are frequently dependent on the data itself, requiring inspection of preliminary results before being able to run a standardized pipeline in full. In addition, pre-processing steps are frequently revised as part of the iterative analysis workflow common to most projects, thus requiring downstream analyses to be re-run as input data changes. These challenges make it cumbersome and error prone to run individual analysis steps manually. Workflow managers such as Snakemake allow for the creation of reproducible and easily\nBiography: Michael is a PhD student in the computational track of the molecular genetics department at the University of Toronto and at the Lunenfeld Tanenbaum Research Institute with Kieran Campbell. His work focusses on better understanding immune escape in pancreatic cancer using machine learning tools and a diverse set of -omics data.\n\nMine Çetinkaya-Rundel\n\nTitle: Reproducible authoring with Quarto\nBiography: I am a Professor of the Practice and the Director of Undergraduate Studies at the Department of Statistical Science and an affiliated faculty in the Computational Media, Arts, and Cultures program at Duke University. My work focuses on innovation in statistics and data science pedagogy, with an emphasis on computing, reproducible research, student-centered learning, and open-source education. I work on integrating computation into the undergraduate statistics curriculum, using reproducible research methodologies and analysis of real and complex datasets. In addition to my academic position, I also work with RStudio, where I focus primarily on education for open-source R packages as well as building resources and tools for educators teaching statistics and data science with R and RStudio.\n\nMonica Alexander\n\nTitle: Reproducibility in Demography: where are we at and where can we go?\n\nBiography: Monica Alexander is an Assistant Professor in Statistical Sciences and Sociology at the University of Toronto. Her research focuses on developing statistical methods to help measure demographic and health outcomes. She received a PhD in Demography and Masters in Statistics from the University of California, Berkeley. She has worked on research projects with organizations such as UNICEF, the World Health Organization, the Bill and Melinda Gates Foundation, and the Human Mortality Database.\n\nNick Radcliffe\n\nTitle: Gentest: Automatic Test Generation for Data Science\n\nAbstract: This talk will focus on reference tests—scripts that test the ongoing correctness of scripts, programs and pipelines with a particular focus on data science-oriented tasks. The TDDA library has long offered support to allow humans to write useful tests for data science workflows, with a focus on supporting tests for what might be called semantic/functional correctness, rather than syntactic/form correctness. New “Gentest” functionality in TDDA goes further by automating large parts of test production. Using Gentest, researchers can concentrate on developing robust/correct analysis pipelines, verifying them in the usual way (probably by hand), and then use Gentest to generate executable tests automatically. Although Gentest is written in Python, it can be also used to generate tests for R or almost any other language. If all goes well, this talk will include a demonstration of automatically generating tests for R scripts.\n\nBiography: Nick Radcliffe is the founder of the data science consulting and software firm, Stochastic Solutions Limited, the Interim Chief Scientist at the Global Open Finance Centre of Excellence, and a Visiting Professor in Maths and Stats at University of Edinburgh, Scotland. His background combines theoretical physics, operations research, machine learning and stochastic optimization. Nick’s current research interests include a focus on test-driven data analysis, (an approach to improving correctness of analytical results that combines ideas from reproducible research and test-driven development) and privacy-respecting analysis. He is the lead author of the open-source Python tdda package, which provides practical tools for testing analytical software and data, and also of the Miró data analysis suite.\n\nParaskevi Massara\n\nTitle: MOSS4Research: A maturity model to evaluate and improve reproducibility in research projects.\n\nAbstract: Our ability to gather large amounts of data, store it and analyze it efficiently has created new research opportunities in health sciences and it has led to novel practices. One such practice is the creation of large datasets that can be used in multiple studies effectively increasing our research output. However, big data is no free lunch and it comes with its own challenges. On one hand, improper management of data may lead to problems when communicating or sharing data. Different terminology, inaccessible storage, ethical/economic/social barriers may be some of the problems related to sharing the common large datasets. On the other hand, improper management is not constrained only in data, but can extend to the analysis as well, where processes or analytical tasks are not properly documented or permanently stored. These problems significantly inhibit the reproducibility of studies, which in turn may make the verification of research results practically impossible, and they can also lead to waste in terms of lost data, time, effort and funds. Other practical domains, such as computer science or engineering, have long employed methods to systematically document data and processing tasks to allow for repetition and reproducibility. Based on such methods, we propose a novel framework to evaluate the maturity of the reproducibility practices employed in the context of individual projects or within an entire research team. The framework consists of a self-assessment questionnaire and a maturity model to allow teams to evaluate the maturity of their responsibility practices and a guide on how to increase their maturity level. The guide contains practices drawn from other domains to improve communication, collaboration and reproducibility.\n\nBiography: Paraskevi Massara is a PhD candidate supervised by Drs. Elena Comelli and Robert Bandsma. Her research interests include growth pattern detection in children in association with gut microbiome. She is a coding enthusiast and an aspiring data science have extensive practical experience with programming, machine learning and statistics, and development and management platforms such as Github. She is a member of R ladies and Women Who Code. She is the recipient among others of Ontario Graduate Scholarship, Peterborough Hunter Charitable Foundation Graduate Award, Connaught International Scholarship.\n\nRobert Hanisch\n\nTitle: Reproducibility: A Metrology Perspective\nBiography: Dr. Robert J. Hanisch is the Director of the Office of Data and Informatics in the Material Measurement Laboratory at NIST. Prior to this appointment (July 2014) he was a Senior Scientist at the Space Telescope Science Institute (STScI), Baltimore, Maryland, and Director of the US Virtual Astronomical Observatory. In the past twenty-five years Dr. Hanisch has led many efforts in the astronomy community in the area of information systems and services, focusing particularly on efforts to improve the accessibility and interoperability of data archives and catalogs. He was the first chair of the International Virtual Observatory Alliance Executive Committee (2002-2003) and continues as a member of the IVOA Executive. From 2000 to 2002 he served as Chief Information Officer at STScI, overseeing all computing, networking, and information services for the Institute. Prior to that he had oversight responsibilities for the Hubble Space Telescope Data Archive and led the effort to establish the Multimission Archive at Space Telescope—MAST—as the optical/UV archive center for NASA astrophysics missions. He has served as chair of the Program Organizing Committee for the Astronomical Data Analysis Software and Systems (ADASS) conferences, chair of the Astrophysics Data Centers Coordinating Committee, and co-chair of the Decadal Survey Study Group on Computation, Simulation, and Data Handling. He is currently president of IAU Commission 5 (Data and Documentation), chair of the IAU Comm. 5 Working Group on Virtual Observatories, Data Centers, and Networks, and co-chair of the Comm. 5 Working Group on Libraries. He completed his Ph.D. in Astronomy in 1981 at the University of Maryland, College Park, working in the field of extragalactic radio astronomy with Prof. William Erickson.\n\nShannon Ellis\n\nTitle: Structuring & Managing Group Projects in Large-Enrollment Undergraduate Data Science Courses\nAbstract: Computational notebooks are a popular tool for generating technical data science reports, as they allow for narrative text, code, and code outputs in a single explanatory document. Given their popularity, many data science courses utilize computational notebooks for instruction, assignments, and projects, the output of which can be analyzed to better understand student behavior and improve instruction. Here, we present the results from the analysis of 686 final group data science projects from 8 iteractions of the undergraduate course COGS 108 Data Science in Practice to explain how students approach open-ended data science projects and provide data science instructors with general recommendations on structuring and managing reproducible data science projects in large-enrollment data science courses.\n\nBiography: Shannon E. Ellis is an Assistant Teaching Professor at UC San Diego in the Cognitive Science Department, where her primary focus is teaching programming and data science to thousands of undergraduate students each academic year. Prior to her arrival at UC San Diego, Shannon received her Ph.D. in Human Genetics from the Johns Hopkins School of Medicine and completed a postdoctoral fellow in the Department of Biostatistics at the Johns Hopkins Bloomberg School of Public Health. Shannon is particularly passionate about data science, ethical data analysis, and education. She aims to ensure that data science education is accessible to everyone, with a particular focus on individuals from marginalized groups who typically have not had access to such materials and training.\n\nShilaan Alzahawi\n\nTitle: Lay perceptions of scientific findings: Swayed by the crowd?\nAbstract: Every day, important scientific findings are rejected at large. To increase public faith in science, some have proposed the use of crowd science. Drawing from theories on social norms and numerical cognition, we test whether crowd science improves lay perceptions of scientific findings. We run an experiment (N = 2,019; preregistration, data, code, and materials at osf.io/vedb4) to study the effects of scientific findings emerging from a crowd of researchers (vs. a typical research collaboration) on lay consumers’ posterior beliefs, confidence in an aggregate effect size estimate, and ratings of credibility, bias, and error. We focus on crowdsourced data analysis: a crowd of scientists who independently analyze the same data to estimate and report a parameter of interest. Contrary to our hypotheses, we do not find that consistent crowd estimates increase the sway and credibility of scientific findings to lay consumers: instead, to our surprise, they lead to lower posterior beliefs and higher ratings of error. In the future, it is important for crowd scientists to consider how to tackle science skepticism and effectively communicate variable crowd estimates to lay consumers.\n\nBiography: Shilaan Alzahawi is a Master student in Statistics at Ghent University and a PhD candidate in Organizational Behavior at Stanford University. Shilaan is interested in meta-science and inferential statistics, with a particular interest in the coordination and effectiveness of large-scale science collaborations.\n\nStephen Eglen\n\nTitle: Evaluating the reproducibility of computational results reported in scientific journals\nAbstract: A recent study (&lt; http://dx.doi.org/10.1371/journal.pbio.3001107&gt;) estimated that only 2% of biomedical articles shared code relating to computations. This lack of sharing of code inhibits reproducibility of findings and reusability of methods. I will introduce our CODECHECK project &lt; https://codecheck.org.uk&gt; that reviews computational findings underlying research articles in biosciences. Compared to traditional peer review, this review is open and interactive, with the aim of helping all authors make their work reproducible. All code/data required to reproduce computational results, and the results themselves, are shared freely following FAIR guidelines. We hope our system will be used across multiple publishers and bring a cultural change towards more transparent, open, and reusable computational workflows. This is joint work with Daniel Nust.\nBiography: SJE is Professor of Computational Neuroscience, in the Department of Applied Mathematics and Theoretical Physics, University of Cambridge. He has a long-standing interest in open science and reproducible research. He co-leads the CODECHECK project for reproducibility of computations in scientific publications (https://codecheck.org.uk). He is an associate editor for BiorXiv and is on advisory boards for F1000 Research and Gigabyte.\n\nValentin Danchev\n\nTitle: Reproducibility and Replicability of Large Pre-trained Language Models\nAbstract: A major recent development in artificial intelligence and deep learning research are large language models (LLMs) (e.g., BERT, GPT-3, Gopher) that are trained on a massive amount of language data and are subsequently applied to a wide range of downstream tasks. Over the last couple of years, LLMs have been adopted and have shown promise across research domains, pointing to the importance of evaluating the scientific potential and challenges of these models through the lenses of research transparency, computational reproducibility, and replicability. While challenges for reproducibility and replicability in data-intensive computational applications are not new, pre-trained LLMs built on deep learning approaches bring some novel epistemic challenges as well as related ethical and social risks. Specifically, the massive and often sensitive, publicly unavailable, and proprietary data sets on which these models are pre-trained; the scale of the models with hundreds of billions of parameters and associated computationally intensive infrastructure; and the pre-trained nature of the models forming a basis for subsequent applications in the context of restricted access to many of the models, their software, and training procedures can all pose challenges to research transparency, computational reproducibility, and replicability. I will discuss these challenges and outline possible improvements drawing on principles of responsible and reproducible research and on recent frameworks and practices in data-intensive computational sciences aiming to securely access and model sensitive data at scale.\n\nBiography: Valentin Danchev is a Lecturer in Computational Social Science at the University of Essex and a Fellow of the Software Sustainability Institute. He holds a DPhil from the University of Oxford and held postdoctoral positions at the University of Chicago and the Stanford University School of Medicine. His research combines computational methods from data science and network analysis with approaches from reproducible research and metascience to study the transparency, reproducibility, bias, and social impact of data-intensive research, with a current focus on evaluating and improving the transparency and reproducibility of applications of data science, artificial inteligence, and machine learning in the social and health sciences. In another stream of research, he uses computational social science and network analysis to examine health-related misinformation, digital-health interventions, and inequality in network structures of global migration. He teaches data science with an emphasis on open reproducible workflows and responsible analysis of real-world data.\n\nYann Joly\n\nTitle: Incentivizing open data sharing - what’s in it for me!?"
  },
  {
    "objectID": "events-reproducibility.html#section-2",
    "href": "events-reproducibility.html#section-2",
    "title": "Toronto Workshop on Reproducibility",
    "section": "2021",
    "text": "2021\n\nTimetable\n\nThursday, 25 February, 2021\n\n\n\n\n\n\n\n\n\nTime\nSpeaker\nFocus\nRecording\n\n\n\n\n9:00-9:10am\nRohan Alexander, University of Toronto\nWelcome\n-\n\n\n9:10-9:20am\nRadu Craiu, University of Toronto\nOpening remarks\nhttps://youtu.be/JGGVEgMBURU\n\n\n9:20-9:30am\nWendy Duff, University of Toronto\nOpening remarks\nhttps://youtu.be/Z3aWU1A0FCw\n\n\n9:30-10:25am\nMine Çetinkaya-Rundel, University of Edinburgh\nKeynote - Teaching\nhttps://youtu.be/ANH2tv2vkew\n\n\n10:30-11:30am\nRiana Minocher, Max Planck Institute for Evolutionary Anthropology\nKeynote - Evaluating\nhttps://youtu.be/O3t8TwWeli0\n\n\n11:30-11:55am\nTiffany Timbers, University of British Columbia\nTeaching\nhttps://youtu.be/mh93W8XimOg\n\n\nNoon-12:25pm\nTyler Girard, University of Western Ontario\nTeaching\nhttps://youtu.be/k3qgmUAjIvA\n\n\n12:30-12:55pm\nShiro Kuriwaki, Harvard University\nPractices\nhttps://youtu.be/-J-eiPnmoNE\n\n\n1:00-1:25pm\nMeghan Hoyer, Washington Post & Larry Fenn AP\nPractices\nhttps://youtu.be/FFwMfNk83rc\n\n\n1:30-1:55pm\nTom Barton, Royal Holloway, University of London\nEvaluating\nhttps://youtu.be/YTdhcSDqFNQ\n\n\n2:00-2:25pm\nBreak\n-\n-\n\n\n2:30-2:55pm\nMauricio Vargas, Catholic University of Chile & Nicolas Didier Arizona State University\nEvaluating\nhttps://youtu.be/VpTavLYEMgg\n\n\n3:00-3:25pm\nJake Bowers, University of Illinois & The Policy Lab\nPractices\nhttps://youtu.be/3N0YwJIbbHg\n\n\n3:30-3:55pm\nAmber Simpson, Queens University\nPractices\nhttps://youtu.be/uUfrcB6aynQ\n\n\n4:00-4:25pm\nGarret Christensen, US FDIC\nEvaluating\nhttps://youtu.be/595KkVKJ29w\n\n\n4:30-4:55pm\nYanbo Tang, University of Toronto\nPractices\nhttps://youtu.be/0x6gOkldOvk\n\n\n5:00-5:25pm\nLauren Kennedy, Monash University\nPractices\nhttps://youtu.be/HhfogRbgbA4\n\n\n5:30-6:00pm\nLisa Strug, University of Toronto & CANSSI Ontario\nClosing remarks\nhttps://youtu.be/B_9puTSp3f8\n\n\n\n\n\nFriday, 26 February, 2021\n\n\n\n\n\n\n\n\n\nTime\nSpeaker\nFocus\nRecording\n\n\n\n\n8:00-8:30am\nNick Radcliffe and Pei Shan Yu, Global Open Finance Centre of Excellence & University of Edinburgh\nPractices\nhttps://youtu.be/pWEc8XoIIKE\n\n\n8:30-9:00am\nJulia Schulte-Cloos, LMU Munich\nPractices\n-\n\n\n9:00-9:25am\nSimeon Carstens, Tweag/IO\nPractices\nhttps://youtu.be/fpoFzDvrJAA\n\n\n9:30-9:55am\nBreak\n-\n-\n\n\n10:00-10:55am\nEva Vivalt, University of Toronto\nKeynote - Practices\nhttps://youtu.be/0WZUzf2oSGY\n\n\n11:00-11:25am\nAndrés Cruz, Pontificia Universidad Católica de Chile\nPractices\nhttps://youtu.be/HjdPDEACxmA\n\n\n11:30-11:55am\nEmily Riederer, Capital One\nPractices\nhttps://youtu.be/BknQ0ZNkMNY\n\n\nNoon-12:25pm\nFlorencia D’Andrea, National Institute of Agricultural Technology\nPractices\nhttps://youtu.be/9FVUIPfBeXw\n\n\n12:30-12:55pm\nJohn Blischak, Freelance scientific software developer\nPractices\nhttps://youtu.be/RrcaGukYDyE\n\n\n1:00-1:25pm\nShemra Rizzo, Genentech\nPractices\nhttps://youtu.be/rEYtB3CG76Q\n\n\n1:30-2:25pm\nBreak\n-\n-\n\n\n2:30-2:55pm\nWijdan Tariq, University of Toronto\nEvaluating\n-\n\n\n3:00-3:25pm\nSharla Gelfand, Freelance R Developer\nPractices\nhttps://youtu.be/G5Nm-GpmrLw\n\n\n3:30-3:55pm\nRyan Briggs, University of Guelph\nPractices\nhttps://youtu.be/_dgGbxItiB4\n\n\n4:00-4:25pm\nMonica Alexander, University of Toronto\nPractices\nhttps://youtu.be/yvM2C6aZ94k\n\n\n4:30-4:55pm\nAnnie Collins, University of Toronto\nPractices\nhttps://youtu.be/u4ibhN_nWyI\n\n\n5:00-5:25pm\nNancy Reid, University of Toronto\nPractices\nhttps://youtu.be/sIsOPuZOQL4\n\n\n5:30-6:00pm\nRohan Alexander, University of Toronto\nClosing remarks\nhttps://youtu.be/7LttFNOI6p8\n\n\n\n\n\n\nPresenter biographies and abstracts\nKeynotes:\n\nEva Vivalt\n\nBio: Eva Vivalt is an Assistant Professor in the Department of Economics at the University of Toronto. Her main research interests are in cash transfers, reducing barriers to evidence-based decision-making, and global priorities research.\nAbstract: An overview of the role of forecasting and a new platform for making them.\n\nMine Çetinkaya-Rundel\n\nBio: Mine Çetinkaya-Rundel is a Senior Lecturer in Statistics and Data Science in the School of Maths at University of Edinburgh, and currently on leave as Associate Professor of the Practice in the Department of Statistical Science at Duke University as well as a Professional Educator and Data Scientist at RStudio. She is the author of three open source statistics textbooks and is an instructor for Coursera. She is the chair-elect of the Statistical Education Section of the American Statistical Association. Her work focuses on innovation in statistics pedagogy, with an emphasis on student-centered learning, computation, reproducible research, and open-source education.\n\nAbstract: In the beginning was R Markdown. In this talk I will give a brief review of teaching statistics and data analysis through the lens of reproducibility with R Markdown, and how to use this tool effectively in teaching to maintain reproducibility as the scope of your students’ projects and their experience grow.\n\nRiana Minocher\n\nBio: Riana Minocher is a doctoral student at the Max Planck Institute for Evolutionary Anthropology in Leipzig. She is an evolutionary biologist with broad interests. She has worked on a range of projects on human and non-human primate behaviour and ecology. She is particularly interested in the evolutionary processes that create and shape diversity between and within groups. Through her PhD research, she is keen on exploring the dynamics of cultural transmission and learning in human populations, to better understand the diverse patterns of behaviour we observe.\n\nAbstract: Interest in improving reproducibility, replicability and transparency of research has increased substantially across scientific fields over the last few decades. We surveyed 560 empirical, quantitative publications published between 1955 and 2018, to estimate the rate of reproducibility for research on social learning, a large subfield of behavioural ecology. We found supporting materials were available for less than 30% of publications during this period. The availability of data declines exponentially with time since publication, with a half-life of about six years, and this “data decay rate” varies systematically with both study design and study species. Conditional on materials being available, we estimate that a reasonable researcher could expect to successfully reproduce about 80% of published results, based on our evaluating a subset of 40 publications. Taken together, this indicates an overall success rate of 24% for both acquiring materials and recovering published results, with non-reproducibility of results primarily due to unavailable, incomplete, or poorly-documented data. We provide recommendations to improve the reproducibility of research on the ecology and evolution of social behaviour.\n\n\nInvited presentations:\n\nAmber Simpson\n\nBio: Amber Simpson is the Canada Research Chair in Biomedical Computing and Informatics and Associate Professor in the School of Computing (Faculty of Arts and Science) and Department of Biomedical and Molecular Sciences (Faculty of Health Sciences). She specializes in biomedical data science and computer-aided surgery. Her research group is focused on developing novel computational strategies for improving human health. She joined the Queen’s University faculty in 2019, after four years as faculty at Memorial Sloan Kettering Cancer Center in New York and three years as a Research Assistant Professor in Biomedical Engineering at Vanderbilt University in Nashville. She is an American Association of Cancer Research award winner and the holder of multiple National Institutes of Health grants. She received her PhD in Computer Science from Queen’s University.\n\nAbstract: The development of predictive and prognostic biomarkers is a major area of investigation in cancer research. Our lab specializes in the development of quantitative imaging markers for personalized treatment of cancer. Progress in developing these novel markers is limited by a lack of optimization, standardization, and validation, all critical barriers to clinical use. This talk will describe our work in the repeatability and reproducibility of imaging biomarkers.\n\nAndrés Cruz\n\nBio: Andrés Cruz is an adjunct instructor at Pontificia Universidad Católica de Chile, where he teaches computational social science. He holds a BA and MA in Political Science, and is the co-editor of “R for Political Data Science: A Practical Guide” (CRC Press, 2020), an R manual for social science students and practitioners.\n\nAbstract: inexact is an RStudio addin to supervise fuzzy joins. Merging data sets is a simple procedure in most statistical software packages. However, applied researchers frequently face problems when dealing with data in which ID variables are not properly standardized. For instance, politicians’ names can be spelled differently in multiple sources (press reports, official documents, etc.), causing regular merging methods to fail. The most common approach to fix this issue when working with small and medium data sets is manually fixing the problematic values before merging. However, this solution is time-consuming and not reproducible. An RStudio addin called “inexact” was created to help with this. The package draws from approximate string matching algorithms, which quantify the distance between two given strings. When merging data sets with non-standardized ID variables, inexact users benefit from automatic match suggestions, while also being able to override the automatic choices when needed, using a user-friendly graphical user interface (GUI). The output is simply code to perform the corrected merging procedure, which registers the employed algorithm and any corrections made by the user, ensuring reproducibility. A development version of inexact is available on GitHub.\n\nAnnie Collins\n\nBio: Annie Collins is an undergraduate student in the Department of Mathematics specializing in applied mathematics and statistics with a minor in history and philosophy of science. In her free time, she focusses her efforts on student governance, promoting women’s representation in STEM, and working with data in the non-profit and charitable sector.\n\nAbstract: We create a dataset of all the pre-prints published on medRxiv between 28 January 2020 and 31 January 2021. We extract the text from these pre-prints and parse them looking for keyword markers signalling the availability of the data and code underpinning the pre-print. We are unable to find markers of either open data or open code for 81 per cent of the pre-prints in our sample. Our paper demonstrates the need to have authors categorize the degree of openness of their pre-print as part of the medRxiv submissions process, and more broadly, the need to better integrate open science training into a wide range of fields\n\nEmily Riederer\n\nBio: Emily Riederer is a Senior Analytics Manager at Capital One. Her team focuses on reimagining our analytical infrastructure by building data products, elevating business analysis with novel data sources and statistical methods, and providing consultation and training to our partner teams.\n\nAbstract: Complex software systems make performance guarantees through documentation and unit tests, and they communicate these to users with conscientious interface design. However, published data tables exist in a gray area; they are static enough not to be considered a ‘service’ or ‘software’, yet too raw to earn attentive user interface design. This ambiguity creates a disconnect between data producers and consumers and poses a risk for analytical correctness and reproducibility. In this talk, I will explain how controlled vocabularies can be used to form contracts between data producers and data consumers. Explicitly embedding meaning in each component of variable names is a low-tech and low-friction approach which builds a shared understanding of how each field in the dataset is intended to work. Doing so can offload the burden of data producers by facilitating automated data validation and metadata management. At the same time, data consumers benefit by a reduction in the cognitive load to remember names, a deeper understanding of variable encoding, and opportunities to more efficiently analyze the resulting dataset. After discussing the theory of controlled vocabulary column-naming and related workflows, I will illustrate these ideas with a demonstration of the convo R package, which aids in the creation, upkeep, and application of controlled vocabularies. This talk is based on my related blog post and R package.\n\nFlorencia D’Andrea\n\nBio: Florencia D’Andrea is a post-doc at the Argentine National Institute of Agricultural Technology where she develops computer tools to assess the risk of pesticide applications for aquatic ecosystems. She holds a PhD in Biological Sciences from the University of Buenos Aires, Argentina, and is part of the ReproHack core-team and the R-Ladies global team. She believes that code and data should also be recognized as valuable products of scientific work.\n\nAbstract: Choose your own adventure to a reproducible scientific article: learnings from ReproHack “I shared the code and data of my last scientific article, does it mean that it is reproducible?” One might think that having access to the research data and the code used to analyze that data would be enough to reproduce published results, but often this is much more involved. Is reproducibility dependent on the reviewer’s knowledge? What things do we not usually think about can affect reproducibility? Can the choice of how to capture the computational environment influence the experience of the reviewer? In this talk, we are going to think together some of the necessary steps that make someone else able to reproduce a scientific article or project. I will share some thoughts from my experience in ReproHack and show you how reviewing is a great practice to learn about reproducibility. What is ReproHack? Reprohack is a hackathon-style event focused on the reproducibility of research results. These hackathons provide a low-pressure sandbox environment for practicing reproducible research: Authors can practice producing reproducible research and receive friendly feedback and appreciation of their efforts. Participants can practice reviewing, learn about reproducibility best practices as well as common pitfalls from working with real-life materials rather than just dummy. They also get inspired and grow confidence in working more openly themselves. Research Community benefits from: Evaluating what best practice is in practice. More practice in both developing and reviewing materials.\n\nGarret Christensen\n\nBio: Garret Christensen received his economics PhD from UC Berkeley in 2011. He is an economist with the FDIC. Before that he worked for the Census Bureau, and he was a project scientist with the Berkeley Initiative for Transparency in the Social Sciences and a Data Science Fellow with the Berkeley Institute for Data Science.\n\nAbstract: Adoption of Open Science Practices is Increasing: Survey Evidence on Attitudes, Norms and Behavior in the Social Sciences. Has there been meaningful movement toward open science practices within the social sciences in recent years? Discussions about changes in practices such as posting data and pre-registering analyses have been marked by controversy—including controversy over the extent to which change has taken place. This study, based on the State of Social Science (3S) Survey, provides the first comprehensive assessment of awareness of, attitudes towards, perceived norms regarding, and adoption of open science practices within a broadly representative sample of scholars from four major social science disciplines: economics, political science, psychology, and sociology. We observe a steep increase in adoption: as of 2017, over 80% of scholars had used at least one such practice, rising from one quarter a decade earlier. Attitudes toward research transparency are on average similar between older and younger scholars, but the pace of change differs by field and methodology. According with theories of normal science and scientific change, the timing of increases in adoption coincides with technological innovations and institutional policies. Patterns are consistent with most scholars underestimating the trend toward open science in their discipline.\n\nJake Bowers\n\nBio: Jake Bowers is a Senior Scientist at The Policy Lab and a member of the Lab’s data science practice. Jake is Associate Professor of Political Science and Statistics at the University of Illinois Urbana-Champaign. He has served as a Fellow in the Office of Evaluation Sciences in the General Services Administration of the US Federal Government and is Methods Director for the Evidence in Governance and Politics network. Jake holds a PhD in Political Science from the University of California, Berkeley, and a BA in Ethics, Politics and Economics from Yale University.\n\nAbstract: For evidence-based public policy to grow in impact and importance, practices to enhance scientific credibility should be brought into governmental contexts and also should be modified for those contexts. For example, few analyses of governmental data allow data sharing (in contrast with most scientific studies); and many analyses of governmental administrative data inform high stakes immediate decisions (in contrast with the slow accumulation of scientific knowledge). We make several proposals to adjust scientific norms of reproducibility and pre-registration to the policy context.\n\nJohn Blischak\n\nBio: John Blischak is a freelance scientific software developer for the life sciences industry. He is the primary author of the R package workflowr and the co-maintainer of the CRAN Task View on Reproducible Research. He received his PhD in Genetics from the University of Chicago.\n\nAbstract: The workflowr R package helps organize computational research in a way that promotes effective project management, reproducibility, collaboration, and sharing of results. workflowr combines literate programming (knitr and rmarkdown) and version control (Git, via git2r) to generate a website containing time-stamped, versioned, and documented results. Any R user can quickly and easily adopt workflowr, which includes four key features: (1) workflowr automatically creates a directory structure for organizing data, code, and results; (2) workflowr uses the version control system Git to track different versions of the code and results without the user needing to understand Git syntax; (3) to support reproducibility, workflowr automatically includes code version information in webpages displaying results and; (4) workflowr facilitates online Web hosting (e.g. GitHub Pages) to share results. Our goal is that workflowr will make it easier for researchers to organize and communicate reproducible results. Documentation and source code are available.\n\nJulia Schulte-Cloos\n\nBio: Julia Schulte-Cloos is a Marie Skłodowska-Curie funded research fellow at LMU Munich. She has earned her PhD in Political Science from the European University Institute. Julia is passionate about developing tools and templates for generating reproducible workflows and creating reproducible research outputs with R Markdown.\n\nAbstract: We present a template package in R that allows users without any prior knowledge of R Markdown to implement reproducible research practices in their scientific workflows. We provide a single Rmd-file that is fully optimized for two different output formats, HTML and PDF. While in the stage of explorative analysis and when focusing on content only, researchers may rely on the ‘draft mode’ of our template that knits to HTML When in the stage of research dissemination and when focusing on the presentation of results, in contrast, researchers may rely on the ‘manuscript mode’ that knits to PDF. Our template outlines the basics for successfully writing a reproducible paper in R Markdown by showing how to include citations, figures, and cross-references. It also provides examples for the use of ggplot2 to include plots, both in static and animated outputs, and it shows how to present the most commonly used tables in scientific research (descriptive statistics and regression tables). Finally, in our template, we discuss some more advanced features of literate programming and helpful tweaks in R Markdown.\n\nLauren Kennedy\n\nBio: Lauren Kennedy is a lecturer in the Econometrics and Business Statistics department at Monash University. She works on applied statistical problems in the social sciences using primarily Bayesian methodology. Her most recent work is with survey data, particularly the use of model and poststratify methods to make population and subpopulation predictions.\n\nAbstract: Survey data is challenging to work with. It frequently contains entry errors (either from respondent recollection or interviewer entry) that are difficult to verify and identify. Survey data is often received in a form that is sensible for the software for which entry is intuitive, which does not necessarily follow through to a data structure that is intuitive to work with as an analyst. When we consider the use of tools like multilevel regression and poststratification, our challenges compound. Even if the population data is precleaned before release, measurements and items in the sample need to be mapped to measurements and items in the population. In this talk we discuss case studies of how and where these challenges appear in practice.\n\nLarry Fenn\n\nBio: Larry Fenn is a data journalist at the Associated Press. His investigative work has covered a broad range of topics, from guns to education to housing policy. Prior to journalism, he was an adjunct lecturer at Hunter College for applied mathematics and statistics.\n\nAbstract: Please see Meghan Hoyer.\n\nMauricio Vargas Sepúlveda\n\nBio: Mauricio Vargas Sepúlveda loves working with data and statistical programming, and is constantly learning new skills and tooling in his spare time. He mostly works in R due to its huge number of libraries and emphasis on reproducible analysis.\n\nAbstract: Evidence-based policymaking has turned into a high priority for governments across the world. The possibility of gaining efficiencies in the public expenditure and linking the policy design to the desired outcomes have been presented as significant advantages for the field of comparative policy. However, the same movement that supports the use of evidence in public policy decision making has brought a great concern about the sources of the supposed evidence. How should policymakers evaluate the evidence? The possibilities are open and depend on the institutional arrangements that support governmental operation and the possibility of properly judging the nature of the evidence. The movement of science reproducibility could enlighten the discussion about the quality of the evidence by providing a structured approach towards the source’s validity based on the possibility of reproducing the logic and analysis proper of scientific communication. This paper attempts to analyze the nature and quality of civil society organizations’ contributions to develop evidence for policymaking process from reproducibility perspective.\n\n\nMeghan Hoyer\n\nBio: Meghan Hoyer is Data Director at The Washington Post where she leads data projects and acts as a consulting editor on data-driven stories, graphics and visualizations across the newsroom. Before this she helped lead the AP’s data journalism. Meghan earned a bachelor of science in journalism at Northwestern University and an MFA in creative nonfiction writing at Old Dominion University.\n\nAbstract: This talk will cover AP DataKit, which is an open-source command-line tool designed to better structure and manage projects, and more generally, talk about creating sane, reproducible workflows.\n\nMonica Alexander\n\nBio: Monica Alexander is an Assistant Professor in Statistical Sciences and Sociology at the University of Toronto. She received her PhD in Demography from the University of California, Berkeley. Her research interests include statistical demography, mortality and health inequalities, and computational social science.\n\nAbstract: Sharing code for papers and projects is an important part of reproducible research. However, sometimes sharing code may be difficult, if the researcher feels their code is ‘not good enough’ and may reflect poorly on their broader research skills. This presentation contains some brief reflections from research, consulting, and teaching experiences that have led to overcoming my own barriers to sharing code and to help others do the same.\n\n\nNancy Reid\n\nBio: Nancy Reid is Professor of Statistical Sciences at the University of Toronto and Canada Research Chair in Statistical Theory and Applications. Her main area of research is theoretical statistics. This treats the foundations and properties of methods of statistical inference. She is interested in how best to use information in the data to construct inferential statements about quantities of interest. A very simple example of this is the widely quoted ‘margin of error’ in the reporting of polls, another is the ubiquitous ‘p-value’ reported in medical and health studies. Much of her research considers how to ensure that these inferential statements are both accurate and effective at summarizing complex sets of data.\n\nAbstract: Are p-values contributing to a crisis in replicability and reproducibility? This has been the topic of many dialogues, diatribes, and discussions among statisticians and scientists in recent years. I will share my thoughts on the issues, with emphasis on the role of inferential theory in helping to clarify the arguments.\n\nNick Radcliffe\n\nBio: Nick Radcliffe is the founder of the data science consulting and software firm, Stochastic Solutions Limited, the Interim Chief Scientist at the Global Open Finance Centre of Excellence, and a Visiting Professor in Maths and Stats at University of Edinburgh, Scotland. His background combines theoretical physics, operations research, machine learning and stochastic optimization. Nick’s current research interests include a focus on test-driven data analysis, (an approach to improving correctness of analytical results that combines ideas from reproducible research and test-driven development) and privacy-respecting analysis. He is the lead author of the open-source Python tdda package, which provides practical tools for testing analytical software and data, and also of the Miró data analysis suite.\n\nAbstract: The Global Open Finance Centre of Excellence is currently engaged in analysis of the financial impact of COVID-19 on the citizens and businesses of the UK. This research uses non-consented but de-identified financial data on individuals and businesses, on the basis of legitimate interest. All analysis is carried out in a highly locked-down analytical environment known as a Safe Haven. This talk will explain our approach to the challenges of ensuring the correctness and robustness of results in an environment where neither code nor input data can be opened up for review and even outputs need to be subject to disclosure control to reduce further any risks to privacy. Topics will include: testing input data for conformance and lack of personal identifiers using constraints; multiple implementations and verification of equivalence of results; regression tests and reference tests; verification of output artefacts; verification of output disclosure controls; data provenance and audit trails; test-driven data analysis—the underlying philosophy (and library) that we use to underpin this work.\n\nNicolas Didier\n\nBio: Nicolas Didier is studying for a PhD in Public Administration and Policy at the Arizona State University. During his PhD studies and previous studies, he has worked extensively on developing evidence that addresses policy in labour markets and public expenditure.\n\nAbstract: Please see Mauricio Vargas Sepúlveda.\n\nRyan Briggs\n\nBio: Ryan Briggs is a social scientist who studies the political economy of poverty alleviation. Most of his research focuses on the spatial targeting of foreign aid. He is an Assistant Professor in the Guelph Institute of Development Studies and Department of Political Science at the University of Guelph. Before that, he taught at Virginia Tech and American University.\n\nAbstract: It is hard to do research. One reason for this is that it has a production function where one low quality input (among many high quality inputs) can poison a final result. This talk explains how such ‘o-ring’ production functions work and draws out lessons for applied researchers.\n\nSharla Gelfand\n\nBio: Sharla Gelfand is a freelance R and Shiny developer specializing in enabling easy access to data and replacing manual, redundant processes with ones that are automated, reproducible, and repeatable. They also co-organize R-Ladies Toronto and the Greater Toronto Area R User Group. They like R (of course), dogs, learning Spanish, playing bass, and punk.\n\nAbstract: Getting stuck, looking around for a solution, and eventually asking for help is an inevitable and constant aspect of being a programmer. If you’ve ever looked up a question only to find some brave soul getting torn apart on Stack Overflow for not providing a minimum working example, you know it’s also one of the most intimidating parts! A minimum working example, or a reproducible example as it’s more often called in the R world, is one of the best ways to get help with your code - but what exactly is a reproducible example? How do you create one, and do it efficiently? Why is it so scary? This talk will cover what components are needed to make a good reproducible example to maximize your ability to get help (and to help yourself!), strategies for coming up with an example and testing its reproducibility, and why you should care about making one. We will also discuss how to extend the concept of reproducible examples beyond “Help! my code doesn’t work” to other environments where you might want to share code, like teaching and blogging.\n\nShemra Rizzo\n\nBio: Shemra Rizzo is a senior data scientist in Genentech’s Personalized Healthcare group. Shemra’s role includes research on COVID-19 using electronic health records, and the development of data-driven approaches to evaluate clinical trial eligibility criteria. Shemra obtained her PhD in Biostatistics from UCLA. Before joining Genentech, Shemra was an assistant professor of statistics at UC Riverside, where her research covered topics in mental health, health disparities, and nutrition. In her free time, Shemra enjoys spending time with her family and running.\n\nAbstract: Real world data for an emerging disease has unique challenges. In this talk, I’ll describe how our group made sense of complex Electronic Health Records (EHR) data for COVID19 early in the pandemic. I will share our experience working towards reliable, replicable and reproducible studies using EHR licensed data.\n\nShiro Kuriwaki\n\nBio: Shiro Kuriwaki is a PhD Candidate in the Department of Government at Harvard University. His research focuses on democratic representation in American Politics. In an ongoing project, he studies the structure of voter’s choices across levels of government and the political economy of local elections, using cast vote records and surveys. His other projects also help understand the mechanics of representation, including: public opinion and Congress, modern survey statistics and causal inference, and election administration. Prior to and during graduate school, he worked at the Analyst Institute in Washington D.C.\n\nAbstract: I show how new features of the dataverse R package facilitate reproducibility in empirical, substantive projects. While packages and scripts make our code transparent and portable across forms, the import of large and complex datasets is often a nuisance in project workflows that involve various data cleaning and wrangling tasks. And the GUI for Dataverse can be sometimes tedious to integrate into code-based workflow. Will Beasley and I, along with multiple other contributors, updated the dataverse R package for the first time since 2017 with the goal of spreading its use in empirical workflow. In this iteration, we make it easier to retrieve dataframes of various file format and options for version specification and variable subsetting. I also discuss the latest updates to pyDataverse, a independent implementation in Python which is currently more advanced in its implementation but focused on uploading and creating datasets to dataverse.\n\nSimeon Carstens\n\nBio: Simeon Carstens is a Data Scientist at Tweag I/O, a software innovation lab and consulting company. Originally a physicist, Simeon did a PhD and postdoc research in computational biology, focusing on Bayesian determination of three-dimensional chromosome structures.\n\nAbstract: Data analysis often requires a complex software environment containing one or several programming languages, language-specific modules and external dependencies, all in compatible versions. This poses a challenge to reproducibility: what good is a well-designed, tested and documented data analysis pipeline if it is difficult to replicate the software environment required to run it? Standard tools such as Python / R virtual environments solve part of the problem, but do not take into account external and system-level dependencies. Nix is a fully declarative, open-source package manager solving this problem: a program packaged with Nix comes with a complete description of its full dependency tree, down to system libraries. In this presentation, I will give an introduction to Nix, show in a live demo how to set up a fully reproducible software environment and compare Nix to existing solutions such as virtual environments and Docker.\n\nTiffany Timbers\n\nBio: Tiffany Timbers is an Assistant Professor of Teaching in the Department of Statistics and an Co-Director for the Master of Data Science program (Vancouver Option) at the University of British Columbia. In these roles she teaches and develops curriculum around the responsible application of Data Science to solve real-world problems. One of her favourite courses she teaches is a graduate course on collaborative software development, which focuses on teaching how to create R and Python packages using modern tools and workflows.\n\nAbstract: In the data science courses at UBC, we define data science as the study and development of reproducible and auditable processes to obtain value (i.e., insight) from data. While reproducibility is core to our definition, most data science learners enter the field with other aspects of data science in mind, such as predictive modelling. This fact, along with the highly technical nature of the industry standard reproducibility tools currently employed in data science, present out-of-the gate challenges in teaching reproducibility in the data science classroom. Put simply, students are not as intrinsically motivated to learn this topic, and it is not an easy one for them to learn. What can a data science educator do? Over several iterations of teaching courses focused on reproducible data science tools and workflows, we have found that motivation, direct instruction and practice are key to effectively teach this challenging, yet important subject. In this talk, I will present examples of how we deeply motivate, effectively instruct and provide ample practice opportunities to our Master of Data Science students to effectively engage them in learning about this topic.\n\nTom Barton\n\nBio: Tom Barton is a PhD student in Politics at Royal Holloway, University of London. His PhD focuses on the impact of Voter Identification laws on political participation and attitudes. More generally his interests include elections, public opinion (particularly social values) and quantitative research methods.\n\nAbstract: I reproduce Surridge, 2016, ‘Education and liberalism: pursuing the link’, Oxford Review of Education, 42:2, pp. 146-164, using the 1970 British Cohort Study (BCS70), instead using a difference-in-difference regression approach with more waves of data. I find that whilst there is evidence for both socialisation and self-selection models, self-selection dominates the link between social values and university attendance. This is counter to what Surridge (2016) concluded. The need for re-specification was two-fold, first Surridge’s methodology did not fully test for causality and secondly later waves have data have become available since.\n\n\nTyler Girard\n\nBio: Tyler Girard is a PhD Candidate in political science at the University of Western Ontario (London, Ontario, Canada). His dissertation research seeks to explain the origins and diffusion of the global financial inclusion agenda by focusing on the role of ambiguous ideas in mobilizing and consolidating transnational coalitions. More generally, his work also explores new approaches to conceptual measurement in international relations.\n\nAbstract: In what ways can we incorporate reproducible practices in pedagogy for social science courses? I discuss how individual and group exercises centered around the replication of existing datasets and analyses offer a flexible tool for experiential learning. However, maximizing the benefits of such an approach requires customizing the activity to the students and the availability of instructor support. I offer several suggestions for effectively using replication exercises in both undergraduate and graduate level courses.\n\nWijdan Tariq\n\nBio: Wijdan Tariq is an undergraduate student in the Department of Statistical Sciences at the University of Toronto.\n\nAbstract: I undertake a narrow replication of Caicedo, 2019, ‘The Mission: Human Capital Transmission, Economic Persistence, and Culture in South America’, Quarterly Journal of Economics, 134:1, pp. 507-556. Caicedo reports of a remarkable, religiously inspired human capital intervention that took place in remote parts of South America 250 years ago and whose positive economic effects, he claims, persist to this day. I replicate some of the paper’s key results using data files that are available on the Harvard Dataverse portal. I discuss some lessons learned in the process of replicating this paper and share some reflections on the state of reproducibility in economics.\n\nYanbo Tang\n\nBio: Yanbo Tang is a PhD candidate at the University of Toronto in the Department of Statistical Sciences, under the joint supervision of Nancy Reid and Daniel Roy. He is interested in the study and application of methods in higher order asymptotics and statistical inference in the presence of many nuisance parameters. Nowadays, he works under the careful gaze of his pet parrot.\n\nAbstract: Hypothesis testing results often rely on simple, yet important assumptions about the behavior of the distribution of p-values under the null and alternative. We show that commonly held beliefs regarding the distribution of p-values are misleading when the variance or location of the test statistic are not well-calibrated or when the higher order cumulants of the test statistic are not negligible. We further examine the impact of having these misleading p-values on reproducibility of scientific studies, with some examples focused on GWAS studies. Certain corrected tests are proposed and are shown to perform better than their traditional counterparts in certain settings."
  },
  {
    "objectID": "events-reproducibility.html#code-of-conduct",
    "href": "events-reproducibility.html#code-of-conduct",
    "title": "Toronto Workshop on Reproducibility",
    "section": "Code of conduct",
    "text": "Code of conduct\nCode\nThe organizers of the Toronto Workshop on Reproducibility are dedicated to providing a harassment-free experience for everyone regardless of age, gender, sexual orientation, disability, physical appearance, race, or religion (or lack thereof).\nAll participants (including attendees, speakers, sponsors and volunteers) at the Toronto Workshop on Reproducibility are required to agree to the following code of conduct.\nThe code of conduct applies to all conference activities including talks, panels, workshops, and social events. It extends to conference-specific exchanges on social media, for instance posts tagged with the identifier of the conference (e.g. #TOrepro on Twitter), and replies to such posts.\nOrganizers will enforce this code throughout and expect cooperation in ensuring a safe environment for all.\nExpected Behaviour\nAll conference participants agree to:\n\nBe considerate in language and actions, and respect the boundaries of fellow participants.\nRefrain from demeaning, discriminatory, or harassing behaviour and language. Please refer to ‘Unacceptable Behaviour’ for more details.\nAlert Rohan Alexander - rohan.alexander@utoronto.ca - or Kelly Lyons - kelly.lyons@utoronto.ca - if you notice someone in distress, or observe violations of this code of conduct, even if they seem inconsequential. Please refer to the section titled ‘What To Do If You Witness or Are Subject To Unacceptable Behaviour’ for more details.\n\nUnacceptable Behaviour\nBehaviour that is unacceptable includes, but is not limited to:\n\nStalking\nDeliberate intimidation\nUnwanted photography or recording\nSustained or willful disruption of talks or other events\nUse of sexual or discriminatory imagery, comments, or jokes\nOffensive comments related to age, gender, sexual orientation, disability, race or religion\nInappropriate physical contact, which can include grabbing, or massaging or hugging without consent.\nUnwelcome sexual attention, which can include inappropriate questions of a sexual nature, asking for sexual favours or repeatedly asking for dates or contact information.\n\nIf you are asked to stop harassing behaviour you should stop immediately, even if your behaviour was meant to be friendly or a joke, it was clearly not taken that way and for the comfort of all conference attendees you should stop.\nAttendees who behave in a manner deemed inappropriate are subject to actions listed under ‘Procedure for Code of Conduct Violations’.\nAdditional Requirements for Conference Contributions\nPresentation slides and posters should not contain offensive or sexualised material. If this material is impossible to avoid given the topic (for example text mining of material from hate sites) the existence of this material should be noted in the abstract and, in the case of oral contributions, at the start of the talk or session.\nProcedure for Code of Conduct Violations\nThe organizing committee reserves the right to determine the appropriate response for all code of conduct violations. Potential responses include:\n\na formal warning to stop harassing behaviour\nexpulsion from the conference\ncancellation or early termination of talks or other contributions to the program\n\nWhat To Do If You Witness or Are Subject To Unacceptable Behaviour\nIf you are being harassed, notice that someone else is being harassed, or have any other concerns relating to harassment, please contact Rohan Alexander - rohan.alexander@utoronto.ca, or Kelly Lyons - kelly.lyons@utoronto.ca.\nWe will take all good-faith reports of harassment by Toronto Workshop on Reproducibility participants seriously.\nWe reserve the right to reject any report we believe to have been made in bad faith. This includes reports intended to silence legitimate criticism.\nWe will respect confidentiality requests for the purpose of protecting victims of abuse. We will not name harassment victims without their affirmative consent.\nQuestions or concerns about the Code of Conduct can be addressed to rohan.alexander@utoronto.ca.\nAcknowledgements\nParts of the above text are licensed CC BY-SA 4.0. Credit to SRCCON. This code of conduct was based on that developed for useR! 2018 which was a revision of the code of conduct used at previous useR!s and also drew from rOpenSci’s code of conduct."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Rohan Alexander",
    "section": "",
    "text": "I am an assistant professor at the University of Toronto, jointly appointed in the Faculty of Information and the Department of Statistical Sciences. I am interested in developing workflows that improve the trustworthiness of data science. I am particularly interested in the role of code and testing in data science.\nI am co-founder of the Toronto Data Workshop, a weekly, online, seminar series that brings together academia and industry to share data science and AI best practice. You can sign up here.\nI am the Assistant Director of CANSSI Ontario; the Director, Technical Skills Curriculum & Instruction, for the Data Sciences Institute’s Data Science and Machine Learning Certificates; a Senior Fellow at Massey College; a faculty affiliate at the Schwartz Reisman Institute for Technology and Society; and a co-lead of the DSI Thematic Program in Reproducibility.\n\nMy book, Telling Stories With Data, argues that a trustworthiness revolution is needed in data science, and I propose a view of what it could look like. It has been strongly endorsed, including: Andrew Gelman “I absolutely love this book”, Kosuke Imai “I highly recommend this unique book!”, Sir David Spiegelhalter “[A]n extraordinary, wonderful, book…”, Daniela Witten “An excellent book”, and Richard McElreath “This is not another statistics book. It is much better than that.” You can buy a print copy here and you can access the free online version here.\nMy teaching helps students from a wide range of backgrounds learn how to use data to tell convincing stories. I am an associate editor of the Journal of Statistics and Data Science Education.\nI hold a PhD in Economics from the Australian National University where I focused on economic history and was supervised by John Tang (chair), Martine Mariotti, Tim Hatton, and Zach Ward.\nI am married to Monica Alexander, and we have two young children."
  },
  {
    "objectID": "teaching-modeling.html",
    "href": "teaching-modeling.html",
    "title": "Introduction to Modeling",
    "section": "",
    "text": "After developing comfort with the idea of how our world becomes data and everything that goes into creating a nice, tidy, analysis dataset, we may be interested to model it. This course builds on “Data science foundations” by developing the modeling side of things. Many of the results that we will obtain from our models are determined well before we ever contemplate regression, and the credibility of any conclusions relies on a robust workflow.\nThis course takes those skills as given, and focuses on doing a good job of the modeling. The typical person completing this course is a PhD student who is looking to apply statistics to some other discipline.\n\n\n\nBy the end of the course you should have three quantitative papers that can be submitted to a journal/conference. More generally you should be able to:\n\nEngage critically with ideas and readings related to statistical modeling (demonstrated in all papers and the notebook).\nConduct quantitative research in a high-quality, reproducible and ethical way (demonstrated in all papers).\nClearly communicate what was done, what was found, and why in writing (demonstrated in all papers).\nRespectfully identify strengths and weaknesses in the data science research conducted by others (demonstrated in quizzes, and the peer review).\nDevelop the ability to appropriately choose and apply statistical models to real-world situations (demonstrated in the final paper)\nConduct all aspects of the typical data science workflow (demonstrated in all papers).\n\n\n\n\n\n“Data science foundations”, or equivalent coverage of Chapters 1-13 of Telling Stories with Data.\n\n\n\n\n\nMcElreath, 2020, Statistical Rethinking, 2nd edition, Chapman & Hall/CRC. Please purchase a print copy, which will be about about $70. The author provides a free set of lectures that we will go through, which make purchasing the book not unreasonable.\nJohnson, Ott, and Dogucu, 2021, Bayes Rules!, Chapman & Hall/CRC. You are welcome to purchase a print copy, but there is also a free version online.\n\n\n\n\nThis course—especially the key feature of combining the two books—is largely based on Andrew Heiss’ ‘Bayesian Statistics’ course. I have tweaked the order of content a little, added articles, and changed assessment. I am grateful for Andrew’s ideas, suggestions, and generosity."
  },
  {
    "objectID": "teaching-modeling.html#preamble",
    "href": "teaching-modeling.html#preamble",
    "title": "Introduction to Modeling",
    "section": "",
    "text": "After developing comfort with the idea of how our world becomes data and everything that goes into creating a nice, tidy, analysis dataset, we may be interested to model it. This course builds on “Data science foundations” by developing the modeling side of things. Many of the results that we will obtain from our models are determined well before we ever contemplate regression, and the credibility of any conclusions relies on a robust workflow.\nThis course takes those skills as given, and focuses on doing a good job of the modeling. The typical person completing this course is a PhD student who is looking to apply statistics to some other discipline.\n\n\n\nBy the end of the course you should have three quantitative papers that can be submitted to a journal/conference. More generally you should be able to:\n\nEngage critically with ideas and readings related to statistical modeling (demonstrated in all papers and the notebook).\nConduct quantitative research in a high-quality, reproducible and ethical way (demonstrated in all papers).\nClearly communicate what was done, what was found, and why in writing (demonstrated in all papers).\nRespectfully identify strengths and weaknesses in the data science research conducted by others (demonstrated in quizzes, and the peer review).\nDevelop the ability to appropriately choose and apply statistical models to real-world situations (demonstrated in the final paper)\nConduct all aspects of the typical data science workflow (demonstrated in all papers).\n\n\n\n\n\n“Data science foundations”, or equivalent coverage of Chapters 1-13 of Telling Stories with Data.\n\n\n\n\n\nMcElreath, 2020, Statistical Rethinking, 2nd edition, Chapman & Hall/CRC. Please purchase a print copy, which will be about about $70. The author provides a free set of lectures that we will go through, which make purchasing the book not unreasonable.\nJohnson, Ott, and Dogucu, 2021, Bayes Rules!, Chapman & Hall/CRC. You are welcome to purchase a print copy, but there is also a free version online.\n\n\n\n\nThis course—especially the key feature of combining the two books—is largely based on Andrew Heiss’ ‘Bayesian Statistics’ course. I have tweaked the order of content a little, added articles, and changed assessment. I am grateful for Andrew’s ideas, suggestions, and generosity."
  },
  {
    "objectID": "teaching-modeling.html#content",
    "href": "teaching-modeling.html#content",
    "title": "Introduction to Modeling",
    "section": "Content",
    "text": "Content\nEach week you should read the chapters, watch the relevant lecture, and attempt the exercises.\n\nWeek 1\n\nRead Statistical Rethinking Chapter 1, “The Golem of Prague”\nWatch Statistical Rethinking 2023 videos, Lecture 1, “The Golem of Prague”\nRead Statistical Rethinking Chapter 2, “Small Worlds and Large Worlds”\nRead Bayes Rules! Chapter 1, “The (Big) Bayesian Picture”\nRead Bayes Rules! Chapter 2, “Bayes’ Rule”\nRead Freedman, David, 1991, “Statistical Models and Shoe Leather” Sociological Methodology, Vol. 21, pp. 291-313.\n\n\n\nWeek 2\n\nWatch Statistical Rethinking 2023 videos, Lecture 2, “The Garden of Forking Data”\nRead Statistical Rethinking Chapter 3, “Sampling the Imaginary”\nRead Bayes Rules! Chapter 3, “The Beta-Binomial Bayesian Model”\n\n\n\nWeek 3\n\nRead Statistical Rethinking Chapter 4, “Geocentric Models”\nWatch Statistical Rethinking 2023 videos, Lecture 3, “Geocentric Models”\nWatch Statistical Rethinking 2023 videos, Lecture 4, “Categories & Curves”\nRead Bayes Rules! Chapter 4, “Balance and Sequentiality in Bayesian Analyses”\nRead Bayes Rules! Chapter 5, “Conjugate Families”\n\n\n\nWeek 4\n\nRead Statistical Rethinking Chapter 5, “The Many Variables & The Spurious Waffles”\nWatch Statistical Rethinking 2023 videos, Lecture 5, “Elemental Confounds”\nRead Statistical Rethinking Chapter 6, “The Haunted DAG & The Causal Terror”\nWatch Statistical Rethinking 2023 videos, Lecture 6, “Good and Bad Controls”\n\n\n\nWeek 5\n\nRead Statistical Rethinking Chapter 7, “Ulysses’ Compass”\nWatch Statistical Rethinking 2023 videos, Lecture 7, “Fitting over and under”\nRead Bayes Rules! Chapter 9, “Simple Normal Regression”\nRead Bayes Rules! Chapter 10, “Evaluating Regression Models”\nRead Bayes Rules! Chapter 11, “Extending the Normal Regression Model”\n\n\n\nWeek 6\n\nRead Statistical Rethinking Chapter 9, “Markov Chain Monte Carlo”\nWatch Statistical Rethinking 2023 videos, Lecture 8, “Markov Chain Monte Carlo”\nRead Bayes Rules! Chapter 6, “Approximating the Posterior”\nRead Bayes Rules! Chapter 7, “MCMC under the Hood”\nRead Bayes Rules! Chapter 8, “Posterior Inference & Prediction”\n\n\n\nWeek 7\n\nRead Statistical Rethinking Chapter 10, “Big Entropy and the Generalized Linear Model”\nRead Statistical Rethinking Chapter 11, “God Spiked the Integers”\nWatch Statistical Rethinking 2023 videos, Lecture 9, “Modeling events”\nWatch Statistical Rethinking 2023 videos, Lecture 10, “Counts and Hudden Confounds”\nRead Bayes Rules! Chapter 12, “Poisson & Negative Binomial Regression”\nRead Bayes Rules! Chapter 13, “Logistic Regression”\nRead Burch, Tyler James, 2023, “2023 NHL Playoff Predictions”, April.\n\n\n\nWeek 8\n\nRead Statistical Rethinking Chapter 12, “Monsters and Mixtures”\nWatch Statistical Rethinking 2023 videos, Lecture 11, “Ordered Categories”\n\n\n\nWeek 9\n\nRead Statistical Rethinking Chapter 13, “Models with Memory”\nWatch Statistical Rethinking 2023 videos, Lecture 12, “Multilevel Models”\nRead Bayes Rules! Chapter 15, “Hierarchical Models are Exciting”\nRead Bayes Rules! Chapter 16, “(Normal) Hierarchical Models without Predictors”\nRead Bayes Rules! Chapter 17, “(Normal) Hierarchical Models with Predictors”\n\n\n\nWeek 10\n\nRead Statistical Rethinking Chapter 14, “Adventures in Covariance”\nWatch Statistical Rethinking 2023 videos, Lecture 13, “Multilevel adventures”\nWatch Statistical Rethinking 2023 videos, Lecture 14, “Correlated features”\nRead Bayes Rules! Chapter 18, “Non-Normal Hierarchical Regression & Classification”\nRead Bayes Rules! Chapter 19, “Adding More Layers”\n\n\n\nWeek 11\n\nRead Statistical Rethinking Chapter 15, “Missing Data and Other Opportunities”\nWatch Statistical Rethinking 2023 videos, Lecture 17, “Measurement and misclassification”\nWatch Statistical Rethinking 2023 videos, Lecture 18, “Missing Data”\nRead Rubin, D, 1976, “Inference and missing data”, Biometrika.\n\n\n\nWeek 12\n\nRead Statistical Rethinking Chapter 16, “Generalized Linear Madness”\nRead Statistical Rethinking Chapter 17, “Horoscopes”\nWatch Statistical Rethinking 2023 videos, Lecture 19, “Generalized Linear Madness”\nWatch Statistical Rethinking 2023 videos, Lecture 20, “Horoscopes”"
  },
  {
    "objectID": "teaching-modeling.html#assessment",
    "href": "teaching-modeling.html#assessment",
    "title": "Introduction to Modeling",
    "section": "Assessment",
    "text": "Assessment\n\nNotebook\n\nDue date: Try to keep this updated weekly, on average, over the course of the term.\nTask: Use Quarto to keep a notebook of what you read in the style of this one by Andrew Heiss. In particular, please add notes as you read/watch, and attempt at least half the exercises.\nWeight: 20 per cent.\n\n\n\nPaper I\n\nDue date: Thursday, noon, Week 1.\n\nMarking starts, noon, on the following Monday, and you can update until then to incorporate peer review comments. Please do not make any changes after marking starts.\n\nTask: Donaldson Paper\nWeight: 10 per cent.\nYou must do this individually.\nYou do not need to do this again if you already did it as part of “Data science foundations”. In that case, the weight will be redistributed to Papers II and III.\n\n\n\nPaper II\n\nDue date: Thursday, noon, Week 4.\n\nMarking starts, noon, on the following Monday, and you can update until then to incorporate peer review comments. Please do not make any changes after marking starts.\n\nTask: Murrumbidgee Paper\nYou are welcome to work in teams up to size three.\nWeight: 20 per cent.\nThe mark is conditional on the paper being submitted to a journal/conference within two weeks of marking (being rejected is fine—it just has to have been submitted).\n\n\n\nPaper III\n\nDue date: Thursday, noon, Week 8.\n\nMarking starts, noon, on the following Monday, and you can update until then to incorporate peer review comments. Please do not make any changes after marking starts.\n\nTask: Spadina Paper or Spofforth Paper\nYou are welcome to work in teams up to size three.\nWeight: 20 per cent.\nThe mark is conditional on the paper being submitted to a journal/conference within two weeks of marking (being rejected is fine—it just has to have been submitted).\n\n\n\nFinal Paper\n\nDue date: Thursday, noon, Week 12.\n\nMarking starts, noon, two weeks after this date, and you can update update until then to allow you to incorporate peer review comments. Please do not make any changes after marking starts.\n\nTask: Final Paper\nYou must do this individually.\nWeight: 30 per cent.\nThe mark is conditional on the paper being submitted to a journal/conference within two weeks of marking (being rejected is fine—it just has to have been submitted)."
  },
  {
    "objectID": "posts/2021-09-17-andrew-gelman-dsi-intro/andrew_gelman_intro.html",
    "href": "posts/2021-09-17-andrew-gelman-dsi-intro/andrew_gelman_intro.html",
    "title": "Introduction to Andrew Gelman",
    "section": "",
    "text": "The slides are here."
  },
  {
    "objectID": "posts/2021-09-17-andrew-gelman-dsi-intro/andrew_gelman_intro.html#introduction",
    "href": "posts/2021-09-17-andrew-gelman-dsi-intro/andrew_gelman_intro.html#introduction",
    "title": "Introduction to Andrew Gelman",
    "section": "Introduction",
    "text": "Introduction\nHi, my name is Rohan Alexander. I’m an assistant professor jointly across Information and Statistical Sciences. I’m also the Assistant Director of the Canadian Statistical Sciences Institute (CANSSI), Ontario, which strengthens research and training in data science. And in that capacity it is a pleasure to introduce our next speaker, Andrew Gelman."
  },
  {
    "objectID": "posts/2021-09-17-andrew-gelman-dsi-intro/andrew_gelman_intro.html#andrew-gelman",
    "href": "posts/2021-09-17-andrew-gelman-dsi-intro/andrew_gelman_intro.html#andrew-gelman",
    "title": "Introduction to Andrew Gelman",
    "section": "Andrew Gelman",
    "text": "Andrew Gelman\nAndrew Gelman is Higgins Professor of Statistics and Professor of Political Science at Columbia University. He believes ‘the #1 neglected topic in statistics is measurement’"
  },
  {
    "objectID": "posts/2021-09-17-andrew-gelman-dsi-intro/andrew_gelman_intro.html#rent",
    "href": "posts/2021-09-17-andrew-gelman-dsi-intro/andrew_gelman_intro.html#rent",
    "title": "Introduction to Andrew Gelman",
    "section": "Rent",
    "text": "Rent\nAnd so, with apologies to Rent, how should we measure his career? With blog posts? With papers? With software? With cups of coffee? With books? Or by the others who relied?\nI’ve been unable to verify his coffee consumption, but by any other measure, Andrew Gelman, puts most to shame."
  },
  {
    "objectID": "posts/2021-09-17-andrew-gelman-dsi-intro/andrew_gelman_intro.html#books",
    "href": "posts/2021-09-17-andrew-gelman-dsi-intro/andrew_gelman_intro.html#books",
    "title": "Introduction to Andrew Gelman",
    "section": "Books",
    "text": "Books\nHe has published many books and I’ll just touch on two.\nWith co-authors, his foundational Bayesian statistics textbook—Bayesian Data Analysis—was first published in 1995. It has been cited something like 30,000 times. And now on its 3rd edition, it is the basis for many foundational courses on Bayesian statistics.\nA more applied textbook, co-authored with Jennifer Hill—Data Analysis using Regression and Multilevel Models—was published in 2006. That book has been cited something like 14,000 times, and forms the foundation for many applied statistics courses.\nMy wife, Monica, is an assistant professor jointly in statistical sciences and sociology, and in my household these are known, respectively, as the Old and New Testament."
  },
  {
    "objectID": "posts/2021-09-17-andrew-gelman-dsi-intro/andrew_gelman_intro.html#blog",
    "href": "posts/2021-09-17-andrew-gelman-dsi-intro/andrew_gelman_intro.html#blog",
    "title": "Introduction to Andrew Gelman",
    "section": "Blog",
    "text": "Blog\nGelman’s blog—Statistical Modeling, Causal Inference, and Social Science—launched in 2004—is the go-to place for a fun mix of somewhat-nerdy statistics-focused content. The very first post promised to ‘…report on recent research and ongoing half-baked ideas, including … Bayesian statistics, multilevel modeling, causal inference, and political science.’ 17 years on, the site has very much kept its promise."
  },
  {
    "objectID": "posts/2021-09-17-andrew-gelman-dsi-intro/andrew_gelman_intro.html#reproducibility",
    "href": "posts/2021-09-17-andrew-gelman-dsi-intro/andrew_gelman_intro.html#reproducibility",
    "title": "Introduction to Andrew Gelman",
    "section": "Reproducibility",
    "text": "Reproducibility\nOne thing the blog does is enable Gelman to make contributions and influence debate in a way that isn’t possible in papers or books. For instance, it enables him to share short snippets of code, explain how he approaches problems, and sketch solutions.\nGelman’s blog is where he’s done a lot of writing about reproducibility—one of the DSI’s Thematic Programs. Arguably Andrew’s blog posts brought the replication crisis that enveloped social sciences to the fore. A lot of my colleagues now push reproducibility; both in their teaching and in their research. And that’s in large part because we intellectually grew up reading Gelman’s blog posts about it."
  },
  {
    "objectID": "posts/2021-09-17-andrew-gelman-dsi-intro/andrew_gelman_intro.html#papers",
    "href": "posts/2021-09-17-andrew-gelman-dsi-intro/andrew_gelman_intro.html#papers",
    "title": "Introduction to Andrew Gelman",
    "section": "Papers",
    "text": "Papers\nBy any measure, Andrew Gelman is a prolific author of papers, with something like 400 based on a recent count. My favourite of these is affectionately known as ‘the XBox paper’ which was published in 2015.\nThese days the statistical approach that underpinned that paper—multilevel regression with post-stratification or MRP—is used by almost every major polling company to some extent. And teams of undergraduates at the UofT used MRP to correctly forecast a Biden victory last year. If you’re interested in learning more about MRP, Andrew Gelman, Lauren Kennedy and I are putting together a book about MRP that is forthcoming with Cambridge University Press next year."
  },
  {
    "objectID": "posts/2021-09-17-andrew-gelman-dsi-intro/andrew_gelman_intro.html#stan",
    "href": "posts/2021-09-17-andrew-gelman-dsi-intro/andrew_gelman_intro.html#stan",
    "title": "Introduction to Andrew Gelman",
    "section": "Stan",
    "text": "Stan\nAndrew was foundational in the development of Stan - a probabilistic programming language, which has become the predominant way to implement Bayesian models.\nFunnily enough the origin of Stan is that Gelman was trying to expand on some of the models in his book with Jennifer Hill that I mentioned earlier. He found that the software was struggling to do it. With co-authors he found that a change in the way random samples were obtained worked well, especially when combined with a few other innovations. One thing led to another, and a revolution in scientific computing ensued."
  },
  {
    "objectID": "posts/2021-09-17-andrew-gelman-dsi-intro/andrew_gelman_intro.html#impact-on-others",
    "href": "posts/2021-09-17-andrew-gelman-dsi-intro/andrew_gelman_intro.html#impact-on-others",
    "title": "Introduction to Andrew Gelman",
    "section": "Impact on others",
    "text": "Impact on others\nAndrew Gelman is someone who has made incredible contributions in a range of data science and perfectly characterises what we’re trying to do with this Data Sciences Institute. To this point, I’ve stuck with things that are relatively easy to measure. Following the spirit of our guest, I’d like to close with something less easy to measure; and that is, the impact on others.\nIn my own case, I wouldn’t be speaking here today if it weren’t for Andrew: I learnt statistics from his books, his XBox paper gave my career purpose, I copy-paste his code most days, his blog provided me with a community, and from his example I learnt the type of academic I wanted to be.\nAnd when I asked a few others it seems that feeling was unanimous.\n\nHis blog made me realise that there is a place for people like me, that is, people who want to do good applied stats on interesting social problems. This and reading Gelman Hill influenced my decision to go to grad school.\nMonica Alexander\n\n\nAndrew provided a voice that cut through the often confusing and rule based interpretation of statistics to instead provide an interpretation that is reason based. This represented a turning point for me in the way I understood existing statistical problems and rationalised about new challenges.\nLauren Kennedy\n\n\nHis class (and writing) showed me the connections between statistics, science, and philosophy and gave me a language through which to express my frustrations with the ways that data is tortured. I particularly appreciated his emphasis that “stats is hard” and it’s OK to make mistakes as long as you learn from them!\nJames Doss-Gollin\n\n\nWorking in government and politics, I channel Andy, both as a statistician and as a teammate. I quote insights from his books, papers, and blog posts. I learn from his humor, openness about being confused, and commitment to doing the right thing.\nShira Mitchell\n\n\nAndrew’s my Obi-Wan-Kenobi.\nBob Carpenter\n\n\nAndrew’s blog made me aware of the shortcomings in my training and research, and gave me motivation to improve. It also gave me the motivation to get out of projects that could become posts in the Zombies category.\nMarta Kołczyńska\n\n\nHere’s something that I really appreciate about Andrew and isn’t common enough in academia: if you do good work Andrew will want to work with you (even hire you) regardless of your credentials.\nJonah S Gabry\n\n\nMy conception of what good research was - what kind of research I wanted to be doing - completely changed when I read Andrew’s paper on the garden of forking paths. It was one of the main reasons I decided to go back to school for a degree in statistics, and it’s deeply influenced the way I try to do research and the research practices I advocate for at my workplace. Since that paper came out in 2013 I’ve been a dedicated reader of his blog, and his posts have made me fundamentally rethink the way I do research an embarrassing number of times - and they continue to motivate me to try to do careful and thoughtful work.\nIsaac Maddow-Zimet\n\nSo it’s a very warm virtual welcome to Toronto, Professor Andrew Gelman. Thank you for everything you’ve done for data science, and thank you for helping us to launch our Data Sciences Institute."
  },
  {
    "objectID": "posts/2021-09-17-andrew-gelman-dsi-intro/andrew_gelman_intro.html#acknowledgments",
    "href": "posts/2021-09-17-andrew-gelman-dsi-intro/andrew_gelman_intro.html#acknowledgments",
    "title": "Introduction to Andrew Gelman",
    "section": "Acknowledgments",
    "text": "Acknowledgments\n\nThank you very much to Monica Alexander, and members of my research group, for reading a draft of this. And to Jonah Gabry for the Stan historical background. And thank you to Isaac, Jonah, Marta, Bob, Shira, James, Lauren, and Monica for providing quotes."
  },
  {
    "objectID": "posts/2018-08-13-cleaning-hansard/cleaning-hansard.html",
    "href": "posts/2018-08-13-cleaning-hansard/cleaning-hansard.html",
    "title": "Cleaning Hansard: The pay’s not great but the work is hard",
    "section": "",
    "text": "Thanks to Monica for the title.\nAfter getting an email along the lines of ‘hey aren’t you doing something with Australia’s Hansard?’ I realised that I’ve been a bit remiss about sharing my Australian Hansard progress. This is the first of a series of posts to fix that.\nI haven’t written about what I’ve been doing with Australian Hansard to this point because: 1) my knowledge of the political science literature is piecemeal and I’m sure someone must have already done all this and I just can’t find it; and 2) my coding knowledge is also piecemeal and there’s no doubt a million ways to better do what I’ve done so far. If anyone has advice on either aspect (or anything really!) I’d be keen to hear it - please email.\nBest I can tell, Australian Hansard is a treasure-trove of data and it’s hard to believe it hasn’t been more analysed. I’m probably missing a whole bunch of literature (insert standard joke about economists here) but so far I can really only find a handful of papers using Australia’s Hansard.1 There’s plenty of work using the parliamentary records of other Commonwealth countries such as Canada2, NZ3 and the UK4. I think that in Australia it’s really only Patrick Leslie who may be using it at the moment (big thank you to Jill Sheppard for pointing this out), but I’ll update this if I find others.\nThe good news is that Australia’s Hansard has been digitised and is available on the parliament’s website, so a figurative pseudo-Manhattan-project isn’t required (cf. what was needed in Canada, see: https://www.lipad.ca/). If you just want short, specific, sections then the situation is fine - for pre-1981 go to Tim Sherratt’s brilliant Historic Hansard website (http://historichansard.net); for 1981 to 2006 just use the parliament’s website; and for 2006 onward just use Open Australia. The bad news is that Hansard isn’t really available as a nice corpus for larger scale analysis. Making this nice corpus has been keeping me busy, and will be the subject of this series of posts.\nHelpfully, various people/organisations have gone to the Hansard website to get the XML files they provide and made them available as an easy download (note that these tend to have been posted as they were provided by the parliament, so they’re full of typos, transcription errors, and a bunch of other mistakes):\nThese helpful people/organisations were able to get those dates (1901-1980 and 1998-current) because the Hansard provides the XML for those years on their website. The problem is the 1980s and the early/mid 1990s because they don’t have the XML available on the website (and from emails with them - it seems as though they simply don’t have it) - the only choice seems to be either to scrape it manually from the Hansard website or to grab all the PDFs, convert them, and then fix the mistakes. I’ve started on the second option - unsure how wise it is but I don’t know of any alternative.\nFuture posts:"
  },
  {
    "objectID": "posts/2018-08-13-cleaning-hansard/cleaning-hansard.html#footnotes",
    "href": "posts/2018-08-13-cleaning-hansard/cleaning-hansard.html#footnotes",
    "title": "Cleaning Hansard: The pay’s not great but the work is hard",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nFor instance: Turpin ‘An Attempt to Measure the Quality of Questions in Question Time of the Australian Federal Parliament’.↩︎\nFor instance: Beelen, Thijm, Cochrane, Halvemaan, Hirst, Kimmins, Lijbrink, Marx, Naderi, Rheault, Polyanovsky, Whyte ‘Digitization of the Canadian Parliamentary Debates’.↩︎\nFor instance: Curran, Higham, Ortiz, Vasques Filho ‘Look Who’s Talking: Bipartite Networks as Representations of a Topic Model of New Zealand Parliamentary Speeches’.↩︎\nFor instance: Abercrombie, Batista-Navarro ‘Aye or No? Speech-level Sentiment Analysis of Hansard UK Parliamentary Debate Transcripts’↩︎"
  },
  {
    "objectID": "posts/2021-12-17-james_webb_space_telescope/james_webb_space_telescope.html",
    "href": "posts/2021-12-17-james_webb_space_telescope/james_webb_space_telescope.html",
    "title": "What data science can learn from the James Webb Space Telescope",
    "section": "",
    "text": "The slides are here."
  },
  {
    "objectID": "posts/2021-12-17-james_webb_space_telescope/james_webb_space_telescope.html#introduction",
    "href": "posts/2021-12-17-james_webb_space_telescope/james_webb_space_telescope.html#introduction",
    "title": "What data science can learn from the James Webb Space Telescope",
    "section": "Introduction",
    "text": "Introduction\nHi, my name is Rohan Alexander. I’m an assistant professor at the University of Toronto in the Faculty of Information and the Department of Statistical Sciences. I’m also the assistant director of CANSSI Ontario, and I’d encourage you all to go to our website - https://canssiontario.utoronto.ca/ - and apply for funding from our programs.\nI’d like to thank Josh and Kartheik for the opportunity to talk today.\nI know that it’s an auspicious time for all you astronomy-inclined folks with the imminent launch of the James Webb Space Telescope, so I appreciate you coming to listen to me. Hopefully in a year, I can come back and you can tell me about all the wonderful datasets James Webb has provided all of you.\nToday I’d like to talk a little about what I see data science as, and why I think astronomers are so good at it; and then talk about a few examples of my work, focusing more on sharing my process and what I learnt, rather than the work itself; and finally close with some open questions.\nNone of what I’m about to say is cannon, this talk is more my way of trying to work out what I think, so I’d appreciate your reactions and comments."
  },
  {
    "objectID": "posts/2021-12-17-james_webb_space_telescope/james_webb_space_telescope.html#what-is-data-science",
    "href": "posts/2021-12-17-james_webb_space_telescope/james_webb_space_telescope.html#what-is-data-science",
    "title": "What data science can learn from the James Webb Space Telescope",
    "section": "What is data science?",
    "text": "What is data science?\nWhen we think about data science, I think that we all have different things in mind.\nThe only thing that is certain, is that there is no agreed definition of data science, but a lot of people have tried. For instance, Wickham and Grolemund (2016) say it is ‘…an exciting discipline that allows you to turn raw data into understanding, insight, and knowledge.’ Similarly, Leek and Peng (2020) say it is ‘…the process of formulating a quantitative question that can be answered with data, collecting and cleaning the data, analyzing the data, and communicating the answer to the question to a relevant audience.’ Benjamin S. Baumer and Horton (2021) say it is ‘…the science of extracting meaningful information from data.’ And Timbers, Campbell, and Lee (2021) say they define ‘data science as the process of generating insight from data through reproducible and auditable processes’.\nCraiu (2019), who is one of Josh’s bosses and also one of mine, argues that the lack of certainty as to what data science is, might not matter because ‘…who can really say what makes someone a poet or a scientist?’ He goes on to broadly say that a data scientist is ‘…someone with a data driven research agenda, who adheres to or aspires to using a principled implementation of statistical methods and uses efficient computation skills.’\nRegardless of who is right, alongside those specific, more-technical, definitions, there is value in having a simple definition, even if we lose a bit of specificity. For instance, probability is often informally defined as ‘counting things’ (McElreath 2020, 10). In a similar informal sense, data science can be defined as something like: ‘humans measuring stuff, typically related to other humans, and using sophisticated averaging to explain and predict’.\nThat may sound a touch cute, but Francis Edgeworth, the nineteenth century statistician and economist considered statistics to be the science ‘of those Means which are presented by social phenomena,’ so it is in good company (Edgeworth 1885).\nIn any case, one feature of this definition is that it does not treat data as terra nullius, or nobody’s land. Statisticians tend to see data as the result of some process that we can never know, but that we try to use data to come to understand. Many statisticians care deeply about data and measurement, but there are many cases in statistics where data kind of just appear; they belong to no one. But that is never actually the case.\nData must be gathered, cleaned, and prepared, and these decisions matter. Every dataset is sui generis, or a class by itself, and so when you come to know one dataset well, you just know one dataset, not all datasets.\nIn my experience, this fact is ingrained into astronomers. My guess is that it’s because most of you get into astronomy being the actual data collectors - that is, you loved looking at the sky through telescopes when you were a kid. Eventually you got more sophisticated and layered on physics and math, but I think that astronomers make such great data scientists because you’re fundamentally lashed to the data collection process.\nMore broadly, I think that much of data science focuses on the ‘science,’ but it is important that we also focus on ‘data.’ And that is another feature of my cutesy definition of data science which I posited before. A lot of data scientists are generalists, who are interested in a broad range of problems. Often, the thing that unites these is the need to gather, clean, and prepare messy data. And often it is the specifics of those data that require the most time, that update most often, and that are worthy of our most full attention. Unfortunately, it’s not typically the type of thing that is professionally rewarded."
  },
  {
    "objectID": "posts/2021-12-17-james_webb_space_telescope/james_webb_space_telescope.html#astronomical-origins",
    "href": "posts/2021-12-17-james_webb_space_telescope/james_webb_space_telescope.html#astronomical-origins",
    "title": "What data science can learn from the James Webb Space Telescope",
    "section": "Astronomical origins",
    "text": "Astronomical origins\nAt this point I’d like to look back, for a moment, at the origins of data science. As astronomers, I know that you all are used to looking back in time. My PhD is in economic history, so I’m also very keen on looking back in time also, although usually only decades or centuries, rather than the billions of years that you all tend to look back! Anyway, if we look at the history of statistics, we very quickly find ourselves in astronomy.\nFor instance, speaking about the development of least squares in the 1700s, Stigler (1986, 16) describes how it was associated with the following problems:\n\nDetermining the motion of the moon in a way that takes into account minor perturbations.\nReconciling the non periodic motion of Jupiter and Saturn.\nDetermining the shape of the earth.\n\nThe very stuff of astronomy!\nIn the work associated with least squares, we see statistical names that we still speak of today including Euler, Gauss, and Laplace. But it may be of interest to you all as astronomers that Stigler (1986) describes the papers at the time as ‘a story of statistical success (by a major astronomer, Mayer (Figure @ref(fig:iceberg)) and statistical failure (by a leading mathematician, Euler)’!\n\n\n\n\n\nExample of Mayer from Stigler, 1986\n\n\n\n\nThe fundamental issue at the time with least squares was that of hesitancy to combine different observations. These days, we just take this for granted and do it without thinking much about it. But if one steps back for a moment, one quickly sees why this was such a mental leap. And again, here we have astronomers, as you are so immersed in your data, as bringing a unique perspective.\nComparing Euler and Mayer, Stigler (1986, 28) says:\n\nThe two men brought absolutely first-rate intellects to bear on their respective problems, and both problems were in astronomy. Yet there was an essential conceptual difference in their approaches that made it impossible for Euler’…. The differences were these: Mayer approached his problem as a practical astronomer, dealing with observations that he himself had made under what he considered essentially similar observational conditions, despite the differing astronomical conditions. Euler, on the other hand, approached his problem as a mathematician, dealing with observations made by others over several centuries under unknown observational conditions. Mayer could regard errors, or variation, in his observations as random’… and he could take the step of aggregating equations without fear that bad observations would contaminate good ones’…. Euler could not bring himself to accept such a view with respect to his data. He ‘… [took] the mathematician’s view that errors actually increase with aggregation rather than taking the statistician’s view [and here Stigler should actually also add astronomers] that random errors tend to cancel one another.’…\n\nOn the other hand, Stigler (1986) criticism of Mayer is that ‘…He made no attempt to describe his calculation as a method that would be useful in other problems. He did not do what Legendre did, namely, abstract the method from the application where it first appeared.’ Namely astronomy!\nIn any case, by around 1800, least squares became more widely appreciated. But, oddly, not in my own area of interest—namely social sciences, where we would take another 70 or 80 years, a veritable lifetime, before we widely applied it.\nBefore we return to the present, I’d be remiss if I didn’t mention one final historical astronomer, Quetelet. In this talk I’m arguing that it is your foundation in data gathering that makes you such great data scientists. And this is going to be an example where that feature held back social sciences.\nQuetelet was a Belgium astronomer, but he also had social sciences interests and made foundational contributions in terms of the ‘average man’ and ‘fitting distributions’. I’d like to touch on a circumstance where his being deeply involved in the data was a handicap.\nStigler (1986, 163) describes how by 1826 Quetelet had become involved in the statistical bureau, and they were planning for a census. As some of you who have taken courses with my wife, Monica Alexander, know, the foundation of demography is births, deaths, and migration. Quetelet argued that births and deaths had been taken care of, but migration had not. He proposed a ratio estimator based on counts in specific geographies—think provinces—which could then be scaled up to the whole country. There was various criticism of this plan, mostly focused on how one could possibly pick appropriate geographies, and Stigler describes how Quetelet then changed his mind.\nStigler (1986) continues\n\nHe [Quetelet] was acutely aware of the infinite number of factors that could affect the quantities he wished to measure, and he lacked the information that could tell him which were indeed important. He, like the generation of social scientists to follow, was reluctant to group together as homogenous, data that he had reason to believe was not’…. In some respects, Quetelet’s view of his social data was like Euler’s view of astronomical data. To be aware of a myriad of potentially important factors, without knowing which are truly important and how their effect may be felt, is often to fear the worst’…. He [Quetelet] could not bring himself to treat large regions as homogeneous, [and so] he could not think of a single rate as applying to a large area’… Astronomers had overcome a similar case of intellectual cold feet in the previous century by comparing observations with fact, by comparing prediction with realization. Success had bolstered confidence in the combination of observations. Social scientists were to require massive empirical data gathered under a wide variety of circumstances before they gained the astronomers’ confidence that the quantities they measured were of sufficient stability that the uncertainty of the estimates was itself susceptible to measurement."
  },
  {
    "objectID": "posts/2021-12-17-james_webb_space_telescope/james_webb_space_telescope.html#on-balance-between-data-and-science",
    "href": "posts/2021-12-17-james_webb_space_telescope/james_webb_space_telescope.html#on-balance-between-data-and-science",
    "title": "What data science can learn from the James Webb Space Telescope",
    "section": "On balance between data and science",
    "text": "On balance between data and science\nReturning to the present, as I’ve been saying, one of the reasons that I think astronomers make such great data scientists, is that you are used to knowing that your data is a biased sample that comes to you via instruments—in your case telescopes.\nWe always use various instruments to turn the world into data. In astronomy, the development of better telescopes, and eventually satellites and probes, and soon, hopefully, James Webb, enabled new understanding of other worlds. For instance, returning to history again for just a moment, Stigler (1986, p.25) describes how knowledge of that second problem that I mentioned—Jupiter/Saturn non-periodic motion—was ‘due to improved accuracy of astronomical observations’. Returning to the present day, we similarly have new instruments for turning our own world into data being developed each day.\nIn the social sciences, a census was once a generational-defining event. And it’s appropriate that this talk happens near the Christmas holidays given the role that the census played in that event. But now we have regular surveys, transactions data available by the second, and almost all interactions on the internet become data of some kind. The development of such instruments has enabled exciting new stories to be told with data.\nThe telescope that we all hope is launching soon—James Webb—has a unique feature, compared with its predecessor in popular imagination - Hubble. And that feature is that it will be sent to L2. My understanding, and please correct me after the talk if I’m wrong, is that the defining feature of L2 is that the Earth’s gravity and the Sun’s gravity and the moon to a certain extent, effectively hold objects there without much in the way of rocket thrust being needed. So the side of the telescope with the instrumentation that needs to be kept cold and dark, can more easily always be kept cold and dark. And so, in this case James Webb can stay there for about 10 years, focusing on answering big questions, without much in the way of needing to worry about pointing the wrong way. We should all be so lucky!\nIn a way, for James Webb, being at L2 is all about getting the balance right so that it can focus on answering big questions. And I think for the past ten years or so, we’ve had the balance wrong in data science. There has been too much focus on the ‘science’ bit, without sufficient focus on the ‘data’ bit. And we all know what happens when you go too close to the sun.\nIt is not just the ‘science’ bit that is hard, it is the ‘data’ bit as well. I feel that Foster would have known this, and we are just reinventing things. For instance, researchers went back and examined one of the most popular text datasets in computer science, and they found that around 30 per cent of the data were inappropriately duplicated (Bandy and Vincent 2021). There is an entire field—linguistics—that specialises in these types of datasets, and inappropriate use of data is one of the dangers of any one field being hegemonic. The strength of data science is that it brings together folks with a variety of backgrounds and training to the task of learning about some dataset. It is not constrained by what was done in the past. But this means that we must go out of our way to show respect for those who do not come from our own tradition, but who are nonetheless as similarly interested in a dataset, or in a question, as we are.\nI’m picking on CS a little here, but my home of the social sciences is just as bad and researchers like me are trying to refocus us on data a little more than we have been in the past.\nSo what are some examples of things that I work on?"
  },
  {
    "objectID": "posts/2021-12-17-james_webb_space_telescope/james_webb_space_telescope.html#examples-of-my-work",
    "href": "posts/2021-12-17-james_webb_space_telescope/james_webb_space_telescope.html#examples-of-my-work",
    "title": "What data science can learn from the James Webb Space Telescope",
    "section": "Examples of my work",
    "text": "Examples of my work\nI mentioned that my PhD is in economic history, but I spent most of my PhD trying to deal with big text datasets. And when I say ‘dealing’ I mean using R to clean and tidy them, which was the work of years. My supervisors—John Tang, Tim Hatton, Martine Mariotti, and Zach Ward—gave me the freedom to do what I wanted, with regular weekly meetings. It’s not a topic that traditionally would have been appropriate in economics, and I’m grateful they gave me the space because traditional economics is not for me, but data science is.\n\nEffect of elections and changed prime ministers\nOne result of this work was the paper—The Increased Effect of Elections and Changing Prime Ministers on Topics Discussed in the Australian Federal Parliament between 1901 and 2018—(Alexander and Alexander 2021) which was co-authored with my wife Monica. In Australia there is a written record of what is said in parliament called Hansard. We grabbed the text of everything said between 1901 and 2018 and then built a statistical model to look at whether the topics of discussion changed when there was an election or when there was a change in the prime minister. We found that changes in prime minister tend to be associated with topic changes even when the party in power does not change; and the effect of elections has been increasing since the 1980s, regardless of whether the election results in a change of prime minister.\n\nLesson: Always start small and then iterate.\nI came to that paper hopelessly naive and lost. I didn’t know that text analysis, and natural language processing more generally, was something that was incredibly popular. I didn’t have the skills that I needed, and really all I had was an all-consuming interest in the broad area—I didn’t even have a question. The dataset has over a billion words in it. So the main lesson for me was in dealing with big datasets. And here I don’t mean in terms of technical skills, I mean in terms of approach. These days when I have students and they want to use the Hansard or any large dataset, I start by insisting that they just use one month of data. After they analyse and write that up, then they can go to a year, and then they can continue to build up slowly.\nThe reason that I know now that’s the best way to go is that I somehow convinced myself that I couldn’t possibly answer any interesting questions until I had a century worth of data. And because I was teaching myself everything as I went, it took me more than a year to just get the data into a usable format.\nIf you take nothing away from this talk, please I beg you take this away: the most important, vital thing, is that you create a minimal viable product of any research. And that minimal viable product needs to be something that you can finish within a week. If you can’t do that then adjust the scope and the question until you can. Then you achieve that MVP and then you start to scale up to the question that you’re interested in, which is hopefully something that can be published.\n\n\n\nExplaining Why Text is Sexist or Racist with GPT-3\nAs anyone who has cared for young children knows, the response to almost any statement can be ‘why?’. One can quickly find oneself trying to explain the finer details of photosynthesis to a 3-year-old, and the extent to which one struggles with this tends to put in sharp relief the extent of one’s knowledge. Large language models such as OpenAI’s GPT-3 can generate text that is indistinguishable from that created by humans. They can also classify whether some given text is sexist or racist (Chiu and Alexander 2021).\nIn the paper—Explaining Why Text is Sexist or Racist with GPT-3—which was co-authored by Ke-Li Chiu we assess the extent to which GPT-3 can generate explanations for why a given text is sexist or racist. We prompt GPT-3 to generate explanations in a question-and-answer manner: ‘Is the following statement in quotes sexist? Answer yes or no and explain why.’ We then assess the adequacy of the explanations generated by GPT-3. We are interested in firstly, the extent to which it correctly classifies whether the statements are sexist/racist; and secondly, the reasonableness of the explanation that accompanies that classification.\nWe find that GPT-3 does poorly in the open-ended approach. When we add more structure to guide its responses the model performs better. But even when it correctly classifies racism or sexism, the accompanying explanations are often inaccurate. At times they even contradict the classification. On a technical level, we find a clear relationship between the hyper-parameter temperature and the number of correctly matched attributes, with substantial decreases as temperature increases.\n\nLesson: Always teach\nI just used a bunch of terms there and you probably assume that I know what they mean. In writing the paper that preceded this one, I actually fooled myself into thinking that I knew what they meant. It wasn’t until I tried to teach the material that I realised that I didn’t have a clue what was going on.\nA lot of my colleagues try to get out of teaching, and I can understand why they would think that, but I’ve found that having to teach has made me a better researcher. For one reason, I found that it’s too easy to fool yourself into thinking you know something until you have to explain it. And I also kind of think that a lot of us in academia need pressure, constraints, and a weekly structure, in order to do our best work.\n\n\n\nReproducibility of COVID-19 pre-prints\nThe final paper that I’d like to touch on is—Reproducibility of COVID-19 pre-prints—by Annie Collins and me (Collins and Alexander 2021). In that paper we are interested in the reproducibility of COVID-19 research. We create a dataset of pre-prints posted to arXiv, bioRxiv, medRxiv, and SocArXiv between 28 January 2020 and 30 June 2021 that are related to COVID-19. We extract the text from these pre-prints and parse them looking for keyword markers signalling the availability of the data and code underpinning the pre-print. For the pre-prints that are in our sample, we are unable to find markers of either open data or open code for 75 per cent of those on arXiv, 67 per cent of those on bioRxiv, 79 per cent of those on medRxiv, and 85 per cent of those on SocArXiv. We conclude that there may be value in having authors categorize the degree of openness of their pre-print as part of the pre-print submissions process, and more broadly, there is a need to better integrate open science training into a wide range of fields.\n\nLesson: You always need the code more than once\nI went into that paper thinking that I’d just quickly write some code to download some files and then be done with it. But then Annie wanted to broaden the scope of the paper, so the code needed to be re-written because she couldn’t understand what I’d done. And then we had to re-run the code because we wanted to update everything before she presented the paper. And that meant re-writing things. And then we to re-run the code before we submitted the paper. Again that meant re-writing things.\nWe eventually just turned the code into an R package, which is on CRAN as heapsofpapers (Alexander and Mahfouz 2021). Which is what I should have done from the start. The lesson that I learnt from this paper is that regardless of how certain you are that you’ll never use some particular code again, you’ll always need it."
  },
  {
    "objectID": "posts/2021-12-17-james_webb_space_telescope/james_webb_space_telescope.html#open-questions",
    "href": "posts/2021-12-17-james_webb_space_telescope/james_webb_space_telescope.html#open-questions",
    "title": "What data science can learn from the James Webb Space Telescope",
    "section": "Open questions",
    "text": "Open questions\n\nHow do we write unit tests for data science?\nOne thing that working with real computer scientists has taught me is the importance of unit tests. Basically this just means writing down the small checks that we do in our heads all the time. Like if we have a column that purports to the year, then it’s unlikely that it’s a character, and it’s unlikely that it’s an integer larger than 2500, and it’s unlikely that it’s a negative integer. We know all this, but writing unit tests has us write this all down.\nIn this case it’s obvious what the unit test looks like. But more generally, we often have little idea what our results should look like if they’re running well. The approach that I’ve taken is to add simulation—so we simulate reasonable results, write unit tests based on that, and then bring the real data to bear and adjust as necessary. But I really think that we need extensive work in this area because the current state-of-the-art is lacking.\n\n\nWhat happened to the revolution?\nI don’t understand what happened to the promised machine learning revolution in social sciences. Specifically, I’m yet to see any convincing application of machine learning methods that are designed for prediction to a social sciences problem where what we care about is understanding. I would like to either see evidence of them or a definitive thesis about why this can’t happen. The current situation is untenable where folks, especially those in fields that have been historically female, are made to feel inferior even though their results are no worse.\n\n\nHow do we think about power?\nAs someone who learnt statistics from economists, but now is partly in a statistics department, I do think that everyone should learn statistics from statisticians. This isn’t anything against economists, but the conversations that I have in the statistics department about what statistical methods are and how they should be used are very different to those that I’ve had in other departments.\nI think the problem is that people outside statistics, treat statistics as a recipe in which they follow various steps and then out comes a cake. With regard to ‘power’—it turns out that there were a bunch of instructions that no one bothered to check—they turned the oven on to some temperature without checking that it was 180C, and that’s fine because whatever mess came out was accepted because the people evaluating the cake didn’t know that they needed to check the temperature had been appropriately set. (I’m ditching this analogy right now).\nAs you know, the issue with power is related to the broader discussion about p-values, which basically no one is taught properly, because it would require changing an awful lot about how we teach statistics i.e. moving away from the recipe approach.\nAnd so, my specific issue is that people think that statistics is a recipe to be followed. They think that because that’s how they are trained especially in social sciences like political science and economics, and that’s what is rewarded. But that’s not what these methods are. Instead, statistics is a collection of different instruments that let us look at our data in a certain way. I think that we need a revolution here, not a metaphorical tucking in of one’s shirt."
  },
  {
    "objectID": "posts/2021-12-17-james_webb_space_telescope/james_webb_space_telescope.html#thank-you",
    "href": "posts/2021-12-17-james_webb_space_telescope/james_webb_space_telescope.html#thank-you",
    "title": "What data science can learn from the James Webb Space Telescope",
    "section": "Thank you",
    "text": "Thank you\nRight. So I think I’ll leave it there. Thank you again to Josh and Kartheik for inviting me.\nLet’s cross our fingers for a successful launch of James Webb next week and its subsequent deployment. I do hope that you invite me back when you start getting data from it to analyse!\nThis is also my last talk for this year, so I’ll take this opportunity to say Merry Christmas, Happy Holidays, and I hope everyone has a wonderful start to 2022.\nI’d be happy to take any questions."
  },
  {
    "objectID": "posts/2021-12-17-james_webb_space_telescope/james_webb_space_telescope.html#acknowledgments",
    "href": "posts/2021-12-17-james_webb_space_telescope/james_webb_space_telescope.html#acknowledgments",
    "title": "What data science can learn from the James Webb Space Telescope",
    "section": "Acknowledgments",
    "text": "Acknowledgments\n\nThanks very much to Josh for the invitation to speak, and to Monica Alexander for her suggestions and comments."
  },
  {
    "objectID": "posts/2021-03-13-grad-school-applications/grad-school-applications.html",
    "href": "posts/2021-03-13-grad-school-applications/grad-school-applications.html",
    "title": "Saturday morning thoughts on grad school applications",
    "section": "",
    "text": "Grad school application season is mostly done. I wanted to quickly put together some general thoughts in case they’re helpful for undergraduates applying next year. Please keep in mind that I’m new and interested in applied statistics. This advice probably doesn’t apply for more-experienced, or less-applied, folks.\nThe main takeaways:\n\nThere’s a huge amount of luck involved, so don’t tie your self-worth to any particular decision either way.\nWorry as much, if not more, about letters and your personal statement as you do about your GPA. There are three bits to the application — transcript, letters, statement/CV. You’re spending four years getting transcripts, so you should also spend a lot of time on the other two.\nCommunicate a clear reason for wanting to go to the school/program that you’re applying for.\nCreate a narrative that runs through your entire application.\n\nThese thoughts are specific to me. You need to get advice from your own advisors, and in particular the faculty that are going to write you letters, but more on that later."
  },
  {
    "objectID": "posts/2021-03-13-grad-school-applications/grad-school-applications.html#overview",
    "href": "posts/2021-03-13-grad-school-applications/grad-school-applications.html#overview",
    "title": "Saturday morning thoughts on grad school applications",
    "section": "",
    "text": "Grad school application season is mostly done. I wanted to quickly put together some general thoughts in case they’re helpful for undergraduates applying next year. Please keep in mind that I’m new and interested in applied statistics. This advice probably doesn’t apply for more-experienced, or less-applied, folks.\nThe main takeaways:\n\nThere’s a huge amount of luck involved, so don’t tie your self-worth to any particular decision either way.\nWorry as much, if not more, about letters and your personal statement as you do about your GPA. There are three bits to the application — transcript, letters, statement/CV. You’re spending four years getting transcripts, so you should also spend a lot of time on the other two.\nCommunicate a clear reason for wanting to go to the school/program that you’re applying for.\nCreate a narrative that runs through your entire application.\n\nThese thoughts are specific to me. You need to get advice from your own advisors, and in particular the faculty that are going to write you letters, but more on that later."
  },
  {
    "objectID": "posts/2021-03-13-grad-school-applications/grad-school-applications.html#cv-and-personal-statement",
    "href": "posts/2021-03-13-grad-school-applications/grad-school-applications.html#cv-and-personal-statement",
    "title": "Saturday morning thoughts on grad school applications",
    "section": "CV and personal statement",
    "text": "CV and personal statement\nBe clear about why you’re applying, show evidence of your work, and weave that into your narrative. When you do sales, the key to everything is identifying a ‘champion’ in the firm that you want to sell to. Your job in sales is then to give that person the information they need to sell the rest of the firm on it. When you’re applying to grad school, it’s a similar situation. You don’t need all the faculty to love your application - you just need at least one of the reviewers to love it and for no one to hate it enough to veto. Your personal statement is a sales opportunity and will be when the faculty reviewers work out who they advocate for. It’s hard to recover from a bad personal statement. You should get your letter writers to review it before you send it.\n\nUse the combination of your CV and personal statement to craft a narrative (which your letters and transcript support).\nArticulate the reason that you want to attend the department/school that you’re applying for.\n\nThis is not a statement like ‘[insert school] is a top-ranked school’.\nOne way to do this is to find a faculty member that is of interest, look at their papers and website, and write about how your interests align with their work and you want to be involved in it. That said, don’t fake interest, because it’s usually obvious.\nAnother way is to identify a clear link to your interests, for instance, if you’re into public health then maybe the university is in a state/province that has universal healthcare and so you’d have great data.\nYet another way is to look up the requirements of the program and weave in how you’re really keen to learn about subject X taught by professor Y because of reason Z.\n\nYou don’t need the world’s greatest justification (and no one will hold you to it if you decide you’re actually interested in something else), but the faculty reviewer needs to know why you want to attend their school/department when you could go to any number of other top ranked schools/departments. There’s only a certain number of offers that can be made, so at the very least they need to have some idea of the likelihood that you’ll accept.\nGrades in undergrad tests don’t necessarily correspond to doing well in graduate school. Much of graduate school is about learning how to learn, with the goal to eventually ‘learn things that no one else knows’ (i.e. do research) and so it is helpful for reviewing faculty to see evidence that points to your ability to do that. One way to do this is to create projects that show off what you can do publicly and link to them. Ideally your undergrad professors will integrate projects into their curriculum, but if not then you need to DIY. (BTW don’t use Kaggle – hunt, gather, or farm your own data).\nFor applied quantitative PhDs, it’s great if you are already able to write code in R or Python, and even better if you can include a link to your GitHub account that shows this. Ideally, your GitHub should be in decent order. For instance, have your best projects pinned, and put a lot of effort into making sure these are commented, have READMEs, etc.\nMake a website, even if it’s very minimal. You need to control your online presence and having LinkedIn or whatever come up is not great (for the purposes of grad school applications).\nOne or two years of work experience is great (or even just a summer internship or similar), especially if it links into your overall narrative of why you want to go to that particular grad school.\nDon’t have a long personal statement that begins with you as a child and culminates in your application. Instead, be succinct and clear, and weave your accomplishments into a narrative, rather than just list them (that’s what your CV/transcript are for)."
  },
  {
    "objectID": "posts/2021-03-13-grad-school-applications/grad-school-applications.html#letters",
    "href": "posts/2021-03-13-grad-school-applications/grad-school-applications.html#letters",
    "title": "Saturday morning thoughts on grad school applications",
    "section": "Letters",
    "text": "Letters\nThe best letters are from folks that know you and your work. You need to spend a huge amount of effort getting decent letters. They’re as important, if not more, as your GPA.\n\nThe strongest letters are from folks who know you well. Usually, this type of letter looks better than someone who doesn’t know you well, even if they’re a famous professor, and even if you got an A in their class.\nIf possible try to work for a professor, ideally a quantitative one, because it can be more difficult to understand the evidence that non-quantitative-professors bring.\n\nHow do you get a job with a professor so that they will write you a letter? The best way is to excel in a course that they teach and then ask to work for them. Or excel in a course and then ask that professor to recommend you to the professor you want to work for.\nThere’s a language that good letters use. You need to be very thoughtful in your choice of letter writers because you can’t control what is written about you, other than through your choice of letter writer.\n\nThat said, not everyone can work for a professor. The main point is that you need to identify letter writers that really know your work, and you.\nBe sure to ask the letter writer if they’re comfortable recommending you to the programs that you’re applying for before you ask them to write a letter for those programs.\nThere’s a tendency to ask the most-famous-professor to write you a letter. If you’ve worked closely with them and they’ll say nice things about you, then that’s great. But the thing about famous professors is that they have seen a lot of strong students, and so it can be dangerous. Professor Big-Name saying you’re the best student they’ve ever worked with is of course great. Professor Big-Name saying you were fine to work with is still good, but less clear-cut. Professor Big-Name saying you got an A in their class doesn’t say much that isn’t already in your transcript."
  },
  {
    "objectID": "posts/2021-03-13-grad-school-applications/grad-school-applications.html#transcript",
    "href": "posts/2021-03-13-grad-school-applications/grad-school-applications.html#transcript",
    "title": "Saturday morning thoughts on grad school applications",
    "section": "Transcript",
    "text": "Transcript\nGrades are important, but not everything. Your transcript needs to reflect the narrative that you’re telling in the rest of the application.\n\nIf you are not a 4.0 student, then it’s not the end of the world. But be aware of:\n\nGPA minimums, and\nThat your grades at the end of your degree and/or relevant subjects weigh more heavily.\n\nGrades are important, but students seem to over-weight the importance of grades – all the components of the application are important, and grades are just one part of it."
  },
  {
    "objectID": "posts/2021-03-13-grad-school-applications/grad-school-applications.html#concluding-remarks",
    "href": "posts/2021-03-13-grad-school-applications/grad-school-applications.html#concluding-remarks",
    "title": "Saturday morning thoughts on grad school applications",
    "section": "Concluding remarks",
    "text": "Concluding remarks\nGetting in anywhere requires a healthy dose of luck. It’s not a reflection of your worth as an academic, and especially not as a person. You can’t control who reviews your application, or who has funding to take students that year, or the other students applying that year. There’s also just incredible competition for these spots. There’s an awful lot of chance involved. There’s nothing you can do about this other than not tie your self-worth to it."
  },
  {
    "objectID": "posts/2021-03-13-grad-school-applications/grad-school-applications.html#acknowledgments",
    "href": "posts/2021-03-13-grad-school-applications/grad-school-applications.html#acknowledgments",
    "title": "Saturday morning thoughts on grad school applications",
    "section": "Acknowledgments",
    "text": "Acknowledgments\n\nWithout wanting to implicate her in anything said here, thanks to Monica Alexander for reading many drafts of this."
  },
  {
    "objectID": "posts/2021-05-22-baby-steps/baby-steps.html",
    "href": "posts/2021-05-22-baby-steps/baby-steps.html",
    "title": "Opportunities Provided by Open Data and Reproducibility",
    "section": "",
    "text": "Hi, my name is Rohan Alexander. Thank you very much to Gwen and Josh for the opportunity to speak. I’m a baby professor at the University of Toronto, in the Faculty of Information and the Department of Statistical Sciences. Today I would like to talk about taking some baby steps toward open data and reproducibility. And in particular the opportunities provided by open data and reproducibility across applied statistics and astronomy.\nI’d like to talk about three benefits of openness and reproducibility:\n\nimproved diversity broadly;\nimproving your own (future) life; and\nbuilding on-ramps for your collaborators.\n\nAnd then three baby steps that you could get started with this afternoon:\n\nbuy a bunch of notebooks;\nwrite a datasheet; and\nmake a software package."
  },
  {
    "objectID": "posts/2021-05-22-baby-steps/baby-steps.html#introduction",
    "href": "posts/2021-05-22-baby-steps/baby-steps.html#introduction",
    "title": "Opportunities Provided by Open Data and Reproducibility",
    "section": "",
    "text": "Hi, my name is Rohan Alexander. Thank you very much to Gwen and Josh for the opportunity to speak. I’m a baby professor at the University of Toronto, in the Faculty of Information and the Department of Statistical Sciences. Today I would like to talk about taking some baby steps toward open data and reproducibility. And in particular the opportunities provided by open data and reproducibility across applied statistics and astronomy.\nI’d like to talk about three benefits of openness and reproducibility:\n\nimproved diversity broadly;\nimproving your own (future) life; and\nbuilding on-ramps for your collaborators.\n\nAnd then three baby steps that you could get started with this afternoon:\n\nbuy a bunch of notebooks;\nwrite a datasheet; and\nmake a software package."
  },
  {
    "objectID": "posts/2021-05-22-baby-steps/baby-steps.html#three-benefits",
    "href": "posts/2021-05-22-baby-steps/baby-steps.html#three-benefits",
    "title": "Opportunities Provided by Open Data and Reproducibility",
    "section": "Three benefits",
    "text": "Three benefits\nBy way of background, M. Alexander (2019) says ‘research is reproducible if it can be reproduced exactly, given all the materials used in the study’. She goes on:\n\nReproducibility is not just publishing your analysis code. The entire workflow of a research project––from formulating hypotheses to dissemination of your results––has decisions and steps that ideally should be reproducible.\n\nOpen Data Institute (2017) defines open data as ‘data that’s available to everyone to access, use and share.’ Open data enables reproducibility and is the bedrock of progress.\n\nImproved diversity\nThe first benefit that I see is improved diversity, broadly, in our fields. By making our work accessible to more people, we help to to make education and research resources more equitable. In Canadian STEM fields diversity at an undergraduate level is not great, but it’s also not too bad—we reflect broader Canadian society to some extent—but at each successive level we significantly worsen (Villemure and Webb 2017). In my own Department of Statistical Sciences, University Professor Nancy Reid was the only tenured research-track woman in the department for decades. And in astronomy, speaking more broadly, Prescord-Weinstein (2021, 132) says ‘I’ve never met a Black woman professor in the field of theoretical cosmology because I am the first—not just the first professor, but also the first Black woman cosmology theory PhD’1.\nImproving diversity is important because diverse teams are associated with higher performance (Hunt et al. 2020). That’s just a correlation, but thinking about my own experiences, I do think that diversity in my own team improved my productivity. For instance, I’ve found that students with different experiences push back and criticize me about things that I haven’t considered. Improving those aspects makes the work better.\nAs an aside, I also think that as Canada’s best university, and a public one at that, we have a duty to be more reflective of Canada. There’s a quote from one of my favourite books that I thought this audience would like, because it features an astronomer is talking to a child, trying to convince him that humanities are important, and the astronomer says:\n\nAny science is expensive and astronomy is more expensive than most…. If you need hundreds of millions of pounds sooner or later you are going to have to talk to people who don’t understand what you’re doing and don’t want to understand because they hated science at school.\nDeWitt (2000, 396)\n\nIt’s entirely correct that these people are in charge because that’s who we voted for. But it would be nice if they didn’t hate astronomy or statistics when they take it at university and possibly part of that is reflecting other experiences than just our own.\n\n\nImproving your own (future) life\nThe second benefit is that ‘future-you’ will be helped. I think that most of us who write code to analyse data for a living have had the feeling of coming back to a project after six months and a lot of the time it’s literally like having to start a new project. The variables make no sense, and the code is unintelligible. The main question for me is always ‘why did I do that?’. And then I spend the afternoon re-coding and almost always end up back where I was.\nIf we’re spending eight hours a day writing code to analyse data, then almost anything that adds even one per cent to our productivity is worthwhile, let alone something that saves an afternoon. And that’s particularly the case with open data and reproducibility, because these benefits tend to compound over time and accrue not just to you but other researchers. Talking about knowledge that only you have, doesn’t make you knowledgeable, it makes you a crank, and that includes knowledge that only ‘past-you’ has.\n\n\nBuilding on-ramps\nThe third benefit is that adopting open data and reproducibility can allow us to build better on-ramps for our collaborators. I love applied statistics, and I can’t quite believe that I get paid to do this job, and I really want more people to be able to work with me on my projects. These days I get to work with a lot of students, and I think the best way for me to make it easier for them to get up to speed is to adopt open data and reproducibility principles.\nI don’t think of myself as a scary or intimidating person, but I’m regularly told that in anonymous surveys. New, especially undergraduate, collaborators may be hesitant to ask questions because they feel awkward, or they feel they should know the answer or that they worry I’ll think they’re dumb. Some thoughts that may run through their head when they watch me talk through some data analysis: “Why did he remove the 99s?”, “Maybe everyone does that?”, “Can I just look this up after he finishes talking?”, “Is he ever going to finish talking?”, “Oh no, now I missed it, what is he saying now?”. Open data and reproducibility enable them to not be reliant on me and I hope makes it easier for my collaborators to work with me."
  },
  {
    "objectID": "posts/2021-05-22-baby-steps/baby-steps.html#three-baby-steps",
    "href": "posts/2021-05-22-baby-steps/baby-steps.html#three-baby-steps",
    "title": "Opportunities Provided by Open Data and Reproducibility",
    "section": "Three baby steps",
    "text": "Three baby steps\nRight, so you’re convinced! You want these great benefits! How can you go about getting them? I’m not asking for anything big from you, and I’m not asking you to advocate, or even change much of anything that you do. (Indeed, if you’re faculty then I’m just asking you to spend some research funds and hire some undergrads!2)\n\nBuy a bunch of notebooks\nThe first is to buy a notebook. (If you’re faculty, then go and buy a bunch for your team.) And then just write one dot point each time you write some code chunk. Let’s say you’re cleaning some text data, and you want to change all the instances of ‘Rohan’ to ‘Monica’. Just before you write the code that does that, or perhaps just after, write a simple dot point in that fancy notebook that you bought that explains what you did and why. That’s it. (Don’t worry too much about what you write initially – you’ll get better at it naturally over time.) At the end of the day, you’ll magically have a plain-English list of everything that was done to the dataset. That can easily be added to an appendix or added as comments and documentation alongside the code. Newton and Da Vinci kept notebooks! And if that doesn’t convince you (and it shouldn’t, see: selection bias) the US NIH describes notebooks in science as ‘legal documents’ that support claims to patents, defend against allegations of fraud, and act as your scientific legacy (Ryan 2015).\n\n\nWrite a datasheet\nThe second thing is writing a datasheet for your dataset. I had a quick look at some of the astronomy data repos and there’s some amazing things around, NASA came up immediately of course, but also a whole bunch associated with folks speaking at this workshop! I’ve never used astronomical datasets, but when I use political or economic datasets of a similar nature, I sometimes find that because I didn’t collect, clean, or prepare the dataset myself, it can be difficult to trust it. This is where datasheets come in (Gebru et al. 2020). Datasheets are basically nutrition labels for datasets. It’s really important to understand what you’re feeding your model, but plenty of researchers don’t have any idea. For instance, recently researchers went back and wrote a datasheet for one of the most popular datasets in computer science, and they found that around 30 per cent of the data were duplicated (Bandy and Vincent 2021)!\nInstead of telling you how unhealthy various foods are, a datasheet tells you things like:\n\n‘Who created the dataset and on behalf of which entity?’\n‘Who funded the creation of the dataset?’\n‘Does the dataset contain all possible instances or is it a sample (not necessarily random) of instances from a larger set?’\n‘Is any information missing from individual instances?’\n\nI’ve integrated datasheets into my teaching and an example of one that a student wrote earlier in Winter 2021 is Rosenthal (2021). I’m not saying that this isn’t helpful for the student who made it—I hope it was—but it’s especially helpful for me when I think about how best to use the dataset that he created.\nThe speaker after me, Renée Hlozek, has a bunch of papers documenting datasets, for instance LSST Dark Energy Science Collaboration et al. (2021), and another about data challenges (Hložek 2019) that I think would be interesting to combine with the idea of backfilling datasheets.\n\n\nMake a software package.\nThe third and final thing is to build out internal APIs for your code and then make them external. Now this may sound intimidating, but you can do it! (Or if you’re faculty, again, it’s something that you can have an undergraduate do.) My PhD is in economic history and basically what that means is that I have a PhD in data gathering and cleaning. To my shame, I have literally written code that ‘downloads a PDF from a URL, saves it to my computer, pauses, and goes and gets another PDF from a very similar URL’, literally hundreds of times. And of course, no one should write the same code hundreds of times.\nLast summer I found myself asking a student to go and write code to do that same task and I finally decided that enough was enough. Instead, I had them put together an R package. Now this was literally only the work of a week for them. So instead of everyone in the lab writing code each time they needed to download a PDF, they could just call this R package and use that instead: we wrote an internal API. I decided that I wanted to make it external facing and there’s now an R package—‘heapsofpapers’—that anyone can use (R. Alexander and Mahfouz 2021).\nIf you want to do this and you use R then just follow ‘Chapter 2 - The Whole Game’ from Wickham and Bryan (2021). Within a few hours you can have a workable solution and within a month or two you can have a great external API.\nAnd for those of you who are more Python focused, the speaker before me, Jo Bovy, has all these great Python packages on his GitHub, including a whole course on writing Python packages (Bovy 2021).\nMaking internal APIs external is one reason that Amazon is worth a trillion dollars (Benzell, Lagarda, and Van Alstyne 2017), and while I can’t promise that it’ll make you a millionaire, I can promise that you’ll get a paper out of it, and more importantly, help your field."
  },
  {
    "objectID": "posts/2021-05-22-baby-steps/baby-steps.html#concluding-remarks",
    "href": "posts/2021-05-22-baby-steps/baby-steps.html#concluding-remarks",
    "title": "Opportunities Provided by Open Data and Reproducibility",
    "section": "Concluding remarks",
    "text": "Concluding remarks\nI think that open data and reproducibility can be intimidating. I have a two-year-old and I imagine that when he first started to walk he was pretty intimidated also. But he did take his first steps and now he runs around the whole day. And I assure you that it’s the same for open data and reproducibility.\nI’m looking forward to diving further into the work of the other speakers at this workshop, and thank Gwen and Josh for bringing these two communities together, and for letting me speak today."
  },
  {
    "objectID": "posts/2021-05-22-baby-steps/baby-steps.html#acknowledgments",
    "href": "posts/2021-05-22-baby-steps/baby-steps.html#acknowledgments",
    "title": "Opportunities Provided by Open Data and Reproducibility",
    "section": "Acknowledgments",
    "text": "Acknowledgments\n\nThank you very much to Monica for reading a draft of this. If you want something to watch after reading this then I recommend M. Alexander (2021)"
  },
  {
    "objectID": "posts/2021-05-22-baby-steps/baby-steps.html#footnotes",
    "href": "posts/2021-05-22-baby-steps/baby-steps.html#footnotes",
    "title": "Opportunities Provided by Open Data and Reproducibility",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThanks very much to Vianey Leos Barajas for gifting me this book.↩︎\nStatistical Sciences has something in the order of 4,000 undergrads, which is quite a lot, but the great thing about it is that the top 5 per cent are just incredibly strong. You could hire a few of them for literally $25 an hour, and then just get them to do these three things! Possibly that hiring would even add to the diversity of your team.↩︎"
  },
  {
    "objectID": "posts/2015-08-31-prepare-for-future-economic-crises-now/prepare-for-future-economic-crises-now.html",
    "href": "posts/2015-08-31-prepare-for-future-economic-crises-now/prepare-for-future-economic-crises-now.html",
    "title": "Prepare For Future Economic Crises Now",
    "section": "",
    "text": "Originally published in the Canberra Times.\nFew policymakers were prepared for the financial crisis of 2007-08. Until it hit, their focus was on more obvious threats to the economy, instead of such an unexpected event. Could this be because planning for unexpected economic events is not the explicit responsibility of any particular policy-maker? If so, this has to change.\nAfter the collapse of the US housing market, Lehman Brothers’ bankruptcy became an economic Rorschach test. Policymakers at the Reserve Bank of Australia and the Australian Treasury saw it as evidence that some banks cannot be allowed to fail. In response, the RBA reduced the cash rate and the Treasury implemented a stimulus package.\nWith the benefit of hindsight, some (such as Nobel Laureate Paul Krugman) argue that we should have anticipated the collapse. This is more than just wishful thinking. We now know there were those (for instance Raghuram Rajan, now the Governor of the Reserve Bank of India) whose warnings were more or less spot-on. Unfortunately, they were mostly ignored. It just seemed too unlikely that trouble in the US housing market could cause widespread recessions.\nIn an effort to prevent a repeat of the financial crisis, policymakers at the RBA, Treasury and elsewhere have taken steps to mitigate the weaknesses of the financial sector. And they have studied how those weaknesses could affect the broader economy. But there is a danger that they will overlook economic threats that do not originate in the financial sector. Like a general who trains troops to fight past wars, we may again be caught unprepared.\nThe financial crisis taught us that even infallible elements of an economy can fail. But because such failure is difficult to imagine ex ante, there appears to have been little attempt to act on this lesson. One way would be to practise dealing with unfamiliar – and less obvious – economic threats.\nThe process should begin by systematically examining the economy for weaknesses beyond the financial sector. Policymakers need to identify which elements of the economy pose systemic risks. We now know about the financial sector and there are many initiatives to deal with its risks, but we need to search for others.\nPolicymakers need to stress-test elements of the economy that appear robust and unfaltering, not just the financial sector. And they should conduct scenario analysis to practise responding if those elements were to fail. Just as the possibility that the US sub-prime housing market could cause recessions was almost unfathomable in 2006, the cause of the next economic crisis could seem unlikely to us now. Policymakers need to search broadly for possibilities.\nOf course most of the scenarios that are considered will never occur, and policymakers should not be given carte blanche to make legislative changes. What is needed is less heavy-handed but more useful: policymakers need to publicly consider and practise reacting to, unexpected events. This makes it more likely that they will react well when an unexpected crisis hits.\nThe financial crisis made it clear that unexpected elements of an economy can fail. Policymakers should be better prepared for this. They must fix the weaknesses identified by the financial crisis and deal with current threats. But importantly, policymakers must also give consideration to more obscure risks. It is only through this effort that we can hope to build an economy that is more resilient to future crises."
  },
  {
    "objectID": "posts/2021-11-30-review_of_timbers/timbers_review.html",
    "href": "posts/2021-11-30-review_of_timbers/timbers_review.html",
    "title": "Review of ‘Data Science: A First Introduction’",
    "section": "",
    "text": "“Data Science: A First Introduction” by Tiffany-Anne Timbers, Trevor Campbell, and Melissa Lee from University of British Columbia’s Department of Statistics is the first-year data science textbook by which all others will be judged. It is a kind, yet rigorous, textbook that provides a foundation on which students can quickly answer exciting questions. One can imagine the authors saying to a new student ‘I’ve spent my life learning all this exciting stuff, and I can’t wait to share it with you’.\nThe book is effectively divided into three parts: the first four chapters focus on data; the next six chapters go through statistical methods; and the final three brief chapters introduce tools such as Jupyter, version control, and local installation.\nChapter 1 focuses on R and the tidyverse and is based around a dataset of languages spoken in Canada. It introduces key verbs, such as filter, select, arrange, and slice, as well as ggplot2. Chapter 2 covers how to get data into R; initially using CSV and TSV files, but then turning to SQL. This early focus on SQL (starting at p. 41) is a key feature of this book, and one that will pay dividends for students as they progress with subsequent courses. This chapter again uses the Canadian languages dataset. Chapter 3 focuses on cleaning and wrangling data within the tidy data framework. Key verbs including mutate, summarize, map, pivot_wider, and pivot_longer are introduced in this chapter, as is the base pipe operator |&gt;. This example of the authors’ decision to use the latest innovation, is reflected throughout the book in many other choices. The chapter uses a dataset of Canadian city populations and the Canadian languages dataset. Chapter 4 focuses on data visualization with ggplot2, especially scatter plots, line plots, bar plots, histograms, and how to improve on the default plots. The chapter uses the Old Faithful eruptions dataset and the Michelson’s speed of light dataset. The pages in this chapter that tell students how to explain a visualization (pp. 168-169) are an absolute treat.\nChapters 5 through to 10 focus on statistical methods. Chapters 5 and 6 are focused on classification, firstly introducing K-nearest neighbors, and then using this to introduce tidymodels and data preprocessing, which Chapter 6 then builds on to focus on test and training datasets, evaluation, and tuning, including lovely discussions of pre-processing and cross-validation. These chapters use the breast cancer dataset. There is a very nice discussion of what it means to be ‘good’ when it comes to accuracy on p. 224. Chapter 7 introduces regression using K-nearest neighbors and discusses underfitting and overfitting. It uses a real estate dataset from Sacramento. Chapter 8 turns to classical linear regression including simple and multivariable. There is some discussion of multicollinearity and outliers as well as prediction. This chapter again uses the Sacramento real estate dataset. Chapter 9 focuses on clustering within the context of K-means, including thorough discussions of how it all works, and some of the limitations. It uses the Palmer Penguins dataset. Finally, Chapter 10 discusses statistical inference, and sampling, including a lovely treatment of bootstrapping. It uses a dataset from Airbnb.\nChapters 11 through to 13 focus on tools. Chapter 11 introduces Jupyter including execution and markdown. Chapter 12 introduces version control and GitHub, covering not just commit, push, and pull, but also dealing with GitHub PATs, cloning, and collaboration including merge conflicts. Finally, Chapter 13 briefly covers installing all this locally, on the assumption that the reader to this point has been able to use a cloud solution that was already set-up for them.\nTo a certain extent data science textbooks are playing catch-up: we’ve long had methods texts in discipline-specific areas, but data sciences today is largely following demand from research/industry to move these methods into other areas – what discipline at a university covers real estate pricing? Into that void, Timbers, Campbell, and Lee, is the standard by which all other introductory data science textbooks will be judged. It is a rare immediate addition to the pantheon of introductory data science textbooks released (or updated) in the past five years, taking its place alongside R4DS, ISLR, and Statistical Rethinking. It introduces data science in a way that specifies exactly what a student needs and anticipates many of their questions. Its content will represent table stakes in terms of what we expect of students after they take first-year classes.\nI made it 10 per cent of the way through Timbers et al before I learnt something new. Frankly I was surprised I made it so far. Data science pedagogy has been so disjoint and so many of us are self-taught that it is refreshing to have a class-room-tested textbook that is focused on workflows and reproducibility. The approaches are rigorous and opinionated, and the text is filled with kindness and warmth. It is the book that I wish I had when I first came to learn this material. The book is unashamedly focused on the newest innovations including tidymodels and the native pipe operator, and I soon found myself learning things, on average, at roughly one-thing-per-page, which was an exciting experience for someone who spends his days doing and teaching data science in R. This is a text that I can see myself coming back to regularly, not just in my teaching, but as a reference. I am hopeful that the authors will go on to write “Data Science”, and “Advanced Data Science”, without too much delay!\nI mentioned kindness and warmth earlier, but it permeates the book. And there are many sections that proactively address questions that new students often have. For instance, p. 11 when a dataset is assigned to a name, the authors acknowledge how perplexing it can be to a student that nothing happens. Pleasingly the authors place reproducibility front-and-center of data science. For instance, the use of seeds is emphasised throughout the text, as is the importance of code that others can run.\nThere is extensive use of ‘Notes’ to separate more reflective content. One can imagine many of these are the result of the countless hours the authors have spent teaching this material. More generally, all aspects are clearly explained and built-up slowly. Chapter 3, which focuses on tidy data, is a particularly strong example of this. The text places potential research questions throughout, which may help retain student interest throughout.\nWhile it’s clear that I think the book is great, there are a few areas there I wish they had expanded on a little. For instance, on p. 3 Timbers et al say ‘when you work with data, it is essential to think about how the data were collected, which affects the conclusions you can draw. If your data are biased, then you results will be biased.’ But there is not much coverage of this throughout the book. I wonder if subsequent editions of this book could consider more explicit coverage of ethics especially around data collection, following say, D’Ignazio and Klein Data Feminism, and also around applications of these methods, following say, O’Neil Weapons of Math Destruction? Similarly, it may make sense to introduce sampling concepts alongside data collection.\nA free version of the textbook is available here: https://ubc-dsci.github.io/introduction-to-datascience/ and it is forthcoming from CRC Press. When it’s available I’d recommend that everyone buy it and assign it in their classes."
  },
  {
    "objectID": "posts/2021-11-30-review_of_timbers/timbers_review.html#acknowledgments",
    "href": "posts/2021-11-30-review_of_timbers/timbers_review.html#acknowledgments",
    "title": "Review of ‘Data Science: A First Introduction’",
    "section": "Acknowledgments",
    "text": "Acknowledgments\n\nI was provided with a PDF version of the book for the purposes of writing a blurb. There are no other financial disclosures. I printed and bound it at my own expense. At my invitation Timbers has previously spoken at an event—Toronto Workshop on Reproducibility—that I organized earlier this year."
  },
  {
    "objectID": "posts/2017-07-18-mapping-the-2016-australian-election-polling-place-results/mapping-the-2016-australian-election-polling-place-results.html",
    "href": "posts/2017-07-18-mapping-the-2016-australian-election-polling-place-results/mapping-the-2016-australian-election-polling-place-results.html",
    "title": "Mapping the 2016 Australian Election Polling Place Results",
    "section": "",
    "text": "The note that follows introduces Australia’s political system, and then details the process of downloading and merging first-preference votes by polling place, and then plotting it on an interactive map.\n\nAustralia’s political system\nIn 2016 Australia’s federal government was determined by the outcomes of elections in 150 divisions which each elected one member to the lower house. The Liberal/National Coalition won 76 seats which allowed it to form a majority government; while the Labor party won 69 seats to form the Opposition; the Greens and the Nick Xenophon Team each won one seat; and there were two Independent members (Andrew Wilkie and Cathy McGowan).\nVotes are cast at polling places in each division. In general voters can go to any polling place within their registered division, but some polling places that are close to a boundary will allow voting from there and some major polling places (such as the city hall of a state capital) will allow voting in any division.\nAlthough there are some exceptions divisions are generally constructed so that they each have roughly the same number of people. However this is not the case for polling places – some are much larger than others. Nonetheless it is interesting to see the geographic distribution of which party received the most first-preference votes in each polling place, especially in the context of which party won the division.\n\n\nPolling place data\nThe main packages for the data manipulation are the tidyverse and magrittr. leaflet allows the creation of interactive maps, ggmap creates static maps, and rgdal assists with dealing with geographic data. rmapshaper is used to reduce the size of the shapefile of division boundaries so that it is faster to load.\n\nlibrary(tidyverse)\nlibrary(magrittr)\nlibrary(leaflet)\nlibrary(ggmap)\nlibrary(rgdal)\nlibrary(rmapshaper)\n\nThe polling place results can be downloaded by state from the AEC website at http://results.aec.gov.au/20499/Website/HouseDownloadsMenu-20499-Csv.htm. There the AEC also makes available a dataset that contains geocodes for each of the polling places. The separate datasets for each state need to be merged, and then each polling place needs to be geocoded. Finally some minor changes are needed to make the party names easier to follow.\n\n#### Read in the polling place datasets (are state specific), and the geocodes for each polling place. Then put it all together to have one geocoded polling place dataset for all of Australia: Australia_booths. Finally, create a dataset that is filtered so that it just shows the winner of each booth: Australia_booths_winner. ####\n# Data importing\ngeocodes &lt;- read_csv(\"data/GeneralPollingPlacesDownload-20499.csv\", skip = 1)\nNSW_booths &lt;- read_csv(\"data/HouseStateFirstPrefsByPollingPlaceDownload-20499-NSW.csv\", skip = 1)\nQLD_booths &lt;- read_csv(\"data/HouseStateFirstPrefsByPollingPlaceDownload-20499-QLD.csv\", skip = 1)\nVIC_booths &lt;- read_csv(\"data/HouseStateFirstPrefsByPollingPlaceDownload-20499-VIC.csv\", skip = 1)\nACT_booths &lt;- read_csv(\"data/HouseStateFirstPrefsByPollingPlaceDownload-20499-ACT.csv\", skip = 1)\nTAS_booths &lt;- read_csv(\"data/HouseStateFirstPrefsByPollingPlaceDownload-20499-TAS.csv\", skip = 1)\nSA_booths &lt;- read_csv(\"data/HouseStateFirstPrefsByPollingPlaceDownload-20499-SA.csv\", skip = 1)\nWA_booths &lt;- read_csv(\"data/HouseStateFirstPrefsByPollingPlaceDownload-20499-WA.csv\", skip = 1)\nNT_booths &lt;- read_csv(\"data/HouseStateFirstPrefsByPollingPlaceDownload-20499-NT.csv\", skip = 1)\n# Merge\nAustralia_booths &lt;- rbind(NSW_booths, QLD_booths, VIC_booths, ACT_booths, TAS_booths, SA_booths, WA_booths, NT_booths)\n# Add the geocodes\nAustralia_booths &lt;- Australia_booths %&gt;% \n  left_join(geocodes)\n# Clean up\nrm(NSW_booths, QLD_booths, VIC_booths, ACT_booths, TAS_booths, SA_booths, WA_booths, NT_booths)\n# If you need it use this to get a list of the parties, ordered by the number of first-preference votes\n# first_votes &lt;- Australia_booths %&gt;%\n#   group_by(PartyNm) %&gt;%\n#   summarise(votes = sum(OrdinaryVotes, na.rm = TRUE)) %&gt;%\n#   arrange(desc(votes))\n# Combine some parties that are separate, but equivalent: Australian Labor Party & Australian Labor Party (Northern Territory) Branch & Labor, Country Liberals (NT) & Liberal, The Greens & The Greens (WA).\nAustralia_booths$PartyNm &lt;- recode(Australia_booths$PartyNm, \"Australian Labor Party (Northern Territory) Branch\" = \"Australian Labor Party\", \"Labor\" = \"Australian Labor Party\")\nAustralia_booths$PartyNm &lt;- recode(Australia_booths$PartyNm, \"Country Liberals (NT)\" = \"Liberal/LNP\", \"Liberal National Party of Queensland\" = \"Liberal/LNP\", \"Liberal\" = \"Liberal/LNP\")\nAustralia_booths$PartyNm &lt;- recode(Australia_booths$PartyNm, \"The Greens (WA)\" = \"The Greens\")\n# Create an indicator for who won the polling place then filter on that\nAustralia_booths_winner &lt;- Australia_booths %&gt;% \n  group_by(PollingPlaceID) %&gt;% \n  mutate(polling_place_winner = ifelse(max(OrdinaryVotes) == OrdinaryVotes, max(OrdinaryVotes), 0)) %&gt;%\n  filter(polling_place_winner &gt;= 1)\n#table(Australia_booths_winner$PartyNm)\n# There are three parties that only win one booth, so combine all those into 'Other'\nAustralia_booths_winner$PartyNm &lt;- recode(Australia_booths_winner$PartyNm, \"Australian Recreational Fishers Party\" = \"Other\", \"Christian Democratic Party (Fred Nile Group)\" = \"Other\", \"Derryn Hinch's Justice Party\" = \"Other\")\n\n\n\nDivision data\nThe divisions can be coloured based on which party won overall. The map of the boundaries for each division can be downloaded from the AEC website here: http://www.aec.gov.au/Electorates/gis/gis_datadownload.htm. The shapefile doesn’t have winner of each division so this needs to be merged into it. It is important to put the shapefile dataset first when merging. Finally, the shapefile is quite a large file and this can be reduced for faster loading.\n\n#### Read in the shapefiles (maps) that show each of the boundaries of the divisions (electorates) then add the data to say who won that division. Result is a spatial dataframe called boundaries. ####\n# Overall winner for each division, which will be used to color the division\nDivision_winner &lt;- read_csv(\"data/HouseMembersElectedDownload-20499.csv\", skip = 1)\n# The boundaries of the divisions (downloaded from: http://www.aec.gov.au/Electorates/gis/gis_datadownload.htm)\nboundaries &lt;- readOGR(dsn = \"data/national-midmif-09052016/COM_ELB.TAB\", layer = \"COM_ELB\")\n# Fix a couple - Mcmillan and Mcpherson - that have capitalisation issues\nboundaries$Elect_div &lt;- recode(boundaries$Elect_div, \"Mcmillan\" = \"McMillan\", \"Mcpherson\" = \"McPherson\")\n# Add the overall division winner dataset into the boundaries dataset (thanks to http://www.nickeubank.com/wp-content/uploads/2015/10/RGIS2_MergingSpatialData_part1_Joins.html)\nboundaries &lt;- merge(boundaries, Division_winner, by.x = \"Elect_div\", by.y = \"DivisionNm\")\n# Simplify and reduce the size of the shapefile so that it loads better\nobject.size(boundaries)\nboundaries &lt;- rmapshaper::ms_simplify(boundaries)\nobject.size(boundaries)\n# Clean up\nrm(Division_winner)\n\nThen colours need to be associated with each party.\n\n#### Specify the colour schemes that will be used. ####\n# Set the color scheme for the booth coloring\n# pal &lt;- colorFactor(\n#   palette = \"Dark2\", \n#   domain = unique(Australia_booths$PartyNm)\npal &lt;- colorFactor(palette = c(\"#c04745\", \"#616161\", \"black\", \"purple4\", \"#4776be\", \"#ff5800\", \"cyan1\", \"yellow\", \"#a8c832\", \"brown4\"), \n                          domain = c(\"Australian Labor Party\", \"Independent\", \"Informal\", \"Katter's Australian Party\", \"Liberal/LNP\", \"Nick Xenophon Team\", \"Other\", \"Pauline Hanson's One Nation\", \"The Greens\", \"The Nationals\"))\n# Set the color scheme for the division coloring\npall &lt;- colorFactor(palette = c(\"#c04745\", \"#616161\", \"purple4\", \"#4776be\", \"#4776be\", \"#ff5800\", \"#a8c832\", \"brown4\"), \n                   domain = c(\"Australian Labor Party\", \"Independent\", \"Katter's Australian Party\", \"Liberal\", \"Liberal National Party\", \"Nick Xenophon Team\", \"The Greens\", \"The Nationals\"))\n\n\n\nInteractive map\nFinally, the map can be produced:\n\n#### Pull it all together to make the map ####\n# Make the map\nAustralia_map &lt;- \n  leaflet() %&gt;%\n  # Base groups\n  addTiles() %&gt;%  # Add default OpenStreetMap map tiles\n  addProviderTiles(providers$Stamen.TonerLite, group = \"Toner Lite\") %&gt;% # Add a black and white alternative\n  setView(lng = 133.7751, lat = -25.2744, zoom = 4) %&gt;% # Specify where the map is initially focused\n  addPolygons(data = boundaries, \n              color = \"#444444\", \n              weight = 1, \n              smoothFactor = 0.5,\n              opacity = 1.0, \n              fillColor = pall(boundaries$PartyNm),\n              highlightOptions = highlightOptions(color = \"#666\", weight = 2, bringToFront = FALSE)) %&gt;% # Add the plot of the divisions, coloured by which party won it\n  addCircles(\n    data = Australia_booths_winner,\n    lng = Australia_booths_winner$Longitude, \n    lat = Australia_booths_winner$Latitude, \n    popup = paste(\"&lt;b&gt;Division:&lt;/b&gt;\", as.character(Australia_booths_winner$DivisionNm), \"&lt;br&gt;\",\n                  \"&lt;b&gt;Polling place:&lt;/b&gt;\", as.character(Australia_booths_winner$PollingPlaceNm), \"&lt;br&gt;\",\n                  \"&lt;b&gt;Address:&lt;/b&gt;\", as.character(Australia_booths_winner$PremisesAddress1), \"&lt;br&gt;\",\n                  \"&lt;b&gt;Party with most first-pref votes:&lt;/b&gt;\", as.character(Australia_booths_winner$PartyNm), \"&lt;br&gt;\",\n                  \"&lt;b&gt;First-pref votes:&lt;/b&gt;\", as.character(Australia_booths_winner$OrdinaryVotes), \"&lt;br&gt;\"),\n    label = ~as.character(Australia_booths_winner$DivisionNm),\n    #clusterOptions = markerClusterOptions(),\n    color = pal(Australia_booths_winner$PartyNm),\n    fillOpacity = 0.5) %&gt;% # Plot the booths, coloured by which party got the most first-preferences.\n  # Layers control\n  addLayersControl(\n    baseGroups = c(\"OSM (default)\", \"Toner Lite\"),\n    options = layersControlOptions(collapsed = FALSE)\n  ) %&gt;%\n  addLegend(\"bottomright\", pal = pal, values = Australia_booths_winner$PartyNm,\n            title = \"Which party won\",\n            #labFormat = labelFormat(prefix = \"$\"),\n            opacity = 1\n  )\n# Call the map\nAustralia_map"
  },
  {
    "objectID": "recs.html",
    "href": "recs.html",
    "title": "Recommendations",
    "section": "",
    "text": "Brisbane\n\nThere are free BBQs available on Kangaroo Point cliffs—have an early dinner and watch the night close in.\nThe Barra Boys.\nPizza Caffe at the University of Queensland.\nGo for a run, starting at about the Southbank Ferry Terminal, then cross the Goodwill Bridge, then either left for a shorter run (cross back on Kurilpa Bridge) or right and stick along the river and keep going until you get too tired—making it to Sydney Street Ferry Terminal is great, but even better to try to get to New Farm Park Ferry Terminal—then get a City Cat back to Southbank. That will end up about 10-12km depending on specifics.\nHire a car and drive an hour north to Moffat Beach (not patrolled) or Dicky Beach (patrolled). (Train + bus works also.)\n\nCanberra\n\nThe Cupping Room\nRizla\nVolstead Repeal\nA run from Civic down to the lake, and ideally making it across both bridges is pretty great. Will end up at about 8-10km depending on where you’re leaving from in Civic.\n\nMelbourne\n\nPatricia Coffee Brewers (standing room only)\nFlora Indian Restaurant (cheap but great)\nBrunetti\nYou can book a meeting room at the State library if you’re a member (free to join)\n\nSydney\n\nCampos Coffee Newtown - get an affogato\nCircular Quay to Manly Ferry - get the commuter one, not the “fast”\nNB: “Central” station isn’t actually that central—you’ll likely want Wynyard."
  },
  {
    "objectID": "recs.html#australia",
    "href": "recs.html#australia",
    "title": "Recommendations",
    "section": "",
    "text": "Brisbane\n\nThere are free BBQs available on Kangaroo Point cliffs—have an early dinner and watch the night close in.\nThe Barra Boys.\nPizza Caffe at the University of Queensland.\nGo for a run, starting at about the Southbank Ferry Terminal, then cross the Goodwill Bridge, then either left for a shorter run (cross back on Kurilpa Bridge) or right and stick along the river and keep going until you get too tired—making it to Sydney Street Ferry Terminal is great, but even better to try to get to New Farm Park Ferry Terminal—then get a City Cat back to Southbank. That will end up about 10-12km depending on specifics.\nHire a car and drive an hour north to Moffat Beach (not patrolled) or Dicky Beach (patrolled). (Train + bus works also.)\n\nCanberra\n\nThe Cupping Room\nRizla\nVolstead Repeal\nA run from Civic down to the lake, and ideally making it across both bridges is pretty great. Will end up at about 8-10km depending on where you’re leaving from in Civic.\n\nMelbourne\n\nPatricia Coffee Brewers (standing room only)\nFlora Indian Restaurant (cheap but great)\nBrunetti\nYou can book a meeting room at the State library if you’re a member (free to join)\n\nSydney\n\nCampos Coffee Newtown - get an affogato\nCircular Quay to Manly Ferry - get the commuter one, not the “fast”\nNB: “Central” station isn’t actually that central—you’ll likely want Wynyard."
  },
  {
    "objectID": "recs.html#canada",
    "href": "recs.html#canada",
    "title": "Recommendations",
    "section": "Canada",
    "text": "Canada\nToronto\n\nCalifornia Sandwiches - 244 Claremont St.\nGood Egg—a book shop in Kensington Market that is focused on kitchen books. It sounds niche, and it is, but it’s also great. (HT: Ryan Briggs)\nVietnam Lovely Noodle - 378 Bloor St W.\nCà Phê Rang - 147 Spadina Ave.\nEvangeline Toronto - 51 Camden St\n\nThis is a bar on Level 14 of the Ace Hotel. Go early on a nice evening and try for the outside table around the corner for a great view.\n\nGift Shop Cocktail Bar - 89-B Ossington Ave.\n\nLook for the barber shop - knock, then wait and hope you’re let in. (This may sound weird, but it’s great.) Go early (they open at 7pm) because it’s a small space and fill up quickly. The menu is epic.\n\nTap Phong - 360 Spadina Ave - in Chinatown provides an incredible selection of kitchen supplies at decent prices. Again, sounds weird, but it’s a Toronto institution."
  },
  {
    "objectID": "teaching.html",
    "href": "teaching.html",
    "title": "Teaching",
    "section": "",
    "text": "I enjoy teaching and aim to help students from a wide range of backgrounds learn how to use data to tell convincing stories. My approach is best characterised as practical, research-based, and emphasizing reproducible workflow. I am a RStudio Certified Tidyverse Trainer and an associate editor of the Journal of Statistics and Data Science Education. I co-lead the development of a package to help students learn R, available here: DoSS Toolkit. My book, Telling Stories with Data supports my teaching.\nI have designed and/or taught:\n\nWorlds Become Data\nSurveys, Sampling and Observational Data\nExperimental Design\nEthics and Data Science\nNatural Language Processing\nData Sciences Foundations\nThe Other Course\nHistory of Statistics and Data Sciences"
  },
  {
    "objectID": "posts/2021-03-14-greg-wilson/in-appreciation-of-greg-wilson.html",
    "href": "posts/2021-03-14-greg-wilson/in-appreciation-of-greg-wilson.html",
    "title": "In Appreciation of Greg Wilson",
    "section": "",
    "text": "As a baby professor there is a lot that causes me to wake up crying during the night. But the main reason is teaching. Having largely spent five years alone in an office working on projects by-myself, the idea that I should stand in front of (or Zoom to) one- to two-hundred students and competently teach is still a tad surprising. Greg Wilson is the only reason that I no longer wake up wracked with concern about at least this aspect of baby professor life.\nGreg established and, until recently, ran instructor training at R Studio. This involved putting together and teaching a two-day course about the Tidyverse (there’s also a Shiny version that I didn’t do). After the course, Greg assessed you on a one-to-one basis and when he was happy he certified you.\nThe training assumed that you already knew R (essentially to the level of R4DS). It’s not about R, it’s about teaching you how to teach R. We covered an awful lot, but some highlights include:\n\nThe creation of learner personas to help understand where the student is coming from.\nHow to formally structure assessment, and other feedback, in a way that builds a student’s knowledge and confidence.\nPutting together different aspects in a ‘concept map’ that a student can use to navigate these aspects.\nRemaining mindful of cognitive load and active teaching.\nSeeing things from the student perspective.\nAnd how to do all this in the real-world i.e. a time-constrained mess of priorities and incomplete information.\n\nMy guess is that none of this is pedagogical rocket science, but it’s not something that I knew, or even thought about, before Greg’s course. More important than any specific aspect, Greg provided a foundation for my teaching on which I can iterate and evolve. There’s the famous Cheshire Cat quote in Alice in Wonderland about needing to have a destination before it matters in which direction you go. Greg provided that destination. I don’t know whether it improved outcomes for my students; but it certainly reduced how worried I was about teaching.\nThat all said, I think that the most important thing that I learnt from Greg is ‘Be kind: all else is details.’. Greg taught us this through his example.\nBy my rough count there are something like 200 R Studio certified Tidyverse instructors. A lot of them are similarly professors who will teach large classes of undergrads, smaller classes of grads, and directly supervise PhD students, who themselves will teach… If Greg caused even a one per cent improvement in the quality of the instruction of those folks, then the potential impact that he’s had is enormous.\nI don’t know anything of the circumstances surrounding Greg’s departure from R Studio, and it’s none of my business either way. Greg holds a CS PhD and spent a few years as an assistant professor at the Toronto CS department a few years back. He founded Software Carpentry. In an ideal world we would find space for him at the university. But regardless of what happens, I’ll always be grateful for his teaching, his example, and most importantly, his kindness, and hope to emulate that in my own teaching.\nIf you’d like to learn more about what Greg taught us you should look at his book Teaching Tech Together: http://teachtogether.tech/en/index.html. It’s all good, but even just skimming ‘The Rules’ is worthwhile.\nA month or so ago, Greg stopped by the Faculty of Information at the University of Toronto and taught a class about ‘How to Run a Meeting’. If you’d like to see the recording of that you can do so here: https://youtu.be/qYh6Nzv3RWs.\nA bunch of other folks have written more about the training and you can read some of them here:\n\nhttps://silvia.rbind.io/2020-10-07-rstudio-instructor-certification-tidyverse/\nhttps://shelkariuki.netlify.app/post/certification/\nhttps://www.yuqiliao.com/blog/rstudiocertification/"
  },
  {
    "objectID": "posts/2021-12-02-average_age_of_australian_politicians/average_age_of_parliaments.html",
    "href": "posts/2021-12-02-average_age_of_australian_politicians/average_age_of_parliaments.html",
    "title": "The average age of politicians in the Australian Federal Parliament (1901-2021)",
    "section": "",
    "text": "Last week I had the opportunity to attend my first thesis defence at a French-language institution (just to clarify - the actual defence was nonetheless conducted in English). Congratulations to Florence Vallée-Dubois on a successful defence. Her thesis looks at population ageing and democratic representation in Canada and is absolutely brilliant.\nHer work, and some earlier comments by Ben Readshaw, got me thinking about what the average age of a politician in Australia looks like. Thankfully, this is an example of a question that my new R package with Paul Hodgetts AustralianPoliticians makes easy to answer (Alexander and Hodgetts 2021).\nThe average age of politicians in a parliament may have implications for the types of issues that are emphasised and the policies that are put in place. I examine the average age of Australian politicians in the Senate and the House of Representatives. In general, the Senate tends to be slightly older than the House of Representatives. I find a gradual increase in the average age from Federation through to the 1940s. In 1949 there was an expansion of the parliament and the average age noticeably declined. After a period of relative stability in the 1950s and 1960s, there was another noticeable decrease in the 1970s, followed by a gradual aging."
  },
  {
    "objectID": "posts/2021-12-02-average_age_of_australian_politicians/average_age_of_parliaments.html#overview",
    "href": "posts/2021-12-02-average_age_of_australian_politicians/average_age_of_parliaments.html#overview",
    "title": "The average age of politicians in the Australian Federal Parliament (1901-2021)",
    "section": "",
    "text": "Last week I had the opportunity to attend my first thesis defence at a French-language institution (just to clarify - the actual defence was nonetheless conducted in English). Congratulations to Florence Vallée-Dubois on a successful defence. Her thesis looks at population ageing and democratic representation in Canada and is absolutely brilliant.\nHer work, and some earlier comments by Ben Readshaw, got me thinking about what the average age of a politician in Australia looks like. Thankfully, this is an example of a question that my new R package with Paul Hodgetts AustralianPoliticians makes easy to answer (Alexander and Hodgetts 2021).\nThe average age of politicians in a parliament may have implications for the types of issues that are emphasised and the policies that are put in place. I examine the average age of Australian politicians in the Senate and the House of Representatives. In general, the Senate tends to be slightly older than the House of Representatives. I find a gradual increase in the average age from Federation through to the 1940s. In 1949 there was an expansion of the parliament and the average age noticeably declined. After a period of relative stability in the 1950s and 1960s, there was another noticeable decrease in the 1970s, followed by a gradual aging."
  },
  {
    "objectID": "posts/2021-12-02-average_age_of_australian_politicians/average_age_of_parliaments.html#data-preparation",
    "href": "posts/2021-12-02-average_age_of_australian_politicians/average_age_of_parliaments.html#data-preparation",
    "title": "The average age of politicians in the Australian Federal Parliament (1901-2021)",
    "section": "Data preparation",
    "text": "Data preparation\nThe dataset that I used in this note was primarily collected from records from the Australian Parliamentary Library, Wikipedia, and the Australian Dictionary of Biography. The dataset comprises biographical, political, and other information about every person elected to either the House of Representatives or the Senate. You can get the dataset in a series of CSVs on GitHub or as an R Package AustralianPoliticians available on CRAN.1\n\n# install.packages(\"AustralianPoliticians\")\nlibrary(AustralianPoliticians)\nlibrary(lubridate)\nlibrary(tidyverse)\n\nThe other packages that are required for the analysis are lubridate (Grolemund and Wickham 2011), the tidyverse (Wickham et al. 2019). Additionally this note draws on the knitr (Xie 2020) and distill (Allaire et al. 2021) packages. All analysis is conducted in the statistical programming language R (R Core Team 2020).\nI’m not going to spend a lot of time on descriptives about the dataset here because that’s needed for a paper, and this is meant to be fun.\n\nall &lt;- AustralianPoliticians::get_auspol(\"all\")\nhead(all)\n\n# A tibble: 6 × 20\n  uniqueID   surname allOtherNames      firstName commonName\n  &lt;chr&gt;      &lt;chr&gt;   &lt;chr&gt;              &lt;chr&gt;     &lt;chr&gt;     \n1 Abbott1859 Abbott  Richard Hartley S… Richard   &lt;NA&gt;      \n2 Abbott1869 Abbott  Percy Phipps       Percy     &lt;NA&gt;      \n3 Abbott1877 Abbott  Macartney          Macartney Mac       \n4 Abbott1886 Abbott  Charles Lydiard A… Charles   Aubrey    \n5 Abbott1891 Abbott  Joseph Palmer      Joseph    &lt;NA&gt;      \n6 Abbott1957 Abbott  Anthony John       Anthony   Tony      \n# ℹ 15 more variables: displayName &lt;chr&gt;,\n#   earlierOrLaterNames &lt;chr&gt;, title &lt;chr&gt;, gender &lt;chr&gt;,\n#   birthDate &lt;date&gt;, birthYear &lt;dbl&gt;, birthPlace &lt;chr&gt;,\n#   deathDate &lt;date&gt;, member &lt;dbl&gt;, senator &lt;dbl&gt;,\n#   wasPrimeMinister &lt;dbl&gt;, wikidataID &lt;chr&gt;,\n#   wikipedia &lt;chr&gt;, adb &lt;chr&gt;, comments &lt;chr&gt;\n\nall$member %&gt;% table()\n\n.\n   0    1 \n 578 1205 \n\nall$senator %&gt;% table()\n\n.\n   0    1 \n1155  628 \n\n\nBut briefly, there are 1,783 politicians in the dataset, with 1,205 having sat in the House of Representatives, which is the lower house, and 628 having sat in the Senate, which is the upper house. There is some overlap because there are people who sat in both houses.2\nThe AustralianPoliticians package contains a number of datasets that are related by the uniqueID variable. This note requires the main dataset - ‘all’ - as well as two supporting datasets that provide more detailed information about the members - ‘mps’ - and the senators - ‘senators’.\n\n# Get the members and the dates they were in the house\nmps &lt;- AustralianPoliticians::get_auspol(\"mps\")\n\naustralian_mps &lt;- \n  all %&gt;% \n  filter(member == 1) %&gt;% \n  left_join(mps, \n            by = \"uniqueID\") %&gt;% \n  select(uniqueID, mpFrom, mpTo) %&gt;% \n  mutate(house = \"reps\") %&gt;% \n  rename(from = mpFrom,\n         to = mpTo)\n\n\n# Get the senators and the dates they were in the senate\nsenators &lt;- AustralianPoliticians::get_auspol(\"senators\")\n\naustralian_senators &lt;- \n  all %&gt;% \n  filter(senator == 1) %&gt;% \n  left_join(senators, \n            by = \"uniqueID\") %&gt;% \n  select(uniqueID, senatorFrom, senatorTo) %&gt;% \n  mutate(house = \"senate\") %&gt;% \n  rename(from = senatorFrom,\n         to = senatorTo)\n\naustralian_politicians &lt;- rbind(australian_mps, australian_senators)\nrm(australian_senators, australian_mps, mps, senators)\n\n# Change the names so that they print nicely in graphs/tables\naustralian_politicians &lt;- \n  australian_politicians %&gt;% \n  mutate(house =\n           case_when(\n             house == \"senate\" ~ \"Senate\",\n             house == \"reps\" ~ \"HoR\",\n             TRUE ~ \"OH NO\")\n         )\nhead(australian_politicians)\n\n# A tibble: 6 × 4\n  uniqueID   from       to         house\n  &lt;chr&gt;      &lt;date&gt;     &lt;date&gt;     &lt;chr&gt;\n1 Abbott1869 1913-05-31 1919-11-03 HoR  \n2 Abbott1886 1925-11-14 1929-10-12 HoR  \n3 Abbott1886 1931-12-19 1937-03-28 HoR  \n4 Abbott1891 1940-09-21 1949-10-31 HoR  \n5 Abbott1957 1994-03-26 2019-05-18 HoR  \n6 Abel1939   1975-12-13 1977-11-10 HoR  \n\n# australian_politicians$house %&gt;% table()\n\nFor each day, I would like to know the average age of everyone sitting in the parliament. There are a variety of ways to do this, but one is to create a dataset of two columns: date and uniqueID. Both of these are repeated, so that for every date there is every uniqueID.\n\nstart_date &lt;- ymd(\"1901-01-01\")\nend_date &lt;- ymd(\"2021-12-31\")\n\npoliticians_by_date &lt;- \n  tibble(\n    uniqueID = rep(australian_politicians$uniqueID %&gt;% unique(),\n                   end_date - start_date + 1),\n    date = rep(seq.Date(start_date, end_date, by = \"day\"),\n               australian_politicians$uniqueID %&gt;% unique() %&gt;% length()\n               )\n    )\n\nhead(politicians_by_date)\n\n# A tibble: 6 × 2\n  uniqueID   date      \n  &lt;chr&gt;      &lt;date&gt;    \n1 Abbott1869 1901-01-01\n2 Abbott1886 1901-01-02\n3 Abbott1891 1901-01-03\n4 Abbott1957 1901-01-04\n5 Abel1939   1901-01-05\n6 Adams1951  1901-01-06\n\n\nAlthough the dataset is long at this point, it will be quite sparse as there are many combinations of date and uniqueID that are irrelevant. In the next step I check if each uniqueID was in parliament on each date, and filter away those that were not.\n\n# Add an explicit end date to the uniqueIDs that are still in parliament and\n# hence have NA in the date they left parliament.\naustralian_politicians$to[is.na(australian_politicians$to)] &lt;- end_date\n\npoliticians_by_date &lt;- \n  politicians_by_date %&gt;% \n  left_join(australian_politicians,\n            by = \"uniqueID\"\n            )\n\nWarning in left_join(., australian_politicians, by = \"uniqueID\"): Detected an unexpected many-to-many relationship between\n`x` and `y`.\nℹ Row 1 of `x` matches multiple rows in `y`.\nℹ Row 1 of `y` matches multiple rows in `x`.\nℹ If a many-to-many relationship is expected, set\n  `relationship = \"many-to-many\"` to silence this warning.\n\npoliticians_by_date &lt;- \n  politicians_by_date %&gt;% \n  mutate(in_parliament_interval = interval(from, to),\n         in_parliament_at_date = if_else(date %within% in_parliament_interval, \n                                         1, \n                                         0)\n         ) %&gt;% \n  filter(in_parliament_at_date == 1) %&gt;% \n  select(-in_parliament_interval,\n         -in_parliament_at_date,\n         -from,\n         -to)\n\nhead(politicians_by_date)\n\n# A tibble: 6 × 3\n  uniqueID     date       house\n  &lt;chr&gt;        &lt;date&gt;     &lt;chr&gt;\n1 Bonython1848 1901-04-13 HoR  \n2 Braddon1829  1901-04-25 HoR  \n3 Brown1861    1901-05-11 HoR  \n4 Cameron1851  1901-06-13 HoR  \n5 Chanter1845  1901-07-13 HoR  \n6 Chapman1864  1901-07-14 HoR  \n\n\nNow that the dataset is a bit more tractable, I add the birthday of every uniqueID and then calculate their age, in days, for every date.\n\npoliticians_by_house_and_birthday &lt;- \n  all %&gt;% \n  select(uniqueID, birthDate, member, senator) %&gt;% \n  pivot_longer(cols = c(member, senator), \n               names_to = \"house\", \n               values_to = \"in_it\"\n               ) %&gt;% \n  filter(in_it == 1) %&gt;% \n  select(-in_it) %&gt;% \n  mutate(house = \n           case_when(\n             house == \"senator\" ~ \"Senate\",\n             house == \"member\" ~ \"HoR\",\n             TRUE ~ \"OH NO\")\n         )\n\n# Check if the catch-all has been invoked\n# politicians_by_house_and_birthday[politicians_by_house_and_birthday$house == \"OH NO\",]\n\npoliticians_by_date &lt;- \n  politicians_by_date %&gt;% \n  left_join(politicians_by_house_and_birthday,\n            by = c(\"uniqueID\", \"house\")\n            )\n\npoliticians_by_date &lt;- \n  politicians_by_date %&gt;% \n  filter(!is.na(birthDate)) %&gt;% # I can't find Trish Wortley's birthday and also\n  # we don't know the birthdates of some of the politicians from around Federation.\n  mutate(age_as_at_that_date = date - birthDate)\n\nhead(politicians_by_date)\n\n# A tibble: 6 × 5\n  uniqueID   date       house birthDate  age_as_at_that_date\n  &lt;chr&gt;      &lt;date&gt;     &lt;chr&gt; &lt;date&gt;     &lt;drtn&gt;             \n1 Bonython1… 1901-04-13 HoR   1848-10-15 19172 days         \n2 Braddon18… 1901-04-25 HoR   1829-06-11 26250 days         \n3 Brown1861  1901-05-11 HoR   1861-10-06 14461 days         \n4 Cameron18… 1901-06-13 HoR   1851-11-03 18119 days         \n5 Chanter18… 1901-07-13 HoR   1845-02-11 20605 days         \n6 Chapman18… 1901-07-14 HoR   1864-07-10 13517 days         \n\n\nI need to work out the average age for each day. There are some politicians for whom we do not know their exact birthdate. Those have been removed in this calculation.\n\naverage_age_by_date &lt;- \n  politicians_by_date %&gt;%\n  mutate(age_as_at_that_date = age_as_at_that_date / ddays(1)) %&gt;% # This just converts it into a days count\n  group_by(date, house) %&gt;% \n  summarise(average_age = median(age_as_at_that_date, na.rm = TRUE)) \n\n`summarise()` has grouped output by 'date'. You can\noverride using the `.groups` argument.\n\naverage_age_by_date &lt;- \n  average_age_by_date %&gt;% \n  mutate(average_age_in_years = average_age/365) \n\nhead(average_age_by_date)\n\n# A tibble: 6 × 4\n# Groups:   date [3]\n  date       house  average_age average_age_in_years\n  &lt;date&gt;     &lt;chr&gt;        &lt;dbl&gt;                &lt;dbl&gt;\n1 1901-03-29 HoR         16765                  45.9\n2 1901-03-29 Senate      16086.                 44.1\n3 1901-03-30 HoR         17235                  47.2\n4 1901-03-30 Senate      18878.                 51.7\n5 1901-03-31 HoR         17236                  47.2\n6 1901-03-31 Senate      18878.                 51.7\n\n\nI’ll do the same thing except to work out the average by election period. This requires grabbing the election periods (there’s an R package coming soon about this)!\n\nelections &lt;- read_csv(\"https://raw.githubusercontent.com/RohanAlexander/australian_federal_elections/master/outputs/elections.csv\",\n                      col_types = \"Dicc\")\n\naverage_age_by_election &lt;- \n  politicians_by_date %&gt;%\n  left_join(elections %&gt;% \n              rename(date = electionDate) %&gt;% \n              select(-comment) %&gt;% \n              mutate(house = \"HoR\"),\n            by = c(\"date\", \"house\")) %&gt;% \n  ungroup() %&gt;% \n  filter(house == \"HoR\") %&gt;% \n  arrange(date, uniqueID) %&gt;% \n  mutate(is_election = if_else(is.na(electionWinner), 0, 1),\n         is_election = if_else(is_election == lag(is_election, default = 0), 0, is_election),\n         election_counter = cumsum(is_election))\n\naverage_age_by_election &lt;- \n  average_age_by_election %&gt;%\n  ungroup() %&gt;% \n  mutate(age_as_at_that_date = age_as_at_that_date / ddays(1)) %&gt;% # This just \n  # converts it into a days count\n  group_by(election_counter) %&gt;% \n  summarise(average_age = median(age_as_at_that_date, na.rm = TRUE),\n            first_date = min(date)) %&gt;% \n  mutate(average_age_in_years = average_age/365) %&gt;% \n  ungroup()"
  },
  {
    "objectID": "posts/2021-12-02-average_age_of_australian_politicians/average_age_of_parliaments.html#results",
    "href": "posts/2021-12-02-average_age_of_australian_politicians/average_age_of_parliaments.html#results",
    "title": "The average age of politicians in the Australian Federal Parliament (1901-2021)",
    "section": "Results",
    "text": "Results\n\nOver time\nThere are considerable changes over time (Figure @ref(fig:maingraph)).\n\naverage_age_by_date %&gt;% \n  # filter(year(date) == 1904) %&gt;%\n  # filter(month(date) == 2) %&gt;%\n  # filter(house == \"HoR\") %&gt;%\n  ggplot(aes(x = date, y = average_age_in_years, colour = house)) +\n  geom_point(alpha = 0.5) +\n  labs(x = \"Date\",\n       y = \"Average age (years)\",\n       color = \"House\") +\n  theme_minimal() +\n  scale_color_brewer(palette = \"Set1\") \n\n\n\n\nAverage age in the Australian Federal Parliament on a daily basis\n\n\n\n\nI’ll just separate each of the houses because there’s a lot going on there (Figures @ref(fig:justrepsthings) and @ref(fig:justsenatethigns)).\n\naverage_age_by_date %&gt;% \n  filter(house == \"HoR\") %&gt;% \n  ggplot(aes(x = date, y = average_age_in_years)) +\n  geom_point(alpha = 0.5, color = \"#F90000\") +\n  labs(x = \"Date\",\n       y = \"Average age (years)\") +\n  theme_minimal() +\n  scale_color_brewer(palette = \"Set1\") \n\n\n\n\nAverage age in the lower house on a daily basis\n\n\n\n\n\naverage_age_by_date %&gt;% \n  filter(house == \"Senate\") %&gt;% \n  ggplot(aes(x = date, y = average_age_in_years)) +\n  geom_point(alpha = 0.5, color = \"#0080BD\") +\n  labs(x = \"Date\",\n       y = \"Average age (years)\") +\n  theme_minimal() +\n  scale_color_brewer(palette = \"Set1\") \n\n\n\n\nAverage age in the upper house on a daily basis\n\n\n\n\n\n\nGroup by election period\nWe can also look at a summary of the results, averaged across days, on the basis of each election period (Table @ref(tab:maintable) and Figure @ref(fig:othergraph)).\n\naverage_age_by_election %&gt;% \n  select(-average_age) %&gt;% \n  rename(`Election number` = election_counter,\n         `Period begins` = first_date,\n         `Average age (years)` = average_age_in_years) %&gt;% \n  kable(caption = \"Average age by for each period between lower house elections\",\n        digits = 1,\n        booktabs = TRUE)\n\n\nAverage age by for each period between lower house elections\n\n\nElection number\nPeriod begins\nAverage age (years)\n\n\n\n\n1\n1901-03-29\n48.2\n\n\n2\n1903-12-16\n48.8\n\n\n3\n1906-12-12\n49.0\n\n\n4\n1910-04-13\n50.1\n\n\n5\n1913-05-31\n49.8\n\n\n6\n1914-09-05\n51.9\n\n\n7\n1917-05-05\n53.0\n\n\n8\n1919-12-13\n52.2\n\n\n9\n1922-12-16\n51.7\n\n\n10\n1925-11-14\n52.3\n\n\n11\n1928-11-17\n50.9\n\n\n12\n1929-10-12\n49.4\n\n\n13\n1931-12-19\n52.3\n\n\n14\n1934-09-15\n51.1\n\n\n15\n1937-10-23\n51.9\n\n\n16\n1940-09-21\n52.2\n\n\n17\n1943-08-21\n51.6\n\n\n18\n1946-09-28\n53.3\n\n\n19\n1949-12-10\n50.9\n\n\n20\n1951-04-28\n52.4\n\n\n21\n1954-05-29\n53.7\n\n\n22\n1955-12-10\n52.4\n\n\n23\n1958-11-22\n52.5\n\n\n24\n1961-12-09\n53.5\n\n\n25\n1963-11-30\n53.5\n\n\n26\n1966-11-26\n52.5\n\n\n27\n1969-10-25\n51.3\n\n\n28\n1972-12-02\n49.5\n\n\n29\n1974-05-18\n48.6\n\n\n30\n1975-12-13\n47.1\n\n\n31\n1977-12-10\n47.3\n\n\n32\n1980-10-18\n48.3\n\n\n33\n1983-03-05\n47.7\n\n\n34\n1984-12-01\n48.2\n\n\n35\n1987-07-11\n49.4\n\n\n36\n1990-03-24\n48.5\n\n\n37\n1993-03-13\n48.9\n\n\n38\n1996-03-02\n48.9\n\n\n39\n1998-10-03\n49.7\n\n\n40\n2001-11-10\n50.7\n\n\n41\n2004-10-09\n51.7\n\n\n42\n2007-11-24\n51.2\n\n\n43\n2010-08-21\n51.5\n\n\n44\n2013-09-07\n50.0\n\n\n45\n2016-07-02\n50.2\n\n\n46\n2019-05-18\n51.7\n\n\n\n\n\n\naverage_age_by_election %&gt;% \n  ggplot(aes(x = first_date, y = average_age_in_years)) +\n  geom_point() +\n  labs(x = \"Date\",\n       y = \"Average age (years)\") +\n  theme_minimal() +\n  scale_color_brewer(palette = \"Set1\") \n\n\n\n\nAverage age in the Australian Federal Parliament, by election period\n\n\n\n\n\n\nCompare with average age\nTo get a sense of whether the parliament is unusual we need know what is happening the broader population over this time. Fortunately, I know someone who is fairly handy when it comes to demography who can help.3\nAlthough it’s going to average over elections, which we’ve seen is a big source of variation, we can create an average age for each year, and then compare that against data from the Australian Bureau of Statistics (ABS). The ABS provides an estimate of historical population statistics in 3105.0.65.001 - Australian Historical Population Statistics, 2016, which was released in 2019. I want the second data cube - 2 - Population Age and Sex Structure - and within that I want Table 2.18 - Median age by sex, states and territories, 30 June, 1861 onwards.\n\nlibrary(readxl)\nlibrary(janitor)\n\n\nAttaching package: 'janitor'\n\n\nThe following objects are masked from 'package:stats':\n\n    chisq.test, fisher.test\n\nABS_data &lt;- read_excel(\"3105065001ds0002_2019.xls\", \n                       sheet = \"Table 2.18\",\n                       skip = 4) %&gt;%\n  janitor::clean_names() %&gt;% \n  rename(area_and_type = x1,) %&gt;% \n  select(-x1861, -x1870, -x1871, -x1881, -x1891)\n\nNew names:\n• `` -&gt; `...1`\n\nABS_data &lt;- \n  ABS_data %&gt;% \n  mutate(type = ifelse(area_and_type %in% c(\"Males\", \"Females\", \"Persons\"), area_and_type, NA),\n         area_and_type = ifelse(is.na(type), area_and_type, NA)) %&gt;% \n  select(area_and_type, type, everything()) %&gt;% \n  fill(area_and_type, .direction = \"down\") %&gt;% \n  mutate(area_and_type = str_remove(area_and_type, \"\\\\(f\\\\)\\\\(g\\\\)\"))\n\nABS_data &lt;- \n  ABS_data %&gt;%  \n  filter(area_and_type == \"Australia\",\n         type == \"Persons\")\n\nABS_data &lt;- \n  ABS_data %&gt;%\n  pivot_longer(cols = starts_with(\"x\"),\n               names_to = \"year\",\n               values_to = \"median\") %&gt;% \n  mutate(year = str_remove(year, \"x\"),\n         year = as.integer(year)) %&gt;%\n  rename(area = area_and_type)\n\n\naverage_age_by_year &lt;- \n  politicians_by_date %&gt;%\n  ungroup() %&gt;% \n  mutate(year = year(date)) %&gt;% \n  mutate(age_as_at_that_date = age_as_at_that_date / ddays(1)) %&gt;% # This just \n  # converts it into a days count\n  group_by(year) %&gt;% \n  summarise(average_age = median(age_as_at_that_date, na.rm = TRUE)) %&gt;% \n  mutate(average_age_in_years = average_age/365) %&gt;% \n  ungroup() \n\n\naverage_age_by_year %&gt;% \n  left_join(ABS_data, by = \"year\") %&gt;% \n  select(year, average_age_in_years, median) %&gt;% \n  rename(Politicians = average_age_in_years,\n         Overall = median) %&gt;% \n  pivot_longer(cols = c(Politicians, Overall), \n               names_to = \"population\",\n               values_to = \"median\") %&gt;% \n  ggplot(aes(x = year, y = median, color = population)) +\n  geom_point() +\n  # geom_smooth() +\n  theme_classic() +\n  scale_color_brewer(palette = \"Set1\") +\n  labs(x = \"Year\",\n         y = \"Median age (years)\",\n         color = \"Population\")\n\nWarning: Removed 23 rows containing missing values\n(`geom_point()`)."
  },
  {
    "objectID": "posts/2021-12-02-average_age_of_australian_politicians/average_age_of_parliaments.html#concluding-remarks",
    "href": "posts/2021-12-02-average_age_of_australian_politicians/average_age_of_parliaments.html#concluding-remarks",
    "title": "The average age of politicians in the Australian Federal Parliament (1901-2021)",
    "section": "Concluding remarks",
    "text": "Concluding remarks\nI’ve had a brief look at the average age of politicians in the Australian Parliament. In general, I’ve found that they’re older than the general population, and that there’s a lot of variation.\nMore generally, I’ve shown one of the advantages of putting data onto GitHub, and wrapping that in an R Package where possible. If you don’t like my analysis then you can grab the data yourself and play around with it!\nAs I mentioned at the top, Florence Vallée-Dubois does a lot more of this type of work in her thesis ‘Growing old : Population ageing and democratic representation in Canada’ where she looks at ‘whether the social changes brought about by population ageing also have implications for electoral politics and democratic representation.’ There’s probably a lot of similar work that could be done for Australia."
  },
  {
    "objectID": "posts/2021-12-02-average_age_of_australian_politicians/average_age_of_parliaments.html#acknowledgments",
    "href": "posts/2021-12-02-average_age_of_australian_politicians/average_age_of_parliaments.html#acknowledgments",
    "title": "The average age of politicians in the Australian Federal Parliament (1901-2021)",
    "section": "Acknowledgments",
    "text": "Acknowledgments\n\nThanks very much to Ben Readshaw and Florence Vallée-Dubois for motivating this note, and to Monica Alexander for her thoughtful comments."
  },
  {
    "objectID": "posts/2021-12-02-average_age_of_australian_politicians/average_age_of_parliaments.html#footnotes",
    "href": "posts/2021-12-02-average_age_of_australian_politicians/average_age_of_parliaments.html#footnotes",
    "title": "The average age of politicians in the Australian Federal Parliament (1901-2021)",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nMonica Alexander uses an early version of this dataset to look at the average age of death of Australian politicians: https://www.monicaalexander.com/posts/2019-08-09-australian_politicians/.↩︎\nThere are a handful of politicians who sat in the parliament, but were never elected - when the first parliament sat, there were initially some who were just appointed, and never went on to seek election. Conversely, there is one politician - Heather Hill - who was elected, but whose election was reversed before she could take her seat. While she never served in either house, she is included in the dataset for purposes of being able to link it to the elections dataset. However she is not included in the number of Australian politicians.↩︎\nIf you’re interested in this sort of thing, then a PhD with Monica Alexander is probably the way to go.↩︎"
  },
  {
    "objectID": "posts/2018-07-28-the-sql-is-never-as-good-as-the-original/the-sql-is-never-as-good-as-the-original.html",
    "href": "posts/2018-07-28-the-sql-is-never-as-good-as-the-original/the-sql-is-never-as-good-as-the-original.html",
    "title": "The SQL Is Never As Good As The Original",
    "section": "",
    "text": "Thanks to Monica for the title.\n\nIntroduction\nSQL is a popular way of working with data. Advanced users probably do a lot with it alone, but even just having a working knowledge of SQL has increased the number of datasets that I can get data from to then analyse with other tools such as R or Python. You can use SQL within RStudio if you want. The following are a few notes to help future-Rohan when he needs to use SQL. A worked example with a sample of the Hansard data will be included in a future post.\n\nSQL is fairly straightforward if you’ve used mutate, filter and join in the R tidyverse as the concepts (and sometimes even the verb) are the same. In that case, half the battle is getting used to the terminology, and the other half is getting on top of the order of operations because SQL can be a tad pedantic.\nSQL (“see-quell” or “S.Q.L.” - both camps seem fairly insistent on their way…) is used with relational databases. A relational database is just a collection of at least one table, and a table is just some data organized into rows and columns. If there’s more than one table in the database, then there should be some column that links them. Using it feels a bit like HTML/CSS in terms of being halfway between markup and programming. One fun aspect is that line spaces mean nothing: include them or don’t, but always end a SQL command in a semicolon;\n\n\nCreating a table\nCreate an empty table of three columns of type: int, text, int:\n\nCREATE TABLE table_name (\n  column1 INTEGER,\n  column2 TEXT,\n  column3 INTEGER\n);\n\nAdd a row of data:\n\nINSERT INTO table_name (column1, column2, column3)\n  VALUES (1234, 'Gough Menzies', 32);\n\nAdd a column:\n\nALTER TABLE table_name\n  ADD COLUMN column4 TEXT;\n\n\n\nViewing the data\nSee one column (similar to R’s select):\n\nSELECT column2\n  FROM table_name;\n\nSee two columns:\n\nSELECT column1, column2\n  FROM table_name;\n\nSee all columns:\n\nSELECT *\n  FROM table_name;\n\nSee unique rows in a column (similar to R’s distinct):\n\nSELECT DISTINCT column2\n  FROM table_name;\n\nSee the rows that match a criteria (similar idea to R’s which or filter):\n\nSELECT *\n  FROM table_name\n    WHERE column3 &gt; 30;\n\nAll the usual operators are fine with WHERE: =, !=, &gt;, &lt;, &gt;=, &lt;=. Just make sure the condition evaluates to true/false.\nSee the rows that are pretty close to a criteria:\n\nSELECT *\n  FROM table_name\n    WHERE column2 LIKE  '_ough Menzies';\n\nThe _ above is a wildcard that matches to any character e.g. ‘Cough Menzies’ would be matched here, as would ‘Gough Menzies’. LIKE is not case-sensitive: ‘Gough Menzies’ and ‘gough menzies’ would both match here.\nUse % as an anchor to matches pieces:\n\nSELECT *\n  FROM table_name\n    WHERE column2 LIKE  '%Menzies';\n\nThat matches anything ending with ‘Menzies’, so ‘Cough Menzies’, ‘Gough Menzies’, ‘Sir Menzies’ etc, would all be matched here. Use surrounding percentages to match within, e.g. %Menzies% would also match ‘Sir Menzies Jr’ whereas %Menzies would not.\nThis is wild: NULL values (!) (True/False/NULL) are possible, not just True/False, but they need to be explicitly matched for:\n\nSELECT *\n  FROM table_name\n    WHERE column2 IS NOT NULL;\n\nThis too is wild: There’s an underlying ordering build into number, date and text fields that allows you to use BETWEEN on all those, not just numeric! The following looks for text that starts with a letter between A and M (not including M) so would match ‘Gough Menzies’, but not ‘Sir Gough Menzies’!\n\nSELECT *\n  FROM table_name\n    WHERE column2 BETWEEN 'A' AND 'M';\n\nIf you look for a numeric (as opposed to text) then BETWEEN is inclusive.\nCombine conditions with AND (both must be true to be returned) or OR (at least one must be true):\n\nSELECT *\n  FROM table_name\n    WHERE column2 BETWEEN 'A' AND 'M'\n    AND column3 = 32;\n\nYou can order the result:\n\nSELECT *\n  FROM table_name\n    ORDER BY column3;\n\nAscending is the default, add DESC for alternative:\n\nSELECT *\n  FROM table_name\n    ORDER BY column3 DESC;\n\nRestrict the return to a certain number of values by adding LIMIT at the end:\n\nSELECT *\n  FROM table_name\n    ORDER BY column3 DESC\n    LIMIT 1;\n\n(This doesn’t work all the time - only certain SQL databases.)\n\n\nModifying data and using logic\nEdit a value:\n\nUPDATE table_name\n  SET column3 = 33\n    WHERE column1 = 1234;\n\nImplement if/else logic:\n\nSELECT *,\n  CASE\n    WHEN column2 = 'Gough Whitlam' THEN 'Labor'\n    WHEN column2 = 'Robert Menzies' THEN 'Liberal'\n    ELSE 'Who knows'\n  END AS 'Party'\n  FROM table_name;\n\nThis returns a column called ‘Party’ that looks at the name of the person to return a party.\nDelete some rows:\n\nDELETE FROM table_name\n  WHERE column3 IS NULL;\n\nAdd an alias to a column name (this just shows in the output):\n\nSELECT column2 AS 'Names'\n  FROM table_name;\n\n\n\nSummarising data\nWe can use COUNT, SUM, MAX, MIN, AVG and ROUND in the place of summarise in R. COUNT counts the number of rows that are not empty for some column by passing the column name, or for all using *:\n\nSELECT COUNT(*)\n  FROM table_name;\n\nSimilarly, pass a column to SUM, MAX, MIN, and AVG:\n\nSELECT SUM(column1)\n  FROM table_name;\n\nROUND takes a column and an integer to specify how many decimal places:\n\nSELECT ROUND(column1, 0)\n  FROM table_name;\n\nSELECT and GROUP BY is similar to group_by in R:\n\nSELECT column3, COUNT(*)\n  FROM table_name\n    GROUP BY column3;\n\nYou can GROUP BY column number instead of name e.g. 1 instead of column3 in the GROUP BY line or 2 instead of COUNT(*) if that was of interest.\nHAVING for aggregates, is similar to filter in R or the WHERE for rows from earlier. Use it after GROUP BY and before ORDER BY and LIMIT.\n\n\nCombining data\nCombine two tables using JOIN or LEFT JOIN:\n\nSELECT *\n  FROM table1_name\n  JOIN table2_name\n    ON table1_name.colum1 = table2_name.column1;\n\nBe careful to specify the matching columns using dot notation. Primary key columns uniquely identify rows and are: 1) never NULL; 2) unique; 3) only one column per table. A primary key can be primary in one table and foreign in another. Unique columns have a different value for every row and there can be many in one table.\nUNION is the equivalent of cbind if the tables are already fairly similar."
  },
  {
    "objectID": "posts/2021-10-19-trinity_remarks/trinity_remarks.html",
    "href": "posts/2021-10-19-trinity_remarks/trinity_remarks.html",
    "title": "Remarks at Trinity College",
    "section": "",
    "text": "The slides are here."
  },
  {
    "objectID": "posts/2021-10-19-trinity_remarks/trinity_remarks.html#introduction",
    "href": "posts/2021-10-19-trinity_remarks/trinity_remarks.html#introduction",
    "title": "Remarks at Trinity College",
    "section": "Introduction",
    "text": "Introduction\nHi, my name is Rohan Alexander. I’m an assistant professor in the Faculty of Information and the Department of Statistical Sciences at the University of Toronto. I’d like to thank Academic Don, Statistics, Manmeet Sangha for the invitation to talk today at Trinity College.\nWhen I asked the undergraduate students that I work with who was associated with ‘Trin’, it was no surprise to me that the college was well represented among the strongest ones. ‘Trinnies’ have a well-deserved reputation for academic excellence and the number of people that have shown up for this event speaks to how that tradition continues even under trying circumstances.\nI’ve only got 5 minutes, so I’d like to touch on just two aspects:\n\nAn example of what I do in data science.\nSome steps to get started in data science."
  },
  {
    "objectID": "posts/2021-10-19-trinity_remarks/trinity_remarks.html#what-i-do-in-data-science",
    "href": "posts/2021-10-19-trinity_remarks/trinity_remarks.html#what-i-do-in-data-science",
    "title": "Remarks at Trinity College",
    "section": "What I do in data science",
    "text": "What I do in data science\nThere is no agreed definition of data science, but an informal one is: ‘humans measuring stuff, typically related to other humans, and using sophisticated averaging to explain and predict’.\nI like that definition because it does not treat data as terra nullius, or nobody’s land. Data must be gathered, cleaned, and prepared, and these decisions matter. Every dataset is sui generis, or a class by itself.\nA lot of what my research does is try to understand the specific nuances of specific datasets, especially in terms of who is included in them, and who is not, and understand how this affects inference.\nOne example of this is natural language processing. There we use really big models, built on a lot of data, to generate text. To make sure everyone knows what that means, here’s a model that has two parameters: \\(y = x1 + x2\\). And is trained on 5 rows of data.\nOpenAI’s GPT-3 has 175 billion parameters and was trained on essentially the whole internet. I’ll do a quick demonstration of asking it: “What is Trinity College, Toronto?”, “Which famous person went to Trinity College, Toronto?”\n[Live demo]\nGPT-3, like any language model, can generate racist/sexist text. With a student I recently used it to see whether its ability to generate racist/sexist text, also meant that it could recognize racist/sexist text and explain what made it racist/sexist. The results were disappointing and lead to a variety of conclusions, including the fact that we need to do a better job of putting together the enormous datasets that underpin these models."
  },
  {
    "objectID": "posts/2021-10-19-trinity_remarks/trinity_remarks.html#some-steps-to-get-started-in-data-science",
    "href": "posts/2021-10-19-trinity_remarks/trinity_remarks.html#some-steps-to-get-started-in-data-science",
    "title": "Remarks at Trinity College",
    "section": "Some steps to get started in data science",
    "text": "Some steps to get started in data science\nOne way to contribute in data science is to be at the intersection of two areas. Typically statistics and something else. Then you need to immerse yourself in the data. One of the most successful ‘Trinnies’ - Michael Ignatieff (IG-NAH-TEE-UHF) - says about politics that\n\nThe grind of politics, the endless travel, the meetings, the impossible schedule, the constant being on show are all in search of an authority that can be acquired in no other way. You have to learn the country.\n\nAnd it’s the same in data science - you need to become an expert in some dataset by doing the grinding, endless, impossible schlep to understand it. Only then can you even begin to consider inference and prediction.\nA lot of what you do in class is play house; when what you need to do is actually build something. One thing that helped me in this regard was getting to work with professors.\nSo how do you get to work with a professor? There’s an easy answer, which I expect is relevant for many ‘Trinnies’, and that’s to just to get an A+ in their class. But more generally, you should send an email that demonstrates that you’ve got:\n\nan interest; and\nsome skills.\n\nFor instance, these days a lot of papers in the fields that I’m interested in need some type of R package to go alongside them, and it can be hard to find students who can do this. If a student emailed having made an R package, that would demonstrate a lot of interest and skills, and it would be clear how I could involve them. There’s an enormous difference between a student who emails claiming to know R, and a student who emails a link to a GitHub repo that demonstrates it."
  },
  {
    "objectID": "posts/2021-10-19-trinity_remarks/trinity_remarks.html#concluding-remarks",
    "href": "posts/2021-10-19-trinity_remarks/trinity_remarks.html#concluding-remarks",
    "title": "Remarks at Trinity College",
    "section": "Concluding remarks",
    "text": "Concluding remarks\nThank you very much to Manmeet for the invitation to be on this panel. It’s a great honour to be here, alongside my colleagues - Sam, Emma, and Morris - and one of my bosses - Helen! Very much looking forward to the discussion."
  },
  {
    "objectID": "posts/2021-07-06-turning-our-world-into-data/turning-our-world-into-data.html",
    "href": "posts/2021-07-06-turning-our-world-into-data/turning-our-world-into-data.html",
    "title": "Turning our world into data",
    "section": "",
    "text": "Hi, I’d like to thank Jesse Gronsbell for letting me talk today. My name is Rohan Alexander, and I’m an assistant professor in the Faculty of Information and the Department of Statistical Sciences at the University of Toronto.\nProfessor Gronsbell is also a professor in Statistical Sciences, but that’s about where my comparison with Jesse ends. Jesse works on a wide range of biostatistics problems, using classical statistical techniques to solve new problems, especially with large observational health data sets such as electronic health records. It’s a privilege to get to call her a colleague in data science.\nI’m just someone who likes to play with data using the statistical programming language R (R Core Team 2020). And the nice thing about what we now call data science is that there’s space for both of us. And there’s space for you as well."
  },
  {
    "objectID": "posts/2021-07-06-turning-our-world-into-data/turning-our-world-into-data.html#introduction",
    "href": "posts/2021-07-06-turning-our-world-into-data/turning-our-world-into-data.html#introduction",
    "title": "Turning our world into data",
    "section": "",
    "text": "Hi, I’d like to thank Jesse Gronsbell for letting me talk today. My name is Rohan Alexander, and I’m an assistant professor in the Faculty of Information and the Department of Statistical Sciences at the University of Toronto.\nProfessor Gronsbell is also a professor in Statistical Sciences, but that’s about where my comparison with Jesse ends. Jesse works on a wide range of biostatistics problems, using classical statistical techniques to solve new problems, especially with large observational health data sets such as electronic health records. It’s a privilege to get to call her a colleague in data science.\nI’m just someone who likes to play with data using the statistical programming language R (R Core Team 2020). And the nice thing about what we now call data science is that there’s space for both of us. And there’s space for you as well."
  },
  {
    "objectID": "posts/2021-07-06-turning-our-world-into-data/turning-our-world-into-data.html#my-path-to-data-science",
    "href": "posts/2021-07-06-turning-our-world-into-data/turning-our-world-into-data.html#my-path-to-data-science",
    "title": "Turning our world into data",
    "section": "My path to data science",
    "text": "My path to data science\nI’m 34 and so I’m in the ‘data science didn’t exist when I was an undergrad’ generation. Most of you are about 17, or about half my age. When I was 17, I wasn’t thinking about data science summer camps nor self-driving cars, so it’s commendable that you’re already so advanced.\nWhen I was 17 I just went to uni and studied economics, because that’s what my friends were doing. Luckily, I had some terrific mentors including Rabee Tourkey, Flavio Menezes, and Leo Yanes, and what they did was give broad instructions then get out of the way and check in weekly. While I don’t think I delivered anything of value to them, that experience of working directly with professors was formative for me.\nOne day I was in the library with a friend, and I wanted to go to the pub, but my friend insisted I wait until he’d applied for an internship at the Reserve Bank of Australia (RBA). I didn’t want to go to the pub without him, so I applied for that internship as well while I was waiting.\nBe careful who you make friends with because the result was that after uni I worked for a bit at the RBA, which is Australia’s central bank (think: Fed Reserve) and that was great because I learnt about just how vital communication—both written and speaking—is. (My friend didn’t get the internship, but he ended up at Goldman Sachs, so he’s doing just fine.) And also, about navigating a traditional workplace.\nAfter that I started a business with some friends because that was what I was doing when I wasn’t at work, and it seemed to make sense to make that my actual job. And that start-up was great because I learnt about strategy, tactics, execution, teamwork, and leadership; all the stuff that traditional workplaces don’t let junior analysts worry about"
  },
  {
    "objectID": "posts/2021-07-06-turning-our-world-into-data/turning-our-world-into-data.html#why-data-science-is-exciting",
    "href": "posts/2021-07-06-turning-our-world-into-data/turning-our-world-into-data.html#why-data-science-is-exciting",
    "title": "Turning our world into data",
    "section": "Why data science is exciting",
    "text": "Why data science is exciting\nI’d like to come back to that earlier point—that data science didn’t exist when I was 17—because it may imply that one should not just be making decisions that optimize for what data science looks like right now, but also what could happen. While that’s a little difficult, that’s also one of the things that makes data science so exciting. As a 17-year-old that might mean choices like:\n\ntaking courses on fundamentals, not just fashionable applications;\nreading books, not just whatever is trending on Hacker News; and\ntrying to be at the intersection of at least a few different areas, rather than hyper-specialized.\n\nWhen that start-up finished, I started a PhD. My PhD is actually in economic history, so if I had my way, I’d spend my time in dusty, beautiful libraries reading old books and drinking coffee in the morning, and then writing code and drinking wine in the afternoon. Your library at Harvard is very nice, but not quite dusty enough for me, and you’re not allowed wine in there. The Faculty of Information, which is one of my appointments, originally existed to train librarians, so these days I get an office in a library, which is great, and my role is ‘human centered data science’, so it’s very much the best of both worlds. I get to turn our world into data, analyse it, and get paid to do it!\nI spent most of my PhD trying to deal with big text datasets. And when I say ‘dealing’ I mean using R to clean and tidy them, which was the work of years. My supervisors—John Tang, Tim Hatton, Martine Mariotti, and Zach Ward—gave me the freedom to do what I wanted, again with regular weekly meetings. It’s not a topic that traditionally would have been appropriate, and I’m grateful they gave me the space because traditional economics is not for me, but data science is. And I hope that you also consider that it could be for you."
  },
  {
    "objectID": "posts/2021-07-06-turning-our-world-into-data/turning-our-world-into-data.html#what-data-science-means-to-me",
    "href": "posts/2021-07-06-turning-our-world-into-data/turning-our-world-into-data.html#what-data-science-means-to-me",
    "title": "Turning our world into data",
    "section": "What data science means to me",
    "text": "What data science means to me\nWhat is data science? There’s not one agreed on definition, but a lot of people have tried. My other appointment is in the Department of Statistical Sciences, and my boss there says (Craiu 2019):\n\nThis unsureness isn’t necessarily the weakness many consider it. After all, who can really say what makes someone a poet or a scientist? Even so, some things can be said. In broad strokes, … someone with a data driven research agenda, who adheres to or aspires to using a principled implementation of statistical methods and uses efficient computation skills.\n\nI like this definition, but I also think there’s value in having a simple definition, even if we lose a bit of specificity in the process. Probability, which is related to statistics, is often informally defined as ‘counting things’ (McElreath 2020, 10). In a similar informal sense, I’m currently playing around with a definition of data science that is something like ‘measuring stuff and averaging it, with purpose’.\nOne reason that I like this definition is that it doesn’t treat data as terra nullius, or nobody’s land. Statisticians tend to see data as the result of some data generating process, which we can never know, but that we try to use data to come to understand. I’m oversimplifying here, and many statisticians care deeply about data, but at the same time, there’s a lot of cases in statistics where the data kind of just appear; they belong to no one. But that’s just never the case.\nData have to be gathered, cleaned, and prepared, and these decisions matter (Huntington-Klein et al. 2021). I’ve come to believe that every dataset is sui generis, or in a class by itself, and so when you come to know one dataset really well, you just know one dataset, not all datasets.\nDuring this data science camp, you’re going to be exposed to an awful lot of ‘science’. I’d like to spend a little more time in defence of ‘data’. And that’s another reason that I like my definition. I argue that the most important word in ‘data science’ is ‘data’ and would love to convince you of it.\nI’ll take an example from Jordan (2019) where he talks about being in a medical office and being given some probability, based on prenatal screening, that his child, then a fetus, had Down syndrome. By way of background, you can decide to test to know for sure, but that test comes with the risk of the fetus not surviving, so this initial screening probability matters. Jordan (2019) describes how he found those probabilities were determined by a study done a decade earlier in the UK. The issue was that in the ensuing 10 years, imaging technology had improved so the test wasn’t expecting such high-resolution images and there had been a subsequent (false) increase in Down syndrome diagnoses when the images improved. There was no problem with the science here, it was the data that were the issue."
  },
  {
    "objectID": "posts/2021-07-06-turning-our-world-into-data/turning-our-world-into-data.html#tips-for-students",
    "href": "posts/2021-07-06-turning-our-world-into-data/turning-our-world-into-data.html#tips-for-students",
    "title": "Turning our world into data",
    "section": "Tips for students",
    "text": "Tips for students\nIn my opinion, we are realising that it’s not just the ‘science’ bit that’s hard, it’s the ‘data’ bit as well. For instance, researchers went back and examined one of the most popular text datasets in computer science, and they found that around 30 per cent of the data were inappropriately duplicated (Bandy and Vincent 2021). There’s a lot of people who could have told the computer scientists that those datasets would have big problems; there’s an entire field—linguistics—that specialises in this stuff. And this is one of the dangers of any one field being hegemonic, and why it’s important that you don’t specialise too early. Instead, pick at least two different areas, learn how the ‘insiders’ speak in each, and then be the person that translates between them.\nData science needs diversity. And it’s one reason that I’m excited to see you all here with your variety of skills and backgrounds—we need you in our research labs. We need your intelligence and enthusiasm. We need you to be in the room making contributions. And I think, just like me, that working directly with professors would be formative and enjoyable for you. I hope you’re lucky enough, like me, to be given the space to find what types of projects you’re intrinsically interested in, because then everything just becomes a lot easier.\nHow do you push down the door and work with professors? I was very lucky and it was a lot easier for me to get opportunities than it is for you, because there were fewer of us. But basically, what I think you should do is show a professor that you’ve got:\n\nan interest; and\nsome skills.\n\nOne way to do this is through demonstrating your interest in coding and the data science skills that you’re developing at this camp. For instance, these days a lot of papers in the fields that I’m interested in need some type of R package to go alongside them, and it can be hard to find students who can do this. If a student emailed having made an R package, that would show a lot of interest and skills, and it would be clear how I could involve them. Another thing that is increasingly needed is a datasheet (Gebru et al. 2020). So again, you could put together documentation for a dataset and email that. Again, it shows that you’ve got a genuine interest and that you’ve got a base of skills that would enable you to be useful.\nThanks very much for letting me speak. I know that everyone says this to you, and with apologies to Olivia Rodrigo, but being 17 is just such an exciting time of one’s life, and I’m glad to see that you’re making the most of it. One seems to go straight from being the youngest in the room to being the oldest, without any middle. And, just so that you know, you kind of never work life out— everyone is just making it up as they go.\nI’d be happy to take any questions."
  },
  {
    "objectID": "posts/2021-07-06-turning-our-world-into-data/turning-our-world-into-data.html#acknowledgments",
    "href": "posts/2021-07-06-turning-our-world-into-data/turning-our-world-into-data.html#acknowledgments",
    "title": "Turning our world into data",
    "section": "Acknowledgments",
    "text": "Acknowledgments\n\nThank you very much to Monica for reading a draft of this."
  },
  {
    "objectID": "posts/2021-05-04-work-life-balance/work-life-balance.html",
    "href": "posts/2021-05-04-work-life-balance/work-life-balance.html",
    "title": "On work-life balance",
    "section": "",
    "text": "Hi, my name is Rohan Alexander. I’m an assistant professor at the University of Toronto, in the Faculty of Information, and the Department of Statistical Sciences. I’d like to thank Tony Tang for the invitation to talk about work-life balance.\nThe work-life balance that I’m going to talk about today is just something for me, right now. So maybe some of what I’ll talk about will be relevant for you, but maybe my balance won’t be, and that’s okay. My main take-away, in any case, is that you’re smart enough to work out what works for you.\nI don’t think there’s only one optimal work-life balance, and if there is one, then it certainly changes over time. While doing my PhD, and also my wife doing her PhD, and us both now in faculty jobs, we’ve had a variety of opportunities to experience a bunch of different approaches to all this. We’ve also now got a two-year-old and another on the way, so I’ve reflected on all this to come up with a few lessons that I’d like to share."
  },
  {
    "objectID": "posts/2021-05-04-work-life-balance/work-life-balance.html#introduction",
    "href": "posts/2021-05-04-work-life-balance/work-life-balance.html#introduction",
    "title": "On work-life balance",
    "section": "",
    "text": "Hi, my name is Rohan Alexander. I’m an assistant professor at the University of Toronto, in the Faculty of Information, and the Department of Statistical Sciences. I’d like to thank Tony Tang for the invitation to talk about work-life balance.\nThe work-life balance that I’m going to talk about today is just something for me, right now. So maybe some of what I’ll talk about will be relevant for you, but maybe my balance won’t be, and that’s okay. My main take-away, in any case, is that you’re smart enough to work out what works for you.\nI don’t think there’s only one optimal work-life balance, and if there is one, then it certainly changes over time. While doing my PhD, and also my wife doing her PhD, and us both now in faculty jobs, we’ve had a variety of opportunities to experience a bunch of different approaches to all this. We’ve also now got a two-year-old and another on the way, so I’ve reflected on all this to come up with a few lessons that I’d like to share."
  },
  {
    "objectID": "posts/2021-05-04-work-life-balance/work-life-balance.html#lessons",
    "href": "posts/2021-05-04-work-life-balance/work-life-balance.html#lessons",
    "title": "On work-life balance",
    "section": "Lessons",
    "text": "Lessons\n\nLesson 1: You’re smart enough to work out what works for you\nThe first lesson is that you’re smart enough to work out what works for you. When my first child was born, it was all a bit challenging. As folks who have done or are doing a PhD, we’re trained in, and selected for, certain attributes, and none of those have any relation to your ability to care for an infant when it’s 3am and you’ve not slept, and the child won’t eat. I did what I was trained to do, which was look at the literature, synthesize it, plan, and execute. And of course, that didn’t help.\nOne of my PhD supervisors had also had a challenging time around the birth of her children, and she emailed me, and she said, ‘you’re smart enough to work out what works for you’. And that’s the key bit. You’re here. The admissions committee doesn’t make mistakes.\nToday is all about us giving you advice, but in the end you’re smart enough to work out what work-life balance works for you.\n\n\nLesson 2: Ignore what everyone else says and does\nThe second lesson is to ignore what everyone else is saying. Our friend once said on Twitter there are two types of people: one type says they work all the time, and the other type say they never work. The takeaway is that both types of people are probably exaggerating, and you probably shouldn’t believe either of them. Instead, you need to think about your own circumstances and decide what balance makes sense. And you need to recognize that will change as your circumstances change.\nI’ve always been a morning person, so during my PhD I used to get up at 6am to start work and then stop in mid-afternoon and go for a run and then cook dinner early and then maybe do some lighter work in the evening. And I got good at working for long chunks of time. Then the baby arrived. I became a night person for a while, then I became a bursts-of-work person for a while, now I’m back to being a morning person, but it’s more of a 4am start to get a chunk of work done, and then various bursts-of-work, followed by a short morning chunk, and then very little in the afternoon, when I may need to look after him, depending on sleep, and then maybe another chunk at night if he goes down easily.\nIgnore what others are doing, and react to your own circumstances.\n\n\nLesson 3: Doing a PhD is hard\nThe third lesson is that doing a PhD is really, really hard (and it’s meant to be). The fundamental purpose of doing a PhD is to create new knowledge. Knowledge that literally no one in the world has.\nOne way to think about it is that you’re going to become the world champion in one tiny little aspect of the world. The days of Oxbridge gentlemen athletes winning Olympic medals are gone, and that’s true also of easy PhDs. What’s left for us is the stuff that takes a huge amount of effort. Recognizing that fact, and the implication that working really hard at this is what it’s going to take, is crucial.\nThe work-life balance that I experience in a university isn’t, and cannot be, the work-life balance of those that I know outside of universities. This job is just really different. You have to be prepared for things to be hard, and to work hard, and try not to take it personally when things don’t work out straight away. It’s part of the process\n\n\nLesson 4: An extra hour of work is almost always worthwhile\nThe fourth lesson builds on this and it’s that putting an extra hour into your work is always worthwhile. The nature of our job—creating new research—means that there is always something to do. This is not a job where you lay 500 bricks in a day and then call it a day. I had a job like that once, briefly, and that job was hard. But when it was done, it was done. Our job—doing research—is not like that. There’s always something else that you can do. And it’s beneficial to do that thing today, because that way you can build on it tomorrow.\nThere’s enormous compounding in academia. Awards, conference and journal acceptances tend to go to those who already have plenty of them. So how do you get that first one? It’s just hard, but putting an extra hour in will almost always compound over time and be useful. The thing about compounding interest is that initially you don’t notice any benefit, but after a few years there’s an enormous difference.\n\nLesson 4.5: Until it isn’t\nHowever, there is a corollary to that fourth lesson, and that is: Until it isn’t. The fourth and a half lesson, as it were. The temptation is to just spend more and more time working. And that’s fine for a while, and is important to be able to do, from time to time. But it doesn’t work for more than a month or two.\nBut now we have a contradiction. On the one hand, I’ve just told you that there’s enormous benefits to compounding, and we all know about the importance of flow and staying in that state when you can achieve it. And on the other hand, if you take that to its natural conclusion and work all the time, you won’t last in academia for long. So, what do we do?\n\n\n\nLesson 5: Use positive procrastination\nThis leads to the fifth lesson, which is that you use positive procrastination to your advantage. I wrote this talk when I was tired of writing my book, which I write when I’m tired of trying to write a paper. None of this is wasting time, but the task switching ensures a freshness and enthusiasm. Another advantage of positive procrastination when you’re a PhD student is that it also helps you identify what you’re passionate about.\nNow you don’t have to love what you do—plenty of professors don’t—but I do and I’m going to speak now assuming this to be true, because it’s gone part way to resolving this contradiction.\nA friend once said that a way to think about passion is that it’s your brain’s way of tricking you into doing things that are boring or are hard. And if you can identify passion then it’ll make your life a lot easier. When I started my PhD, I was in a bit of a malaise. I’d go from subject to subject, and certainly work at them, but not enjoy anything and there were a lot of long coffees with friends. I’d cycle home, and then at night and on the weekends, I’d spend my whole-time using R to do statistical analysis of political data in a reproducible way. Then during the day I’d go to university and go back to my malaise of not knowing what to do. Of course, with some support from various advisors and mentors, I eventually worked out that what I should be doing is using R to do statistical analysis of political data in a reproducible way for my PhD and after that everything got a lot easier! I loved what I was doing and work no longer felt like work.\nThere’s a limit, of course, to positive procrastination. Because you need to make sure that you’re shooting, and not just setting up shots, as it were. As a PhD student, you probably want to have about three projects, and that’s about it. But I do think that cycling through, especially early in your PhD when it’s easy to just email a professor and take a reading course to write a short paper, is something that more students should do and would help resolve work-life concerns.\nAnd again, the viability of this changes over your career. Now I have about 10-15 different projects because I supervise students, and I have some longer-term gambles that may or may not work out. However, because of how I have to work in fits and starts, I can’t just do what I was trained to do, which is to throw more time into things. I need to try to be smarter about things and look for efficiencies and relationships between them.\n\n\nLesson 6: Take complete, week/s-long breaks\nAnd so we reach the sixth and final lesson, which is about having complete breaks, and I learnt this one from someone that I’ll name, and that’s Professor Kelly Lyons. Because once you’ve identified your passion and you understand the power of compounding and flow, it can be easy to just stay caught up in things. But to have an academic career or to finish a PhD without burning out, and if your work-life trick is going to be that you’d be doing what you get paid to do, even if you didn’t get paid to do it, then you need to make sure that remains the case.\nMonica and I have started insisting on complete week-long breaks from time to time. Minimal computers, and no GitHub. And this is one aspect where having children certainly helps. Because they want all of your attention."
  },
  {
    "objectID": "posts/2021-05-04-work-life-balance/work-life-balance.html#we-get-to-do-what-we-want",
    "href": "posts/2021-05-04-work-life-balance/work-life-balance.html#we-get-to-do-what-we-want",
    "title": "On work-life balance",
    "section": "We get to do what we want",
    "text": "We get to do what we want\nOur work-life balance in academia is different to that of doctors, lawyers, and tech bros. We have the best job in the world. But our job is not like others. One of the great things about academia is the flexibility. We get to do whatever we want.\nThat point bears repeating. We get to do whatever we want. And that means that whatever we’re doing is something that we’re choosing.\nHence, for me, it’s become wrong to think that work and life are separate and that they need to be siloed. People outside of universities talk about work-life balance and the implication is that you need to make time for each. But in academia, that’s not the way that I think of things.\nI think it’s a spectrum. For some people, work is work, and life is life and they’re completely separated, while for others all they do is work and that’s their life. For most of us, we’re probably somewhere in between. And I think that’s especially the case in academia. Because you’ve got so much flexibility and ownership over what we do compared with, say, working at a bank. And so, in terms of work-life balance, it’s about trying to work out what works for you. Thinking about what’s important in your life and how that fits into your work. And also, being aware that this balance is going to change especially around life events such as having a baby, which will affect how you can work, as well as the volume. And that this change is good, appropriate, and normal, even if that’s not recognized in the current academic structures.\nIf you enjoy coding, then taking a break from work might mean doing a fun coding project. If you enjoy hiking, then taking a break from work might mean going hiking. When my wife was doing her PhD, we went on road trips and worked in different environments. We did this because we loved our work, not because anyone was forcing us. We went to different cafes, and ate different foods, and ran in different parks, and went to different pubs, but for most of the day we sat at tables in random Airbnbs and worked. We have great memories of that, but not everyone wants to do that and that’s okay.\nInitially, your supervisor will put a huge amount of pressure on you, but I’ve found that as I’ve gotten older the person putting the most pressure on me is myself. Once you’re at that stage, if you think ‘oh I want a break, but I should get this done because the code isn’t working, or I need to get it to my collaborator, or whatever’, then chances are that it’s okay and you should probably just email the collaborator to say that it’ll be a week late and then take the week off. From time to time, I’ll just declare a day off and not open the computer. If you think that maybe you don’t have time for something, then you are probably right and you need to get out of it.\nIn grad school I’ve found that many people lose their hobbies. But maybe there is value in trying to make time, particularly for exercise. So, things like going for a run or being part of a team might be important for you. During my PhD a lot of my friends continued with rock-climbing, or cycling, or took up walking."
  },
  {
    "objectID": "posts/2021-05-04-work-life-balance/work-life-balance.html#between-the-ideal-and-the-reality-falls-the-shadow",
    "href": "posts/2021-05-04-work-life-balance/work-life-balance.html#between-the-ideal-and-the-reality-falls-the-shadow",
    "title": "On work-life balance",
    "section": "Between the ideal and the reality falls the shadow",
    "text": "Between the ideal and the reality falls the shadow\nIncreasingly we see that the composition of academia is changing, for instance, in terms of the proportion of women in masters and PhD programs and junior faculty. However, institutional structures at all stages of academia are still fundamentally designed for people—particularly men—without caring responsibilities; and as such those who do not fit this mould often face enormous challenges\nWhile things have changed, this structural issue has been increasingly brought to light during the pandemic. NSERC submission deadlines remained the same. NSERC grant application requirements remained the same. I said earlier that you should take a week off, but the amount of work to be done remains the same and NSERC doesn’t change its deadlines just because you took a week off. Similarly, departments here didn’t change their teaching, or service requirements.\nA lot of people with young children have not done any research for a year, and applied for few grants. Generalising from the literature on the effect of long-term job losses, chances are that, on average, our careers will not recover. And that’s okay. We still get to do what we love. We might not become full professors, but being an associate would be great. It’s still the best job in the world.\nWhile I still think that it is correct to say, as I did above, that one shouldn’t compare oneself to others, the fact remains that the academic structures do exactly that. If you are not of the type that academia is set-up for, then every day you will fail to measure up to those expectations. We are not in a normal job, and that’s okay, but you need to recognize that the structures that job exists within may not be set-up for your circumstances. And it may be that these circumstances will dictate what can be accomplished, rather than your abilities."
  },
  {
    "objectID": "posts/2021-05-04-work-life-balance/work-life-balance.html#concluding-remarks",
    "href": "posts/2021-05-04-work-life-balance/work-life-balance.html#concluding-remarks",
    "title": "On work-life balance",
    "section": "Concluding remarks",
    "text": "Concluding remarks\nI don’t think there’s a one-size-fits-all for work-life balance. You need to work it out for yourself and part of that is just worrying about yourself, ignoring what others do or say they do. You’re smart enough to work out what works for you, so just think. And once you work out what works for you, then set-out to make it easier for others.\nI think that there’s value in the struggle. Even if my cohort of academics with young children doesn’t achieve what it may have, there’s value in sticking with it, while that’s possible. Next time, maybe some of us will be the ones who are chairs and NSERC reviewers. Even now, we have opportunities to make things easier for the next cohort, some of whom may be the undergrads that have spent the past year in Zoom classrooms. We have the best job in the world, and sometimes taking a step back and seeing it as bigger than oneself is important.\nThanks very much to Tony for giving me the opportunity to reflect on work-life balance at universities and how that changes over the life course, and I hope that he doesn’t regret providing me with the platform too much. I’d be happy to take any questions."
  },
  {
    "objectID": "posts/2021-05-04-work-life-balance/work-life-balance.html#acknowledgments",
    "href": "posts/2021-05-04-work-life-balance/work-life-balance.html#acknowledgments",
    "title": "On work-life balance",
    "section": "Acknowledgments",
    "text": "Acknowledgments\n\nThank you very much to Monica for reading a draft of this. Thank you also to John, Zach, Martine, Tim, and Jill at the ANU, and Kelly at U of T for helpful conversations about this topic at various points in time."
  },
  {
    "objectID": "posts/2009-05-25-plastic-policies/plastic-policies.html",
    "href": "posts/2009-05-25-plastic-policies/plastic-policies.html",
    "title": "Plastic Policies",
    "section": "",
    "text": "Written with Professor Flavio Menezes and originally published at Australian Policy Online.\nThere is broad agreement that Australian plastic bag consumption should be reduced. To this end, recent South Australian legislation has banned certain types of plastic bags. But other states wishing to reduce their plastic bag consumption may find a tax rather than a ban the more appropriate policy instrument.\nIn South Australia, one of the reasons for a ban rather than a tax was the belief that a tax would impose an additional cost on households. The implied corollary is that a ban on plastic bags would not impose these costs, and thus would presumably be paid for entirely by retailers. This claim is incorrect: not only does a ban on plastic bags impose costs on households, but they are greater than those imposed by a tax.\nTo understand the impact of a tax on plastic bag consumption it is necessary to distinguish between the initial and final ‘incidence’ of the tax — in other words, between who is legally responsible for paying a tax, and whom the burden of paying the tax would be passed on to. It is the final incidence that matters from a policy perspective. Economic theory suggests that except under very special circumstances, final and initial incidences do not coincide. Indeed, evidence from Germany, Ireland and Switzerland suggest that plastic bag consumption decreases as a result of a tax; and that it is both retailers and households who bear the burden of paying the tax, regardless of the initial incidence.\nUnder a ban on plastic bags, retailers must switch to other, more ex- pensive, options. Again the distinction between initial and final incidence is important. Although retailers would be legally responsible for providing the more expensive bags required to replace plastic bags, at least some of this additional cost would be passed onto households through higher prices or an explicit charge. Thus, regardless of whether a tax or a ban is used, households bear an additional cost.\nSo is a tax or a ban the more appropriate instrument? A ban reduces the number of plastic bags to zero, whatever the cost. Bans on smoking in workplaces, for example, are considered desirable because the cost of even a small amount of smoking in the workplace is believed to be unacceptable. For this reason, society is prepared to pay any price to ensure a non-smoking work environment. Is the same true for plastic bags?\nIn contrast, under a tax the reduction in the number of plastic bags is uncertain, but the additional cost imposed on households is known: it is the size of the tax. A tax more explicitly considers the cost of reduction. It allows households a choice that takes into account the cost and the ben- efit and results in plastic bag usage in cases where the benefit of doing so outweighs the cost.\nWhen the benefit of each additional unit of plastic bag reduction is small compared to the cost a tax is the better option; for should this cost be higher than anticipated, a tax rather than a ban would cost society less. And given that a ban supposes the cost of abatement to be entirely outweighed by the benefit for every level of abatement it is, in effect, not possible that a government advocating a ban has underestimated it.\nAlthough a number of factors must be considered when deciding between a tax or a ban, a ban on plastic bags is unlikely to be the best option for minimising the financial impact on households."
  },
  {
    "objectID": "posts/2021-12-16-on_the_privilege_of_turning_our_world_into_data/on_the_privilege_of_turning_our_world_into_data.html",
    "href": "posts/2021-12-16-on_the_privilege_of_turning_our_world_into_data/on_the_privilege_of_turning_our_world_into_data.html",
    "title": "On the privilege of turning our world into data",
    "section": "",
    "text": "The slides are here."
  },
  {
    "objectID": "posts/2021-12-16-on_the_privilege_of_turning_our_world_into_data/on_the_privilege_of_turning_our_world_into_data.html#introduction",
    "href": "posts/2021-12-16-on_the_privilege_of_turning_our_world_into_data/on_the_privilege_of_turning_our_world_into_data.html#introduction",
    "title": "On the privilege of turning our world into data",
    "section": "Introduction",
    "text": "Introduction\nHi, my name is Rohan Alexander. I’m an assistant professor at the University of Toronto in the Faculty of Information and the Department of Statistical Sciences. I’m also one of the co-leads of the University of Toronto Data Sciences Institute Thematic Program in Reproducibility. We’d love to develop ties with other institutions who are similarly interested in these issues, so please do get in touch.\nI’d like to thank Niamh Cahill for the opportunity to talk today. I always feel a little awed by everything that she’s able to accomplish and all her publications. In preparing for this talk I was looking for imminent historical Irish statisticians, and of course arguably one of the most imminent present-day Irish statisticians is Adrian Raftery, who had a PhD student, Leontine Alkema, who was Niamh’s postdoc advisor. And Niamh certainly lives up to her pedigree.\nI must admit that I feel a bit of a fraud talking today at this Young Irish Statisticians group. I’m no longer young, I’m certainly not Irish, and I’m not a statistician. But I do adore your country, and I spend a lot of time with statisticians, and I was once young. That said, I do hope that in a few years you can invite me back and we can do this in person, perhaps at the middle-aged Irish-fans pseudo-statisticians’ group!\nToday I’d like to talk a little about the origins of data science, especially mentioning the contributions of some Irish statisticians. I will turn to what I see data science as, and what I see are some of our roles and responsibilities as quantitatively interested folks. I will then go through various applications including: understanding the effect of elections; hate speech detection; and the reproducibility of COVID-19 pre-prints. While going through those applications I will focus more on what I learned and how I developed, rather than specific results. I will close with a few open issues.\nNone of what I’m about to say is cannon, this talk is more my way of trying to work out what I think, so I’d appreciate your reactions and comments."
  },
  {
    "objectID": "posts/2021-12-16-on_the_privilege_of_turning_our_world_into_data/on_the_privilege_of_turning_our_world_into_data.html#what-is-data-science",
    "href": "posts/2021-12-16-on_the_privilege_of_turning_our_world_into_data/on_the_privilege_of_turning_our_world_into_data.html#what-is-data-science",
    "title": "On the privilege of turning our world into data",
    "section": "What is data science?",
    "text": "What is data science?\nWhen we think about data science, I think that we all have different things in mind.\nThe only thing that is certain, is that there is no agreed definition of data science, but a lot of people have tried. For instance, Wickham and Grolemund (2016) say it is ‘…an exciting discipline that allows you to turn raw data into understanding, insight, and knowledge.’ Similarly, Leek and Peng (2020) say it is ‘…the process of formulating a quantitative question that can be answered with data, collecting and cleaning the data, analyzing the data, and communicating the answer to the question to a relevant audience.’ Benjamin S. Baumer and Horton (2021) say it is ‘…the science of extracting meaningful information from data.’ And Timbers, Campbell, and Lee (2021) say they define ‘data science as the process of generating insight from data through reproducible and auditable processes’.\nCraiu (2019) argues that the lack of certainty as to what data science is, might not matter because ‘…who can really say what makes someone a poet or a scientist?’ He goes on to broadly say that a data scientist is ‘…someone with a data driven research agenda, who adheres to or aspires to using a principled implementation of statistical methods and uses efficient computation skills.’\nRegardless of who is right, alongside those specific, more-technical, definitions, there is value in having a simple definition, even if we lose a bit of specificity. For instance, probability is often informally defined as ‘counting things’ (McElreath 2020, 10). In a similar informal sense, data science can be defined as something like: ‘humans measuring stuff, typically related to other humans, and using sophisticated averaging to explain and predict’.\nThat may sound a touch cute, but Francis Edgeworth, the Irish nineteenth century statistician and economist, who went to Trinity College Dublin, considered statistics to be the science ‘of those Means which are presented by social phenomena,’ so it is in good company (Edgeworth 1885).\nIn any case, one feature of this definition is that it does not treat data as terra nullius, or nobody’s land. Statisticians tend to see data as the result of some process that we can never know, but that we try to use data to come to understand. Many statisticians care deeply about data and measurement, but there are many cases in statistics where data kind of just appear; they belong to no one. But that is never actually the case.\nData must be gathered, cleaned, and prepared, and these decisions matter. Every dataset is sui generis, or a class by itself, and so when you come to know one dataset well, you just know one dataset, not all datasets.\nMore broadly, I think that much of data science focuses on the ‘science,’ but it is important that we also focus on ‘data.’ And that is another feature of my cutesy definition of data science which I posited before. A lot of data scientists are generalists, who are interested in a broad range of problems. Often, the thing that unites these is the need to gather, clean, and prepare messy data. And often it is the specifics of those data that require the most time, that update most often, and that are worthy of our most full attention. Unfortunately, it’s not typically the type of thing that is professionally rewarded."
  },
  {
    "objectID": "posts/2021-12-16-on_the_privilege_of_turning_our_world_into_data/on_the_privilege_of_turning_our_world_into_data.html#what-we-can-learn-from-a-historical-dublin-census",
    "href": "posts/2021-12-16-on_the_privilege_of_turning_our_world_into_data/on_the_privilege_of_turning_our_world_into_data.html#what-we-can-learn-from-a-historical-dublin-census",
    "title": "On the privilege of turning our world into data",
    "section": "What we can learn from a historical Dublin census?",
    "text": "What we can learn from a historical Dublin census?\nAt this point I’d like to look back, for a moment, at the origins of data science. If we look at the history of statistics, we very quickly find ourselves surrounded by Irish contributions.\nIn preparing for this talk I was thrilled to learn about the book by the Reverend James Whitelaw, and this is its actual title, ‘An essay on the population of Dublin. Being the result of an actual survey taken in 1798, with great care and precision, and arranged in a manner entirely new’ (Whitelaw 1905). I really do think that we all maybe need to adopt a little more of the Whitelaw’s detail in our paper titles!\nIn any case, Whitelaw (1905) says that he is fed up with the bad estimates of the size of the population in various capitals. And he particularly singled out London, which he says has estimates that range anywhere between 128,570 and 300,000. Instead, he says that he intends to make an accurate count of the number of inhabitants of Dublin.\nHe describes how:\n\nWhen I first entered on the business, I conceived that I should have little more to do than to transcribe carefully the list of inhabitants affixed to the door of each house by order of the Lord Mayor.\n\nLike many of us who spend our days in data, he found it was not that simple. Instead, he found:\n\n(t)he lists on the doors… presented generally to view a confused chaos of names, frequently illegible, and generally short of the actual number by a third, or even one-half’. He goes onto say that instead, he and his assistants braved ’the dread of infectious diseases,… filth, stench, and darkness,… to explore… every room of those wretched habitations, from the cellar to the garret, and on the spot ascertained their population.\n\nThe resulting tables looked something like this (Figure @ref(fig:iceberg)):\n\n\n\n\n\nExtract of the survey\n\n\n\n\nAnd in case you’re interested, his estimate of the total population of Dublin in 1798 was estimated at 182,370.\nI was surprised by how confident Whitelaw was in his numbers. My PhD is in economic history, where I was interested in Australian economic history, especially political history. And the thing about economic historians is that we have to create, clean, and prepare our own datasets from whatever traces we can find before we can analyse it. So that training focuses one’s mind on measurement, and sampling.\nAnd there’s an odd phenomenon that I’ve observed where the further one is away from decisions to do with the counting, measurement, organisation, and categorisation, the more one tends to trust the resulting datasets. That is, the person that actually does the hard work of constructing our datasets is usually the one that trusts them the least.\nI’ve never understood this confidence that people have in datasets that they don’t construct themselves.\nIt’s an absolute privilege to get to work in data science. To be able to feel, like Reverend Whitelaw did, that one is bringing some order to things. But it’s important to realise the perspective that one brings as one is doing that.\nNow my point here isn’t to attack Whitelaw, as he is long dead and incapable of defending himself, but, I skipped over a bunch of the actual quote in Whitelaw. In particular, after talking about that bit about just looking at the lists on the doors:\n\nAs the families of the middle and upper classes always contained some individual who was competent to the task, and as few had any motive to conceal or misrepresent, I found their lists, in general, extremely correct; but among the lower class, which forms the great mass of the population of this city, the case was very different…. This I at first imputed to design, but was afterwards convinced that it proceeded from ignorance and incapacity.\n\nBut how could he possibly know?\nTo a certain extent we are wasting our time. We have a perfect model of the world—it is the world! But it is too complicated. If we knew perfectly how everything was affected by the uncountable factors that influence it, then we could perfectly forecast a coin toss, a dice roll, and every other seemingly random process each time. But we cannot. Instead, we must simplify things to that which is plausibly measurable, and it is that which we define as data. Our data are a simplification of the messy, complex, world from which they were generated.\nThere are different approximations of ‘plausibly measurable.’ Hence, datasets are always the result of choices. We must decide whether they are nonetheless reasonable for the task at hand. We use statistical models to help us think deeply about, explore, and hopefully come to better understand, our data.\nMuch of statistics is focused on considering, thoroughly, the data that we have. And that was appropriate for when our data were predominately agricultural, astronomical, or from the physical sciences. But with the rise of data science, mostly because of the value of its application to datasets generated by humans, we must also actively consider what is not in our dataset. Who is systematically missing from our dataset? Whose data does not fit nicely into the approach that we are using and are hence is being inappropriately simplified? If the process of the world becoming data necessarily involves abstraction and simplification, then we need to be clear about the points at which we can reasonably simplify, and those which would be inappropriate, recognising that this will be application specific.\nData science needs diversity. And it’s one reason that I’m excited by all the initiatives to increase participation by women and other underrepresented groups. We need our labs, our classrooms, and of course, eventually, our startups and businesses to be representative of the broader society.\nThe process of our world becoming data necessarily involves measurement. As I mentioned before, paradoxically, often those that do the measurement and are deeply immersed in the details have less trust in the data than those that are removed from it. Even seemingly clear tasks, such as measuring distance, defining boundaries, and counting populations, are surprisingly difficult in practice. Turning our world into data requires many decisions and imposes much error. Among many other considerations, we need to decide what will be measured, how accurately we will do this, and who will be doing the measurement.\nAn important example of how something seemingly simple quickly becomes difficult is maternal mortality. That refers to the number of women who die while pregnant, or soon after a termination, from a cause related to the pregnancy or its management (WHO 2019). It is difficult but critical to turn the tragedy of such a death into cause-specific data because that helps mitigate future deaths. Some countries have well-developed civil registration and vital statistics (CRVS). These collect data about every death. But many countries do not have a CRVS and so not every death is recorded. Even if a death is recorded, defining a cause of death may be difficult, especially when there is a lack of qualified medical personal or equipment. Maternal mortality is especially difficult because there are typically many causes. Some CRVS have a checkbox on the form to specify whether the death should be counted as maternal mortality. But even some developed countries have only recently adopted this. For instance, it was only introduced in the US in 2003, and even in 2015 Alabama, California, and West Virginia had not adopted the standard question (MacDorman and Declercq 2018)."
  },
  {
    "objectID": "posts/2021-12-16-on_the_privilege_of_turning_our_world_into_data/on_the_privilege_of_turning_our_world_into_data.html#on-balance-between-data-and-science",
    "href": "posts/2021-12-16-on_the_privilege_of_turning_our_world_into_data/on_the_privilege_of_turning_our_world_into_data.html#on-balance-between-data-and-science",
    "title": "On the privilege of turning our world into data",
    "section": "On balance between data and science",
    "text": "On balance between data and science\nI mentioned at the start of the talk that I’m appointed in both the Faculty of Information and the Department of Statistical Sciences. For those who don’t know, faculties of information were created around the turn of the century to train librarians. Library science of course has a long and distinguished history of archiving, categorising, storing, and retrieving information, and for a long time when computers were first developed, a lot of the initial work was done by librarians, or at libraries. I suspect there is a gendered aspect to this, with librarians often being female, but a lot of that work was systematically overlooked and was re-invented later. This continues today, with our library at the University of Toronto doing excellent work that is consistently overlooked. I used to feel a bit funny about being jointly appointed, but it turns out that the first chair of the Department of Statistics at Trinity College Dublin, Gordon Foster, was not only a statistician, but he also created the International Book Numbering System (ISBN), which we still use today.\nIn 1968 Foster gave the Geary Lecture at the Economic and Social Research Institute in Dublin (Foster 1968). And I think it is worth revisiting that talk because he was clearly, yet another, Irish statistician who was well ahead of his time.\nFoster (1968) first describes how:\n\n[j]ust prior to the time when my own career in statistics was commencing, an important development took place in which I was therefore able to participate straight away, and which has since had a great effect on the development of the subject. I mean of course the invention of the computer.\n\nIt’s funny, but fifty years on, I actually feel the same way about my career!\nFoster (1968) goes on to say that\n\nit was something of an achievement to get a programme actually working. It was useful to have handy a screwdriver and scotch-tape, and to know where to kick the machine if it stuck.\n\nAnd as someone who gets the privilege to write code for living, I do wonder if much has changed!\nIt’s interesting that Foster (1968) points very clearly to data science in this talk, where he says:\n\n(s)tatistics are concerned with the processing and analysis of masses of data and with the development of mathematical methods of extracting information from data. Combine all this activity with computer methods and you have something more than the sum of its parts.\n\nTalking about computers at the time, Foster (1968) says:\n\n(t)hey are capable of performing any structured task, from planning a hospital diet, retrieving a legal precedent, or controlling stocks in a warehouse, to playing a reasonably good game of chess… I think, we begin to realise that the endeavour which I have referred to as information technology is no longer something just affecting specialists, but is bringing about changes in society affecting us all.\n\nHow easy it would be to change ‘information technology’ to ‘data science’ in those quotes for them to ring true almost a lifetime after they were spoken.\nWe always use various instruments to turn the world into data. For instance, in astronomy, the development of better telescopes, and eventually satellites and probes, enabled new understanding of other worlds. We similarly have new instruments for turning our own world into data being developed each day.\nIn the social sciences, a census was once a generational-defining event. And it’s appropriate that this talk happens near the Christmas holidays given the role that the census played in that event. But now we have regular surveys, transactions data available by the second, and almost all interactions on the internet become data of some kind. The development of such instruments has enabled exciting new stories to be told with data.\nI think for the past ten years or so, we’ve had the balance wrong in data science. There has been too much focus on the ‘science’ bit, without sufficient focus on the ‘data’ bit.\nIt is not just the ‘science’ bit that is hard, it is the ‘data’ bit as well. I feel that Foster would have known this, and we are just reinventing things. For instance, researchers went back and examined one of the most popular text datasets in computer science, and they found that around 30 per cent of the data were inappropriately duplicated (Bandy and Vincent 2021). There is an entire field—linguistics—that specialises in these types of datasets, and inappropriate use of data is one of the dangers of any one field being hegemonic. The strength of data science is that it brings together folks with a variety of backgrounds and training to the task of learning about some dataset. It is not constrained by what was done in the past. But this means that we must go out of our way to show respect for those who do not come from our own tradition, but who are nonetheless as similarly interested in a dataset, or in a question, as we are.\nI’m picking on CS a little here, but my home of the social sciences is just as bad and researchers like me are trying to refocus us on data a little more than we have been in the past.\nSo what are some examples of things that I work on?"
  },
  {
    "objectID": "posts/2021-12-16-on_the_privilege_of_turning_our_world_into_data/on_the_privilege_of_turning_our_world_into_data.html#examples-of-my-work",
    "href": "posts/2021-12-16-on_the_privilege_of_turning_our_world_into_data/on_the_privilege_of_turning_our_world_into_data.html#examples-of-my-work",
    "title": "On the privilege of turning our world into data",
    "section": "Examples of my work",
    "text": "Examples of my work\nI mentioned that my PhD is in economic history, but I spent most of my PhD trying to deal with big text datasets. And when I say ‘dealing’ I mean using R to clean and tidy them, which was the work of years. My supervisors—John Tang, Tim Hatton, Martine Mariotti, and Zach Ward—gave me the freedom to do what I wanted, with regular weekly meetings. It’s not a topic that traditionally would have been appropriate in economics, and I’m grateful they gave me the space because traditional economics is not for me, but data science is.\n\nEffect of elections and changed prime ministers\nOne result of this work was the paper—The Increased Effect of Elections and Changing Prime Ministers on Topics Discussed in the Australian Federal Parliament between 1901 and 2018—(Alexander and Alexander 2021) which was co-authored with my wife Monica. In Australia there is a written record of what is said in parliament called Hansard. We grabbed the text of everything said between 1901 and 2018 and then built a statistical model to look at whether the topics of discussion changed when there was an election or when there was a change in the prime minister. We found that changes in prime minister tend to be associated with topic changes even when the party in power does not change; and the effect of elections has been increasing since the 1980s, regardless of whether the election results in a change of prime minister.\n\nLesson: Always start small and then iterate.\nI came to that paper hopelessly naive and lost. I didn’t know that text analysis, and natural language processing more generally, was something that was incredibly popular. I didn’t have the skills that I needed, and really all I had was an all-consuming interest in the broad area - I didn’t even have a question. The dataset has over a billion words in it. So the main lesson for me was in dealing with big datasets. And here I don’t mean in terms of technical skills, I mean in terms of approach. These days when I have students and they want to use the Hansard or any large dataset, I start by insisting that they just use one month of data. After they analyse and write that up, then they can go to a year, and then they can continue to build up slowly.\nThe reason that I know now that’s the best way to go is that I somehow convinced myself that I couldn’t possibly answer any interesting questions until I had a century worth of data. And because I was teaching myself everything as I went, it took me more than a year to just get the data into a usable format.\nIf you take nothing away from this talk, please I beg you take this away: the most important, vital thing, is that you create a minimal viable product of any research. And that minimal viable product needs to be something that you can finish within a week. If you can’t do that then adjust the scope and the question until you can. Then you achieve that MVP and then you start to scale up to the question that you’re interested in, which is hopefully something that can be published.\n\n\n\nExplaining Why Text is Sexist or Racist with GPT-3\nAs anyone who has cared for young children knows, the response to almost any statement can be ‘why?’. One can quickly find oneself trying to explain the finer details of photosynthesis to a 3-year-old, and the extent to which one struggles with this tends to put in sharp relief the extent of one’s knowledge. Large language models such as OpenAI’s GPT-3 can generate text that is indistinguishable from that created by humans. They can also classify whether some given text is sexist or racist (Chiu and Alexander 2021).\nIn the paper—Explaining Why Text is Sexist or Racist with GPT-3—which was co-authored by Ke-Li Chiu we assess the extent to which GPT-3 can generate explanations for why a given text is sexist or racist. We prompt GPT-3 to generate explanations in a question-and-answer manner: ‘Is the following statement in quotes sexist? Answer yes or no and explain why.’ We then assess the adequacy of the explanations generated by GPT-3. We are interested in firstly, the extent to which it correctly classifies whether the statements are sexist/racist; and secondly, the reasonableness of the explanation that accompanies that classification.\nWe find that GPT-3 does poorly in the open-ended approach. When we add more structure to guide its responses the model performs better. But even when it correctly classifies racism or sexism, the accompanying explanations are often inaccurate. At times they even contradict the classification. On a technical level, we find a clear relationship between the hyper-parameter temperature and the number of correctly matched attributes, with substantial decreases as temperature increases.\n\nLesson: Always teach\nI just used a bunch of terms there and you probably assume that I know what they mean. In writing the paper that preceded this one, I actually fooled myself into thinking that I knew what they meant. It wasn’t until I tried to teach the material that I realised that I didn’t have a clue what was going on.\nA lot of my colleagues try to get out of teaching, and I can understand why they would think that, but I’ve found that having to teach has made me a better researcher. For one reason, I found that it’s too easy to fool yourself into thinking you know something until you have to explain it. And I also kind of think that a lot of us in academia need pressure, constraints, and a weekly structure, in order to do our best work.\n\n\n\nReproducibility of COVID-19 pre-prints\nThe final paper that I’d like to touch on is—Reproducibility of COVID-19 pre-prints—by Annie Collins and me (Collins and Alexander 2021). In that paper we are interested in the reproducibility of COVID-19 research. We create a dataset of pre-prints posted to arXiv, bioRxiv, medRxiv, and SocArXiv between 28 January 2020 and 30 June 2021 that are related to COVID-19. We extract the text from these pre-prints and parse them looking for keyword markers signalling the availability of the data and code underpinning the pre-print. For the pre-prints that are in our sample, we are unable to find markers of either open data or open code for 75 per cent of those on arXiv, 67 per cent of those on bioRxiv, 79 per cent of those on medRxiv, and 85 per cent of those on SocArXiv. We conclude that there may be value in having authors categorize the degree of openness of their pre-print as part of the pre-print submissions process, and more broadly, there is a need to better integrate open science training into a wide range of fields.\n\nLesson: You always need the code more than once\nI went into that paper thinking that I’d just quickly write some code to download some files and then be done with it. But then Annie wanted to broaden the scope of the paper, so the code needed to be re-written because she couldn’t understand what I’d done. And then we had to re-run the code because we wanted to update everything before she presented the paper. And that meant re-writing things. And then we to re-run the code before we submitted the paper. Again that meant re-writing things.\nWe eventually just turned the code into an R package, which is on CRAN as heapsofpapers (Alexander and Mahfouz 2021). Which is what I should have done from the start. The lesson that I learnt from this paper is that regardless of how certain you are that you’ll never use some particular code again, you’ll always need it."
  },
  {
    "objectID": "posts/2021-12-16-on_the_privilege_of_turning_our_world_into_data/on_the_privilege_of_turning_our_world_into_data.html#open-questions",
    "href": "posts/2021-12-16-on_the_privilege_of_turning_our_world_into_data/on_the_privilege_of_turning_our_world_into_data.html#open-questions",
    "title": "On the privilege of turning our world into data",
    "section": "Open questions",
    "text": "Open questions\n\nHow do we write unit tests for data science?\nOne thing that working with real computer scientists has taught me is the importance of unit tests. Basically this just means writing down the small checks that we do in our heads all the time. Like if we have a column that purports to the year, then it’s unlikely that it’s a character, and it’s unlikely that it’s an integer larger than 2500, and it’s unlikely that it’s a negative integer. We know all this, but writing unit tests has us write this all down.\nIn this case it’s obvious what the unit test looks like. But more generally, we often have little idea what our results should look like if they’re running well. The approach that I’ve taken is to add simulation - so we simulate reasonable results, write unit tests based on that, and then bring the real data to bear and adjust as necessary. But I really think that we need extensive work in this area because the current state-of-the-art is lacking.\n\n\nWhat happened to the revolution?\nI don’t understand what happened to the promised machine learning revolution in social sciences. Specifically, I’m yet to see any convincing application of machine learning methods that are designed for prediction to a social sciences problem where what we care about is understanding. I would like to either see evidence of them or a definitive thesis about why this can’t happen. The current situation is untenable where folks, especially those in fields that have been historically female, are made to feel inferior even though their results are no worse.\n\n\nHow do we think about power?\nAs someone who learnt statistics from economists, but now is partly in a statistics department, I do think that everyone should learn statistics from statisticians. This isn’t anything against economists, but the conversations that I have in the statistics department about what statistical methods are and how they should be used are very different to those that I’ve had in other departments.\nI think the problem is that people outside statistics, treat statistics as a recipe in which they follow various steps and then out comes a cake. With regard to ‘power’—it turns out that there were a bunch of instructions that no one bothered to check—they turned the oven on to some temperature without checking that it was 180C, and that’s fine because whatever mess came out was accepted because the people evaluating the cake didn’t know that they needed to check the temperature had been appropriately set. (I’m ditching this analogy right now).\nAs you know, the issue with power is related to the broader discussion about p-values, which basically no one is taught properly, because it would require changing an awful lot about how we teach statistics i.e. moving away from the recipe approach.\nAnd so, my specific issue is that people think that statistics is a recipe to be followed. They think that because that’s how they are trained especially in social sciences like political science and economics, and that’s what is rewarded. But that’s not what these methods are. Instead, statistics is a collection of different instruments that let us look at our data in a certain way. I think that we need a revolution here, not a metaphorical tucking in of one’s shirt."
  },
  {
    "objectID": "posts/2021-12-16-on_the_privilege_of_turning_our_world_into_data/on_the_privilege_of_turning_our_world_into_data.html#thank-you",
    "href": "posts/2021-12-16-on_the_privilege_of_turning_our_world_into_data/on_the_privilege_of_turning_our_world_into_data.html#thank-you",
    "title": "On the privilege of turning our world into data",
    "section": "Thank you",
    "text": "Thank you\nRight. So I think I’ll leave it there. Thank you again to Niamh for inviting me. And I hope that I haven’t bored you all too much.Merry Christmas, happy holidays, and I hope everyone has a wonderful start to 2022.\nI’d be happy to take any questions."
  },
  {
    "objectID": "posts/2021-12-16-on_the_privilege_of_turning_our_world_into_data/on_the_privilege_of_turning_our_world_into_data.html#acknowledgments",
    "href": "posts/2021-12-16-on_the_privilege_of_turning_our_world_into_data/on_the_privilege_of_turning_our_world_into_data.html#acknowledgments",
    "title": "On the privilege of turning our world into data",
    "section": "Acknowledgments",
    "text": "Acknowledgments\n\nThanks very much to Niamh Cahill for the invitation to speak, and to Monica Alexander for her suggestions and comments."
  },
  {
    "objectID": "posts/2016-01-14-notes-and-photos-from-iowa/notes-and-photos-from-iowa.html",
    "href": "posts/2016-01-14-notes-and-photos-from-iowa/notes-and-photos-from-iowa.html",
    "title": "Notes and Photos From Iowa",
    "section": "",
    "text": "Sincere thanks to Bec, Callam, Monica, and Owen for reading and improving these notes.\nBernie Sanders seems quite reasonable for a revolutionary. An energetic man of 74, he spoke for an hour in Perry, Iowa, to a room of 300 from only a few lines of handwritten notes, and then fielded half an hour of questions. He does not have the same aura that surrounded, then, Senator Obama in his own Iowa battle with, then, Senator Clinton in 2008 say those who saw both. Instead, Sanders has preternatural calm.\n\n\n\n\n\nPhoto of Bernie Sanders and audience at Perry, Iowa, by Monica Alexander.\n\n\n\n\nSo much calm, in fact, that some audience members who walked into the auditorium undecided, walked out excitedly supporting Sanders’ ‘socialist’ revolution. His revolution includes, amongst other features, universal medical coverage, paid maternity leave and a $15 hourly minimum wage; hardly revolutionary notions in many developed countries. For instance, few politicians that are against these policies get elected in Australia. However, Sanders will test whether a politician who supports them can get elected in the US.\nMuch has been made of Sanders’ recent polling. But the Iowa Caucasus, especially for Democrats, are a test not just of support, but enthusiasm and strength of will. Participants do not vote, they caucus. This means they gather with their neighbours and publicly indicate their support of a candidate - sometimes by raising a hand, sometimes by moving to a side of a room. Supporters must resist peer pressure, and continue supporting their candidate even as others try to sway them.\nIn addition to popularity, winning in Iowa requires organisation and attention to detail. This is why the operatives of Secretary Clinton, who is 68, remain content despite the polls. Caucusing is onerous, especially for those with children or without transport. A campaign is only as good as the number of supporters that it can get to turn up on the night. Clinton operatives are quick to mention their advantage in terms of this ‘ground game’.\nClinton’s ground game advantage is partly due to experience, but it is also due to money. Sanders unexpectedly raised $33 million in the final quarter of 2015, which compares favourably with Clinton’s $37 million, but Clinton has other sources of financial support. Sanders’ supporters should hope that the evident lack of preparation at the event in Perry, Iowa, itself (for instance, Sanders had to repeatedly ask for water as none had been left on the podium, nearly losing his voice on occasion) are not indicative of broader organisational oversights.\n\n\n\n\n\nPhoto of Trump protesters.\n\n\n\n\nDonald Trump is different. More than a thousand people watched Trump, who is 69, speak in Cedar Falls, Iowa. Almost all were white. What he lacks in substance or structure, Trump makes up for with self-confidence. For 40 minutes he verbally picked at this and that, discussing polls, as well as goading opponents via ‘hypotheticals’ and name-calling. The audience was allowed no questions.\n\n\n\n\n\nPhoto of Donald Trump at Cedar Falls, Iowa, by Monica Alexander.\n\n\n\n\nTrump does not campaign in poetry (although he did literally recite song lyrics) and he seems unlikely to govern in prose. His operatives were clad in ill-fitting suits and shiny leather shoes that looked newly purchased. These ‘Storm Trumpers’ were unfailingly polite, but nonetheless menacing. The loudspeaker request, moments before Trump spoke, to not physically harm protesters was chilling rather than reassuring, not least since it was followed by laughter from the crowd. The comfort of knowing that the Secret Service would have confiscated any knives or guns at the door was relative rather than absolute. [Edit 26/1/16: Trump has since asserted that he could shoot somebody and not lose any voters. You get the uneasy sense that it would be better for his theory to remain untested, for fear that he may onto something.]\n\n\n\n\n\nPhoto of Donald Trump audience at Cedar Falls, Iowa, by Monica Alexander.\n\n\n\n\nTrump’s political inexperience seems matched by that of his supporters. His warm-up acts (one of whose qualification, as she explained, was being a runner-up on The Apprentice) spent considerable time explaining the importance of turning out to caucus on 1 February. Unlike the Democrats, Republican caucus-goers do not have to be as resistant to peer pressure - secret ballots are possible. But much of the crowd seemed new to the political process, and getting each of them to turn out, and in some cases register as Republican, may be too much to expect. If so, then it is likely that Senator Ted Cruz, a 45-year-old conservative Republican from Texas, will prevail.\n\n\n\n\n\nPhoto of Trump at Cedar Falls, Iowa, by Monica Alexander.\n\n\n\n\nFor all their differences, it is the same anger that propels Sanders and Trump toward the top of the polls. Neither candidate is an establishment member of their respective parties – Sanders only joined earlier this year despite having generally voted with the Democrats as a senator, and Trump appears to swear allegiance only to himself. It is the feeling of being let down by the status quo, of the system needing a catalyst for something more, that drives their popularity.\nWhile Sanders’ policies may not make much difference for today’s caucus-goers, he speaks to their concern that their children’s lives may not be better than their own. His is an appeal for hope. Trump’s appeal is to those who feel they are worse off now than they were in the past. He gives them someone to blame, and provides solutions such as tariffs and walls, that some see as plausible. Perhaps for Sanders the glass is half-full; for Trump, half-empty?\nThe economic reality is that feeling worse off is reasonable for many Americans. After accounting for inflation the 2014 measure (which is the most recent one) of American household median income is lower than it was in 1997. And, as both Trump and Sanders accurately explained to their audiences, the 5 per cent unemployment rate that President Obama appeals to as a measure of his success is artificially low because some have given up looking for work.\nThose suffering most from the success of Sanders and Trump are candidates such as John Ellis Bush. Jeb, 62, seems to have carefully studied the art of coming across as a nice guy, albeit one who is a little annoyed about having to speak to about 200 people in Coralville, Iowa. Being a Bush comes with baggage and expectations, but it does have its advantages, such as immaculate event advance work and plenty of press.\n\n\n\n\n\nPhoto of Jeb! at Coralville, Iowa, by Monica Alexander.\n\n\n\n\nJeb emphasised his commander-in-chief credentials. His older brother’s war in Iraq may have ensured that the age of aggressive American imperialism is over for now, but Jeb was still introduced by a retired Admiral who spoke of Jeb’s leadership fighting hurricanes in Florida and neighbouring states.\n\n\n\n\n\nPhoto of Jeb! at Coralville, Iowa, by Monica Alexander.\n\n\n\n\nThe crowd was lively and Jeb was frequently interrupted by cheers. He spoke without notes for about 30 minutes to a crowd surrounding him on all four sides. Maybe this is a plan to seem more approachable, or to encourage audience participation? An hour of questions, courtesy of roaming microphones, followed. Jeb worked hard to be nice, but it was apparent that he was, indeed, working at it, because he occasionally lapsed and gently made fun of a questioner.\nFor all the well-acknowledged flaws of presidential systems in general, and the US one specifically, it has much to be proud of. Every candidate for their party’s nomination will visit Iowa at some stage this week, many visiting multiple towns in a day. The scrutiny is intense. Iowa is a state of 3 million people, divided into 99 counties, and by 1 February, many candidates will have visited every county.\n\n\n\n\n\nPhoto in Iowa, by Monica Alexander.\n\n\n\n\nIf one were starting from scratch, the Iowa Caucuses would probably not be the way to go. Iowa is not representative of the rest of the United States, and its outsized electoral importance skews national policy. But in their own way, the Iowa Caucuses are valuable. In general, the nature of the candidates seems to come through in the events because they are so intimate. Given that so much of leadership consists of reacting to unexpected events, and the difficulty that politicians tend to have implementing their desired policies, perhaps this is the most important consideration. It may be that, to rework Churchill’s aphorism, the Iowa Caucuses are the worst way to select presidential candidates, apart from all the others.\n\n\n\n\n\nPhoto in Iowa, by Monica Alexander.\n\n\n\n\nThere are many aspects of the United States that should not be considered, let alone encouraged, in Australia. But the rigorous examination of candidates facilitated by the Iowa Caucuses is something Australia should emulate."
  },
  {
    "objectID": "posts/2016-07-17-trump-revisited/trump-revisited.html",
    "href": "posts/2016-07-17-trump-revisited/trump-revisited.html",
    "title": "Trump, Revisited",
    "section": "",
    "text": "(Comment 4 July 2017: Like much of my writing about Trump, the opinions here haven’t aged too well. Nonetheless, I learnt a lot from writing this and later from thinking about why I went wrong.)\nA few notes and photos from a Trump rally in Indiana earlier this week. The focus is on whether Trump ‘could’ win the election in November. In the interest of transparency, it’s worth acknowledging that I didn’t think Trump could win the Republican nomination. Thanks to Monica for helpful edits.\nDonald Trump is an improved politician, but it’s unlikely to be enough. He has harnessed fervent anti-Clinton sentiment amongst Republicans. But he does not have time to build the coalitions usually needed to win a US presidential election.\nAt a recent rally in Westfield, Indiana, Trump was comparatively structured and measured. There was less name-calling and little of the ludicrous hypotheticals that characterized a January rally in Iowa.1 2 Some things haven’t changed: the crowd is still overwhelmingly white; ‘Storm Trumpers’ in ill-fitting suits are still on patrol; and Trump still lies.3 But he is no longer politically inexperienced, and neither were the Trump supporters that I talked to.\nThe Republican Governor of Indiana, Mike Pence, introduced Trump at the rally. Pence was later announced as Trump’s running mate. If Trump is to win the election then he needs to easily win states such as Indiana, where Republicans have only lost once in the past fifty years.4 Even though the outcome in Indiana should not be in doubt, Hoosier Republicans are important. Trump needs fired-up volunteers to travel to neighboring Ohio, a crucial swing state, and Trump needs money. But mostly, Trump needs friends.\nDespite Trump’s improvement as a politician, winning a presidential election usually requires constructing coalitions. Often this is the work of a lifetime. For instance, Richard Ben Cramer describes how, beginning in his 20s, George H. W. Bush built a Christmas card list. By the time he was Vice President 30,000 ‘friends’ received an annual Christmas card from him.5 The Clintons have been building coalitions since their 20s too. Part of Trump’s appeal is that he has only been a politician for a year, but his campaign is inefficient without coalitions.\nThe most precious resource in any election is a candidate’s time. The US presidential election magnifies this because of the scrutiny, the electoral college, and the size of the country. Yes, a candidate needs to raise money, motivate supporters, and convince undecided voters. But to stand a chance of winning, a candidate usually also needs coalitions that can do all this for them. Without these, there is more pressure on Trump.\nTrump has also only recently put modern campaign essentials in place. For instance, just a few months ago Trump described the use of data in politics as ‘overrated’.6 But he seems to have changed his mind: Trump collected and verified the phone numbers of those who attended the rally in Indiana. Texts and phone calls will be critical to the effort of getting his supporters to turn out to vote in November. Trump is also now sending emails but it takes time to build a high-quality list.\nThe Trump supporters that I spoke to were unfailingly polite. They conscientiously thanked the many law enforcement personnel, and there were many military veterans in the audience. No one supported every aspect of Trump’s platform, but this is not unusual in political campaigns. There was some anti-Muslim sentiment, and a few conspiracy theories. Political correctness was a recurrent issue, as was declining US influence in the world. Although Trump uses ‘The Wall’ as a call-and-response device (Trump: ‘Who’s going to pay for the wall?’ Crowd: ‘Mexico’), anti-Mexican sentiment seemed to be driven more by a perceived willingness of Hispanics to work for low wages than racism.\nThe Republicans I talked to were united only in being against Clinton; but similarly many Democrats seem united only in wanting to stop Trump. How such a campaign translates into votes is unclear. While there don’t seem to be many undecided voters, there are many dejected ones. It will be interesting to see turnout estimates in swing states. Would you stand in line to vote against, rather than for, a candidate?\nIn just one year Trump has changed US politics. He is quickly improving as a politician, but remains divisive. Is Trump the moment, or just of the moment? Although he probably does not have enough time to do the work that would allow him to win in November,7 the impact of his campaign will be felt for many years."
  },
  {
    "objectID": "posts/2016-07-17-trump-revisited/trump-revisited.html#footnotes",
    "href": "posts/2016-07-17-trump-revisited/trump-revisited.html#footnotes",
    "title": "Trump, Revisited",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nGo to https://www.rohanalexander.com/2016/01/14/notes-and-photos-from-iowa/ for that write-up.↩︎\nThe speech can be viewed here: https://youtu.be/ewMhP-V1ed8, as at 15 July 2016.↩︎\nSee, for instance, Politifact (http://www.politifact.com/personalities/donald-trump/) which awarded Trump PolitiFact’s 2015 Lie of the Year and rules 58 per cent of his statements as either ‘False’ or ‘Pants on Fire’.↩︎\nSee the entry for Indiana here: https://en.wikipedia.org/wiki/List_of_United_States_presidential_election_results_by_state, as at 15 uly 2016.↩︎\nSee Richard Ben Cramer’s ‘What It Takes’, page 153 of the Vintage; Reprint edition (June 1, 1993).↩︎\nSee: http://bigstory.ap.org/article/6d588a38061c4657a557d1dde86782ec/trumps-questioning-value-data-worries-republicans, accessed 15 July 2016.↩︎\nIt is estimated that Trump currently has a 20-40 per cent chance of winning the election. For instance, see the Five Thirty Eight election forecast (http://projects.fivethirtyeight.com/2016-election-forecast/), as at 15 July 2016 or the New York Times summary of election polls (http://www.nytimes.com/interactive/2016/us/elections/polls.html), again as at 15 July 2016.↩︎"
  },
  {
    "objectID": "teaching-inf312.html",
    "href": "teaching-inf312.html",
    "title": "Worlds become data",
    "section": "",
    "text": "To a certain extent we are wasting our time. We have a perfect model of the world—it is the world! But it is too complicated. Because of this we must simplify the world in order for it to become data. In this course we explore how we do this, and the implications.\n\n\n\n\nCan I audit this course? Sure, but it is pointless, because the only way to learn this stuff is to do the work.\nWhat is a tutorial? You write a short paper. Then you submit it. The next day, during class, we’ll discuss it.\nWhy is there so much assessment? The only way to learn this stuff is to actually do the work, and students only do the work when they are assessed. It is unfortunate, but there is no way around it.\nHow difficult is the course? The course is not difficult, but the hands-on-projects mean it is a lot of work.\nWhat is the format of the class? There are rarely old-school lectures because those are not effective. You should read the relevant chapter before class. During class we will focus on tutorials and discussion. We will also have industry guests discuss their experience.\n\n\n\n\n\n2024\n\nSyllabus\nStudent evals\n\n2023\n\nSyllabus\nStudent evals\n\n2022\n\nSyllabus\nStudent evals\n\n\n\n\n\n\nNone.\n\n\n\n\nTelling Stories with Data"
  },
  {
    "objectID": "teaching-inf312.html#preamble",
    "href": "teaching-inf312.html#preamble",
    "title": "Worlds become data",
    "section": "",
    "text": "To a certain extent we are wasting our time. We have a perfect model of the world—it is the world! But it is too complicated. Because of this we must simplify the world in order for it to become data. In this course we explore how we do this, and the implications.\n\n\n\n\nCan I audit this course? Sure, but it is pointless, because the only way to learn this stuff is to do the work.\nWhat is a tutorial? You write a short paper. Then you submit it. The next day, during class, we’ll discuss it.\nWhy is there so much assessment? The only way to learn this stuff is to actually do the work, and students only do the work when they are assessed. It is unfortunate, but there is no way around it.\nHow difficult is the course? The course is not difficult, but the hands-on-projects mean it is a lot of work.\nWhat is the format of the class? There are rarely old-school lectures because those are not effective. You should read the relevant chapter before class. During class we will focus on tutorials and discussion. We will also have industry guests discuss their experience.\n\n\n\n\n\n2024\n\nSyllabus\nStudent evals\n\n2023\n\nSyllabus\nStudent evals\n\n2022\n\nSyllabus\nStudent evals\n\n\n\n\n\n\nNone.\n\n\n\n\nTelling Stories with Data"
  },
  {
    "objectID": "teaching-inf312.html#content",
    "href": "teaching-inf312.html#content",
    "title": "Worlds become data",
    "section": "Content",
    "text": "Content\n\nWeek 1\n\nDrinking from a fire hose\n\n\n\nWeek 2\n\nReproducible workflows\nGuest: Samita Prabhasavat\n\n\n\nWeek 3\n\nR essentials\n\n\n\nWeek 4\n\nWriting research\nGuest: Steven Coyne - “Who Owns This? The Ethics of Copyright”\n\n\n\nWeek 5\n\nStatic communication\n\n\n\nWeek 6\n\nFarm data\n\n\n\nWeek 7\n\nGather data\n\n\n\nWeek 8\n\nHunt data\n\n\n\nWeek 9\n\nClean and prepare\n\n\n\nWeek 10\n\nStore and share\n\n\n\nWeek 11\n\nExploratory data analysis\n\n\n\nWeek 12\n\nLinear models"
  },
  {
    "objectID": "teaching-inf312.html#assessment",
    "href": "teaching-inf312.html#assessment",
    "title": "Worlds become data",
    "section": "Assessment",
    "text": "Assessment\n\nSummary\n\n\n\nItem\nWeight (%)\nDue date\nNotes\n\n\n\n\nQuiz\n7\nWednesdays, noon, Weeks 1-12\nOnly best seven out of twelve count.\n\n\nSQL quiz\n1\nWednesday, noon, Week 6\n\n\n\nPersonal website\n1\nWednesday, noon, Week 9\nCreate a personal website using Quarto and make it live via GitHub Pages. At a minimum, it must include a bio and a CV in PDF form.\n\n\nTutorials\n6\nWednesdays, noon, Weeks 1-12\nOnly best three out of twelve count.\n\n\nTerm papers\n48\nWednesdays, noon, Weeks 3, 6, 9\nTerm Paper I: 24 January 2024\nTerm Paper II: 14 February 2024\nTerm Paper III: 13 March 2024\nYou must submit Term Paper I in order to pass the course.\nOnly best two of three term papers count.\nMarking starts, noon, on the Friday after submission, and you can update until then i.e. submissions made by noon, Wednesday, Week 3 can be updated until noon, Friday, Week 3 (this is to allow you to incorporate peer review comments). Please do not make any changes after marking starts.\nTerm Paper I: Donaldson Paper |\nTerm Paper II: Mawson Paper |\nTerm paper III: Howrah Paper |\n\n\nConduct peer review of Term/Final papers\n3\nThursdays, noon, Weeks 3, 6, 9, 12\nConduct peer review for six other term/final papers, by creating a GitHub Issue or Pull Request. Papers will be distributed by a spreadsheet—add a link to the Issue/PR to a term paper that does not have four other entries. You will only have 24 hours to do this.\n\n\nFinal paper\n34\nWednesday, noon, Week 12 (3 April 2024)\nYou must submit this paper.\nMarking starts, noon, Monday 22 April and you can update until then i.e. submissions made by noon, Wednesday, Week 12 can be updated until noon, Monday, 22 April (this is to allow you to incorporate peer review comments). Please do not make any changes after marking starts. |\nFinal paper.\n\n\n\nYou must submit Term Paper 1. You must submit the Final Paper.\nBeyond that, you have scope to pick an assessment schedule that works for you. I will take your best three of the twelve tutorials for that six per cent, and your best seven of twelve quizzes for that seven per cent. I take your two best papers from the three term papers for that 48 per cent (24 per cent for each). You get up to three percentage points for conducting peer review of other student papers, (half a percentage point per review). There is 34 per cent allocated for the Final Paper.\nAdditional details:\n\nQuiz questions are drawn from those in the Quiz section that follows each chapter of Telling Stories with Data. Some of them are multiple choice, and you should expect to know the mark within a few days of submission. Please do them before coming to class.\nTutorial questions are drawn from those in the Tutorial section that follows each chapter of Telling Stories with Data. The general expectation (although this differs from week to week) is about two pages of written content. You should expect to know the mark within a few days of the tutorial.\nIn general term papers require a considerable amount of work, and are due after the material has been covered in quizzes and tutorials (i.e. you would draw on knowledge tested in the quizzes, and potentially material could be re-used from the tutorial material). In general, they require original work to some extent. Papers are taken from the Papers appendix of Telling Stories with Data and students have access to the grading rubrics before submission.\nIf you already have a website, please communicate with me about this early in the term so that I can let you know whether it can be used for the purposes of this submission.\nWhile they vary, a rough rubric for tutorial is:\n\n0 - Any typos, grammatical errors, other table stakes issues for this level. Submission is too short. Other basic mistakes.\n0.25 - Tables/graphs not properly labeled, no references, other aspects that affect credibility.\n0.5 - Makes some interesting and relevant points, related to course material (including required materials), but lacking in terms of structure and story/argument.\n0.80 - Interesting submission that is well-structured, coherent, and credible.\n1 - As with 0.80, but exceptional in some way.\n\nOnly the best two of three term papers counts. This means each is worth 24 per cent."
  },
  {
    "objectID": "teaching-inf312.html#other",
    "href": "teaching-inf312.html#other",
    "title": "Worlds become data",
    "section": "Other",
    "text": "Other\n\nChildren in the classroom\nBabies (bottle-feeding, nursing, etc) are welcome in class as often as necessary. You are welcome to take breaks to feed them or express milk as needed, either in the classroom or elsewhere including here. A list of baby change stations is also available here. Please communicate with me so that I can make sure that we have regular breaks to accommodate this.\nFor toddlers and older children, I understand that unexpected disruptions in childcare/school can happen. You are welcome to bring your child to class in order to cover unforeseen gaps.\n\n\nAccommodations with regard to assessment\nPlease do not reveal your personal or medical information to me. I understand that illness or personal emergencies can happen from time to time. The following accommodations to assessment requirements exist to provide for those situations.\nStraight-forward (will automatically apply to all students—there is no need to ask for these):\n\nQuiz: Only your best seven quizzes count.\nTutorial: Only your best three tutorials count.\nTerm Papers: Only your best two term papers count.\n\nSo for those, if you have a situation, then just do not submit (or in the case of Term Paper I, just submit a blank page).\nSlightly more involved:\n\nTerm Paper I: You must submit something for Term Paper I, even if it gets zero. If you have a medical emergency that makes it impossible for you to submit something, then please email me. In that situation one of the remaining term papers must be done individually to ensure fairness with the rest of the class.\nPeer review: No accommodation or late submission is possible for this because it would hold up the rest of the class. That said, there are many opportunities to get the peer review marks, so if you cannot do any for a particular paper, then just do the others. If you have a medical emergency that makes this impossible, then please email me and cc your faculty/department/college advisor so that we can work out an alternative plan.\nFinal paper: The final paper is a critical piece of assessment. It is also up against deadlines for submission of grades (especially for graduating students). If you have a medical emergency that makes it impossible for you to submit before marking begins, then I may be able to grant you an extension of up to three days. Email me and cc your faculty/department/college advisor so that we can work out a alternative plan.\n\n\n\nRe-grading\nMarking mistakes happen and I want to correct those. Requests to have your work re-graded will not be accepted within 24 hours of the release of grades. This is to give you a chance to reflect. Similarly, requests to have your work re-graded more than seven days after the release of the grades will not be accepted. This is to ensure the course runs smoothly.\nInside that 1-7 day period if you would like to request a re-grade, please email rohan.alexander@utoronto.ca and use the subject line “INF312: re-grade request”. Please specify where the marking mistake was made in relation to the marking guide. The entire assessment will be re-marked and it is possible that your grade could reduce.\nPlenty of students get 0 on the first paper, but go on to get an A+ overall in the course. The nature of the work in this course requires students to adjust from what is expected in other courses, and the forgiving assessment weighting is designed to allow this.\n\n\nPlagiarism and integrity\nPlease do not plagiarize. In particular, be careful to acknowledge the source of code—if it is extensive then through proper citation and if it is just a couple of lines from Stack Overflow then in a comment immediately next to the code.\nYou are responsible for knowing the content of the University of Toronto’s Code of Behaviour on Academic Matters.\nAcademic offenses include (but are not limited to) plagiarism, cheating, copying code without acknowledgement, purchasing labor for assessments (of any kind). Academic offenses will be taken seriously and dealt with accordingly. If you have any questions about what is or is not permitted in this course, please just email me.\nPlease consult the University’s site on Academic Integrity. Please also see the definition of plagiarism in section B.I.1.(d) of the University’s Code of Behaviour on Academic Matters available here. Please read the Code. Please review Cite it Right and if you require further clarification, consult the site How Not to Plagiarize.\n\n\nLate policy\nIf no extension has been granted and no accommodation applies, then late submissions will not be accepted.\n\n\nWriting\nPapers and reports should be well-written, well-organized, and easy to follow. They should flow easily from one point to the next. They should have proper sentence structure, spelling, vocabulary, and grammar. Each point should be articulated clearly and completely without being overly verbose. Papers should demonstrate your understanding of the topics you are studying in the course and your confidence in using the terms, techniques and issues you have learned. As always, references must be properly included and cited. If you have concerns about your ability to do any of this then please make use of the writing support provided to the faculty, colleges and the SGS Graduate Centre for Academic Communication.\n\n\nMinimum submission requirement\nIf you are going to not be able to submit at least two term papers, and/or be unable to submit the final paper then it would be unfair on the other students to allow you to pass the course. But it is not a situation that I want to get into. Please ensure you and your college registrar or faculty/department advisor get in touch with me as early as possible if this may be the case for you so that we can work out a solution.\n\n\nLetters of recommendation\nI am happy to write letters for students who get both an A+ overall and an A+ in the final paper. This allows me to write a strong letter. If you are in this position after the class ends and want me to write a letter, please send me a request early.\n\n\nUse of Generative AI in assignments\nIn general, students are encouraged to use generative AI tools as a starting point. Specific course policies are:\n\nCode: Students may wish to use generative AI tools to aid in initial development and writing of code to answer assignment questions and carry out the research project analysis. If this is the case, the use of such tools should be explicitly acknowledged in the submitted work, and the relevant prompts and responses should be included in a text file in the repo. All code, regardless of how it is generated must be thoroughly commented and explained. Failure to do so may result in penalties.\nWritten work: Using generative AI tools to generate written answers to assignment questions of text contained in the final research project is prohibited in this course. Representing as one’s own an idea, or expression of an idea, that was AI-generated may be considered an academic offense in this course. However, you are welcome to use it to generate a first draft, which you then completely edit. Again, if you use such tools, it should be explicitly acknowledged in the submitted work, and the relevant prompts and responses should be included in a text file in the repo.\n\nThis course policy is designed to promote your learning and intellectual development and to help you reach course learning outcomes."
  },
  {
    "objectID": "students.html",
    "href": "students.html",
    "title": "Students",
    "section": "",
    "text": "I am fortunate to work with many terrific students, including:\n\nCiara Zogheib is a PhD candidate in the Faculty of Information at the University of Toronto."
  },
  {
    "objectID": "students.html#current",
    "href": "students.html#current",
    "title": "Students",
    "section": "",
    "text": "I am fortunate to work with many terrific students, including:\n\nCiara Zogheib is a PhD candidate in the Faculty of Information at the University of Toronto."
  },
  {
    "objectID": "students.html#past",
    "href": "students.html#past",
    "title": "Students",
    "section": "Past",
    "text": "Past\nIn the past I have worked closely with many students, including (in rough reverse chronological order):\n\nLindsay Katz graduated with a Masters in Statistics from the Department of Statistical Sciences at the University of Toronto. We worked together for a year, and she completed a variety of projects, most notably developing a tested dataset of what was said in the Australian parliament.\nAnnie Collins graduated from the University of Toronto with an undergraduate degree specializing in applied mathematics and statistics with a minor in history and philosophy of science. We worked together for about eighteen months during which she worked on many projects including: ‘An Introduction to DoSStoolkit’ and ‘Reproducibility of COVID-19 pre-prints’. Her first job after graduation was as a data scientist at Giving Tuesday.\nCallie Moore graduated from the University of Toronto with an undergraduate degree in engineering science. I supervised Callie’s undergraduate thesis entitled ““. Her first job after graduation was as a developer at the Investigative Journalism Foundation.\nA Mahfouz worked with me as a Master of Information student at the University of Toronto with a background in geography. Their prior work has been largely concerned with data pipelines. A contributed to many projects including heapsofpapers.\nKe-Li Chiu worked with me as a Master of Information student at the University of Toronto interested in natural language processing. Ke-Li worked on: ‘On consistency scores in text data with an implementation in R’ and ‘Detecting Hate Speech with GPT-3’.\nPaul Hodgetts worked with me as a Master of Information student at the University of Toronto. Paul put together cesR, which is an R package that makes it easier to gather and use the Canadian Election Study surveys from 1965 through to 2019, described here: https://osf.io/preprints/socarxiv/a29h8/. Paul additionally put together a fantastic logo. Paul’s first job works as a data scientist at Kingston."
  },
  {
    "objectID": "students.html#future",
    "href": "students.html#future",
    "title": "Students",
    "section": "Future",
    "text": "Future\n\nCurrent Toronto undergrad or masters students\nIf you are a Toronto undergrad or masters student and would like to work with me, then the best way is to do well in a course that I teach and then get in touch. That said, I do advertise for positions from time to time and you should also keep an eye out for those. Impressive applicants:\n\nHave GitHub repos that show off their best work.\nCan write well.\nSupport the claims they make in a cover letter with evidence.\n\n\n\nCurrent Toronto PhD students\nI am open to supervising PhD students. If you are already admitted, then please email me.\n\n\nProspective PhD students\nIf you are not yet a Toronto PhD student:\n\nHaving gone through it myself, I do understand that grad school applications are stressful and there is a lot of conflicting advice.\nWhile I understand that practices differ across disciplines and universities, in my case, there is no point contacting me before you apply.\nI am open to supervising PhD students, but can only help once you are admitted. Admission decisions are made at a department/faculty level, before they are made at an individual level, and there is little I can do to help you until you are through that first hurdle.\nAgain, the first step is to apply to a PhD program. In my case that means either Information or Statistical Sciences.\nAs part of the application you will have the option to list faculty with whom you would like to work. You should include my name in that list so that your application is routed to me.\n\n\n\nReference letters\nI only provide reference letters to students that:\n\nwork with me; or\nget an A+ in a course that I taught.\n\nIf you are in either of those categories, then I would be happen to write you reference letters. Please send me an email and I will send you a form to fill out with the details that I need from you."
  },
  {
    "objectID": "events-llms_colloquium.html",
    "href": "events-llms_colloquium.html",
    "title": "Colloquium on Applications of LLMs",
    "section": "",
    "text": "The Data Sciences Institute is hosting a colloquium on applications of LLMs for faculty and students. This is an opportunity for researchers from different disciplines to exchange initial thoughts, experiences, and ideas, in a casual and informal setting, so that we can all better integrate LLMs into our research.\nThe event is happening on Tuesday, 11 April, 2023, at the Department of Statistical Sciences, University of Toronto.\nIn-person attendance is encouraged, but there will be a Zoom option. To promote an open and honest exchange of preliminary ideas and impressions we won’t record the event."
  },
  {
    "objectID": "events-llms_colloquium.html#overview",
    "href": "events-llms_colloquium.html#overview",
    "title": "Colloquium on Applications of LLMs",
    "section": "",
    "text": "The Data Sciences Institute is hosting a colloquium on applications of LLMs for faculty and students. This is an opportunity for researchers from different disciplines to exchange initial thoughts, experiences, and ideas, in a casual and informal setting, so that we can all better integrate LLMs into our research.\nThe event is happening on Tuesday, 11 April, 2023, at the Department of Statistical Sciences, University of Toronto.\nIn-person attendance is encouraged, but there will be a Zoom option. To promote an open and honest exchange of preliminary ideas and impressions we won’t record the event."
  },
  {
    "objectID": "events-llms_colloquium.html#schedule",
    "href": "events-llms_colloquium.html#schedule",
    "title": "Colloquium on Applications of LLMs",
    "section": "Schedule",
    "text": "Schedule\n\n\n\n\n\n\n\n\n\nTime\nEvent\nSpeaker\nPerspective or topic\n\n\n\n\n09:00\nCoffee\n-\n-\n\n\n09:10\nWelcome\nMike Evans\nStatistical Sciences\n\n\n09:15\nInvited talk\nBree McEwan\nCommunication\n\n\n09:30\nInvited talk\nBenjamin Haibe-Kains\nComputational pharmacogenomics\n\n\n09:45\nInvited talk\nJason Hattrick-Simpers\nMaterials Science and Engineering\n\n\n10:00\nInvited talk\nMarcel Fortin\nMap and Data Library\n\n\n10:15\nInvited talk\nMichael Hoffman\nComputational genomics\n\n\n10:30\nMorning tea\n-\n-\n\n\n10:45\nInvited talk\nJoshua Speagle, Kristen Menou, and Jo Bovy\nAstrostatistics and Data Science; Physics and Astrophysics; Galactic Astrophysics\n\n\n11:15\nInvited talk\nJonathan Rose\nElectrical and Computer Engineering\n\n\n11:30\nIndustry presentation\nEdward Kim\nCohere\n\n\n11:45\nInvited talk\nWendy Wong\nPolitical science\n\n\n12:00\nMasterclass\nJinyue Feng and Annie En-Shiun Lee\nComputer Science. “An overview of how LLMs work”\n\n\n12:45\nLunch break\n-\n-\n\n\n13:30\nHands-on activity - Part I\n-\nCode your own GPT following https://youtu.be/kCc8FmEb1nY\n\n\n14:30\nAfternoon tea\n-\n-\n\n\n14:45\nHands-on activity - Part II\n-\nCode your own GPT following https://youtu.be/kCc8FmEb1nY\n\n\n16:00\nFinish\n-\n-"
  },
  {
    "objectID": "events-llms_colloquium.html#contact",
    "href": "events-llms_colloquium.html#contact",
    "title": "Colloquium on Applications of LLMs",
    "section": "Contact",
    "text": "Contact\nFor any questions or comments, please contact Rohan Alexander: rohan.alexander@utoronto.ca."
  },
  {
    "objectID": "events-tdw.html",
    "href": "events-tdw.html",
    "title": "Toronto Data Workshop",
    "section": "",
    "text": "The Toronto Data Workshop brings together academia and industry to share data science and AI best practice. We are broadly interested, but especially in the code- and data-centric aspects of a project that are often glossed over. We meet weekly for an hour and most talks are recorded.\nIf you would like to attend, please sign up here. Everyone is welcome—it is free and you do not need to be affiliated with the university.\nThe current organizing committee is: Kelly Lyons, Michaela Drouillard, and Rohan Alexander. Past committee members: Amy Farrow (2021-22), Lorena Almaraz De La Garza (2021-22), and Faria Khandaker (2020-21)."
  },
  {
    "objectID": "events-tdw.html#overview",
    "href": "events-tdw.html#overview",
    "title": "Toronto Data Workshop",
    "section": "",
    "text": "The Toronto Data Workshop brings together academia and industry to share data science and AI best practice. We are broadly interested, but especially in the code- and data-centric aspects of a project that are often glossed over. We meet weekly for an hour and most talks are recorded.\nIf you would like to attend, please sign up here. Everyone is welcome—it is free and you do not need to be affiliated with the university.\nThe current organizing committee is: Kelly Lyons, Michaela Drouillard, and Rohan Alexander. Past committee members: Amy Farrow (2021-22), Lorena Almaraz De La Garza (2021-22), and Faria Khandaker (2020-21)."
  },
  {
    "objectID": "events-tdw.html#section",
    "href": "events-tdw.html#section",
    "title": "Toronto Data Workshop",
    "section": "2024",
    "text": "2024\n\nUpcoming\n\nWednesday 26 June 2024, 1pm (EDT)\n\nMatthew Gentzkow, Stanford University\n“The effects of Facebook and Instagram on the 2020 election: A deactivation experiment”\nThe talk will be based on this paper which studies the effect of Facebook and Instagram access on political beliefs, attitudes, and behavior by randomizing a subset of 19,857 Facebook users and 15,585 Instagram users to deactivate their accounts for 6 weeks before the 2020 U.S. election.\nMatthew Gentzkow is the Landau Professor of Technology and the Economy at Stanford University. He studies applied microeconomics with a focus on media and technology industries. He received the 2014 John Bates Clark Medal, given by the American Economic Association to the American economist under the age of forty who has made the most significant contribution to economic thought and knowledge. He is a member of the National Academy of Sciences, a fellow of the American Academy of Arts and Sciences, a fellow of the Econometric Society, a senior fellow at the Stanford Institute for Economic Policy Research, and the Editor of American Economic Review: Insights.\n\nFriday 28 June 2024, noon (EDT)\n\nSasha Issenberg\n“The Lie Detectives: In Search of a Playbook for Winning Elections in the Disinformation Age”\nSasha Issenberg is a journalist and author of “The Lie Detectives: In Search of a Playbook for Winning Elections in the Disinformation Age” and four previous books, most recently “The Engagement: America’s Quarter-Century Struggle Over Same-Sex Marriage.” He teaches in the UCLA Department of Political Science and is a correspondent for Monocle. His work has also appeared in New York, The New York Times Magazine and George, where he was a contributing editor.\n\nFriday 5 July 2024, noon (EDT)\n\nKobi Hackenburg, Oxford Internet Institute, University of Oxford\n“Evaluating the persuasive influence of political microtargeting with large language models”\nAdvances in LLMs have raised concerns over scalable, personalized political persuasion. In this talk, based on a paper recently published in PNAS, we integrate user data into GPT-4 prompts in real-time, facilitating the live creation of messages tailored to persuade individual users on political issues. We then deploy this application at scale to test whether personalized, microtargeted messaging offers a persuasive advantage compared to nontargeted messaging. We find that while messages generated by GPT-4 were persuasive, in aggregate, the persuasive impact of microtargeted messages was not statistically different from that of nontargeted messages. These findings suggest—contrary to widespread speculation—that the influence of current LLMs may reside not in their ability to tailor messages to individuals but rather in the persuasiveness of their generic, nontargeted messages.\nKobi Hackenburg is a PhD candidate in Social Data Science at the Oxford Internet Institute, University of Oxford. His doctoral research, funded by a Clarendon Scholarship and supervised by Helen Margetts and Scott Hale, investigates the persuasive influence of personalized AI systems. More broadly, his work lies at the intersection of computation, language, and society. Alongside his PhD, he works as a Doctoral Researcher in the Public Policy Programme at The Alan Turing Institute, the UK’s national institute for AI and data science.\n\nThursday 11 July 2024, 9am (EDT)\n\nTerry Yue Zhuo, Monash University\n“BigCodeBench: Benchmarking Code Generation with Diverse Function Calls and Complex Instructions”\nIn this talk we introduce BigCodeBench, a benchmark that challenges LLMs to invoke multiple function calls as tools from 139 libraries and 7 domains for 1,140 fine-grained programming tasks. Our evaluation of 60 LLMs shows that LLMs are not yet capable of following complex instructions to use function calls precisely, with scores up to 60%, significantly lower than the human performance of 97%.\nTerry Yue Zhuo is a PhD candidate in Computer Science at Monash University and the CSIRO’s Data61. He holds a Bachelor of Computer Science (Honours) from Monash University. He is additionally an associate member of the Sea AI Lab, a visiting scholar at Singapore Management University, and a research technician at CSIRO’s Data61. His research has been published at venues including EMNLP, ICLR, EACL, and TMLR.\n\nFriday 12 July 2024, noon (EDT)\n\nNaman Jain, UC Berkeley\n“LiveCodeBench: Holistic and contamination free evaluation of large language models for code”\nIn this talk we introduce LiveCodeBench, a comprehensive and contamination-free benchmark for LLMs in code, which continuously collects new problems from LeetCode, AtCoder, and CodeForces. LiveCodeBench evaluates a wide range of capabilities, including self-repair, code execution, and test output prediction. It currently hosts 400 coding problems published between May 2023 and May 2024. We evaluated 18 base LLMs and 34 instruction-tuned LLMs, presenting findings on contamination, performance comparisons, and potential overfitting.\nNaman Jain is a CS Ph.D. student at UC Berkeley, focusing on using machine learning to enhance developer productivity tools like program analysis, synthesis, and repair. He also explores how synthesis and verification can improve algorithm generalizability and explainability. He holds an undergraduate degree from IIT Bombay, where he researched NLP robustness and computer vision. Before his Ph.D., he was a predoctoral research fellow at Microsoft Research India, working on program repair, improving large language models, and learning decision trees.\n\nThursday 18 July 2024, noon (EDT)\n\nHannah Rose Kirk, Oxford Internet Institute, University of Oxford\n“THE PRISM Alignment Project”\nThe PRISM Alignment Project collects disaggregated feedback from a diverse collection of 1,500 participants from 75 countries based on how they converse with over 20 LLMs in real-time. This addresses the concern that human feedback learning in AI systems was collected from a narrow and unrepresentative set of crowdworkers. Details are available in the pre-print or the project page.\nHannah Rose Kirk is a DPhil candidate in Social Data Science at the Oxford Internet Institute, University of Oxford, with a research focus on the alignment of LLMs through granular and diverse human feedback. Her research has been published in venues including Nature Machine Intelligence, NAACL, EMNLP, and NeurIPs. She has previously worked with researchers at NYU, Google, OpenAI, and The Alan Turing Institute.\n\nFriday 19 July 2024, noon (EDT) (in-person and online)\n\nMajeed Kazemitabaar, University of Toronto\n“Improving Steering and Verification in AI-Assisted Data Analysis”\nMajeed Kazemitabaar is a fourth-year Ph.D. student in Computer Science at the University of Toronto, working with Professor Tovi Grossman. His research in Human-Computer Interaction and Computer Science Education focuses on creating engaging computational learning experiences and has been published in CHI, UIST, and IDC, and earned Best Paper and Best Late-Breaking Work awards.\n\n\n\n\nFriday 6 September 2024, noon (EDT)\n\nTBA\n\nFriday 13 September 2024, noon (EDT)\n\nCaroline Weis, gsk.ai\nCaroline Weis is a Senior AI/ML Engineer and team lead at gsk.ai. In 2021, she completed her PhD in Machine Learning for Computational Biology and Healthcare at ETH Zurich. Her research interests lie in the development of personalized healthcare through data analysis and machine learning on medical and biological data.\n\nFriday 20 September 2024, noon (EDT)\n\nTBA\n\nFriday 27 September 2024, noon (EDT)\n\nAnnie Collins, GivingTuesday\nAnnie Collins is a Data Scientist at GivingTuesday, a US-based nonprofit focused on researching generosity and charitable giving behaviours. Beyond GivingTuesday, Annie has spent several years in data management and research roles within the Canadian nonprofit sector. She holds a Bachelors of Science in applied mathematics and statistics from the University of Toronto, and uses her experience to provide data for the public good and support a more data-driven social sector worldwide.\n\nFriday 4 October 2024, noon (EDT)\n\nSean Taylor, Motif\nSean Taylor is a data scientist, social scientist, statistician, and software developer. He mostly specializes in methods for solving causal inference and business decision problems, and is particularly interested in building tools for practitioners working on real-world problems. He is a co-founder and chief scientist at Motif.\n\nFriday 11 October 2024, noon (EDT)\n\nRachel Glennerster, University of Chicago\n“Combining both randomized and nonrandomized variation in one estimation”\nRachel Glennerster is an Associate Professor of Economics in the Division of Social Science at the University of Chicago. She uses randomized trials to study democracy and accountability, health, education, microfinance, and women’s empowerment mainly in West Africa and South Asia. She has also written on strategies to stimulate innovation, promoting more equitable access to vaccines, and the response to Ebola and COVID-19 pandemics.\n\nFriday 18 October 2024, noon (EDT)\n\nXiaojun Su, Unilever\nXiaojun Su is a Machine Learning Lead, Horizon 3 Labs, Unilever where she leads cross-functional teams of data engineers, software developer, data scientists, postgraduate researchers, and 3rd party vendors to launch in-house models to drive significant ROIs. She holds a M.Sc from the University of Toronto.\n\nFriday 25 October 2024, noon (EDT)\n\nTBA\n\nFriday 1 November 2024, noon (EDT)\n\nTBA\n\nFriday 8 November 2024, noon (EST)\n\nJacob Baldwin, Pro Football Focus (PFF)\nJacob Baldwin is a Senior Data Scientist at PFF. He holds an online M.S. degree in Applied Mathematics from the University of Washington, and graduated from Clarkson University with a B.S. in Physics, a B.S. in Applied Mathematics, and a minor in Computer Science.\n\nFriday 15 November 2024, noon (EST)\n\nCrystal Lewis, Crystal Lewis Consulting LLC\nCrystal Lewis is a freelance consultant and trainer with over 10 years of experience as a data manager in the field of education research. She is also the author of the book Data Management in Large-Scale Education Research, which provides a holistic overview of how to manage research data throughout a project life cycle. It is available freely online and a hard copy is available for purchase from Taylor and Francis CRC Press.\n\nFriday 22 November 2024, noon (EST)\n\nTBA\n\nFriday 29 November 2024, noon (EST)\n\nClaire Le Barbenchon, McKinsey & Company\nClaire Le Barbenchon is an Associate at McKinsey & Company. She is passionate about turning evidence into impact. As a management consultant with a specialty in quantitative and financial analysis, she has supported large-scale transformations across multiple Fortune 500 companies in the consumer/retail sector. She has also drawn on 8 years of experience in the public, nonprofit and research sectors to support strategy development and the generation of evidence for programming across social sector clients. She holds a PhD in Public Policy and an MS in Statistical Science.\n\n\n\n\nPast\n\nFriday 14 June 2024, noon (EDT)\n\nJae Yeon Kim, SNF Agora Institute at Johns Hopkins University\n“Field experimentation in the U.S. safety net”\nJae Yeon Kim is an incoming assistant research scientist at the SNF Agora Institute at Johns Hopkins University and a research fellow at the Center for Public Leadership at Harvard Kennedy School. Previously, he worked as a senior data scientist at Code for America, where he collaborated with all levels of the U.S. government to improve access to safety net programs. He completed his PhD in Political Science from the University of California, Berkeley in 2021.\n\nFriday 7 June 2024, noon (EDT)\n\nLars Vilhuber, Cornell University\n“Privacy protection in RCTs: The challenge of privacy protection in the field”\nLars Vilhuber holds a Ph.D. in Economics from Université de Montréal, Canada, and is currently on the faculty of the Cornell University Economics Department. He has interests in labor economics, statistical disclosure limitation and data dissemination, and reproducibility and replicability in the social sciences. He is the Data Editor of the American Economic Association, and Managing Editor of the Journal of Privacy and Confidentiality.\n\nFriday 31 May 2024, noon (EDT)\n\nEthan Busby, Brigham Young University\n“AI-Enabled Persuasion Research: Experimenting with Effective Political Messaging”\nEthan Busby is an Assistant Professor of Political Science at Brigham Young University, specializing in political psychology, extremism, artificial intelligence, and computational social science. His research relies on various methods, using lab experiments, quasi-experiments, survey experiments, text-as-data, surveys, artificial intelligence, and large-language models. He studies extremism in democracies, including what extremism is, who people blame for extremism, and what encourages and discourages extremism.\n\nFriday 24 May 2024, 10am (EDT)\n\nBelinda Li, MIT\n“Eliciting Human Preferences with Language Models”\nBelinda Li a PhD candidate at MIT CSAIL, affiliated with the language & intelligence (LINGO) lab @ MIT. Her work focuses on improving the human-interpretability, reliability, and usability of language models: examining and improving representations of both (objective) world states and (subjective) human preferences in language models. She is funded by an NDSEG Fellowship and Clare Boothe Luce Graduate Fellowship. Previously, she spent a year at Facebook AI Applied Research, and before that, obtained her B.S. in Computer Science at the University of Washington.\n\nFriday 17 May 2024, noon (EDT)\n\nVictoria Angelova, Harvard University\n“Algorithmic Recommendations and Human Discretion”\nVictoria Angelova is a fourth-year PhD student in Economics at Harvard University interested in Applied Microeconomics. She received a AB in Economics from Wellesley College in 2018. Prior to starting her PhD, she was a Research Assistant at the Industrial Relations Section at Princeton University.\n\nFriday 10 May 2024, noon (EDT)\n\nAmanda Coston, Microsoft Research and Berkeley\n“Addressing validity in decision-making algorithms”\nAmanda Coston is a Postdoc at Microsoft Research in the Machine Learning and Statistics Team. In fall 2024 she will join the Department of Statistics at UC Berkeley as an Assistant Professor. Her work considers how – and when – machine learning and causal inference can improve decision-making in societally high-stakes settings. Her research addresses real-world data problems that challenge the validity, equity, and reliability of algorithmic decision support systems and data-driven policy-making. A central focus of her research is identifying when algorithms, data used for policy-making, and human decisions disproportionately impact marginalized groups. Amanda earned her PhD in Machine Learning and Public Policy at Carnegie Mellon University (CMU) where she was advised by Alexandra Chouldechova and Edward H. Kennedy. Amanda is a Rising Star in EECS, Machine Learning and Data Science, Meta Research PhD Fellow, NSF GRFP Fellow, K & L Gates Presidential Fellow in Ethics and Computational Technologies, and Tata Consultancy Services Presidential Fellow. Her work has been recognized by best paper awards and featured in The Wall Street Journal and VentureBeat.\n\nFriday 3 May 2024, noon (EDT)\n\nKosuke Imai, Harvard University\n“Does AI help humans make better decisions? A methodological framework for experimental evaluation”\nKosuke Imai is Professor in the Department of Government and the Department of Statistics at Harvard University. He is also an affiliate of the Institute for Quantitative Social Science where his office is located. Before moving to Harvard in 2018, Imai taught at Princeton University for 15 years where he was the founding director of the Program in Statistics and Machine Learning. Imai specializes in the development of statistical methods and machine learning algorithms and their applications to social science research. His areas of expertise include causal inference, computational social science, and survey methodology. Imai leads the Algorithm-Assisted Redistricting Methodology Project (ALARM) and served as an expert witness for several high-profile legislative redistricting cases. In addition, he is the author of Quantitative Social Science: An Introduction (Princeton University Press, 2017). Outside of Harvard, Imai served as the President of the Society for Political Methodology from 2017 to 2019. His current research interests include: data-driven policy learning and evaluation, causal inference with high-dimensional and unstructured treatments (e.g., texts, images, videos, and maps), fairness and racial disparity analysis, algorithmic redistricting analysis, data fusion and record linkage, census and privacy.\n\nFriday 26 April 2024, noon (EDT)\n\nAbel Brodeur, University of Ottawa\n“Mass Reproducibility and Replicability: A New Hope”\nAbel Brodeur is an Associate Professor in the Department of Economics at the University of Ottawa. He earned a Ph.D. from the Paris School of Economics in 2015, and participated in the European Doctoral Program at the London School of Economics. His research interests include applied microeconomics, with a focus on reproductions and replications in economic research. He has served as a guest editor for special issues dedicated to these topics in Economic Inquiry and Research & Politics. Brodeur is also the founder and chair of the Institute for Replication (I4R) and co-directs the Ottawa Applied Microeconomics Lab.\nThe talk will be based on a recent paper available here.\n\nThursday 4 April 2024, noon - 1pm\n\nLenny Bronner, The Washington Post\n“Election Modeling at The Washington Post”\nLenny Bronner is a data scientist at The Washington Post, specializing in elections.\n\nThursday 28 March 2024, noon - 1pm\n\nCameron Buckner, University of Houston\n“The philosophy of Large Language Models”\nCameron Buckner is an Associate Professor of Philosophy at University of Houston, and author of From Deep Learning to Rational Machines.\n\nThursday 21 March 2024, noon - 1pm\n\nLaura Plein, University of Luxembourg\n“Can LLMs demystify Bug Reports and translate them into Test Cases?”\nLaura Plein conducted this research as an intern in the TruX (Trustworthy Software Engineering) Team at the SnT, University of Luxembourg. Her research focuses on leveraging Large Language models to automate Software Engineering processes such as automatic test case generation and test input extraction in the context of automated program repair. She is currently affiliated to the Saarland University.\n\nThursday 14 March 2024, noon - 1pm\n\nTom Davidson, Rutgers University\n“Start Generating: Harnessing Generative Artificial Intelligence for Sociological Research”\nHow can generative artificial intelligence (GAI) be used for sociological research? This talk explores applications to the study of text and images across multiple domains, including computational, qualitative, and experimental research. Drawing upon recent research and stylized experiments with DALL-E and GPT-4, discuss the potential applications of text-to-text, image-to-text, and text-to-image models for sociological research. Across these areas, GAI can make advanced computational methods more efficient, flexible, and accessible. The paper also emphasizes several challenges raised by these technologies, including interpretability, transparency, reliability, reproducibility, ethics, and privacy, as well as the implications of bias and bias mitigation efforts and the trade-offs between proprietary models and open-source alternatives. When used with care, these technologies can help advance many different areas of sociological methodology, complementing and enhancing our existing toolkits. See: https://osf.io/preprints/socarxiv/u9nft.\nThomas Davidson is an Assistant Professor of Sociology at Rutgers University—New Brunswick. He specializes in using computational methods and data from social media to analyze far-right activism, populism, and hate speech. He is currently researching the relationship between ranking and recommendation algorithms and activism, the applications of generative AI to sociological research, and the social context of content moderation. His work has been published in venues including Social Forces, Mobilization, and Socius.\n\nFriday 8 March 2024, noon - 1pm\n\nJonathan Mellon, West Point\n“Using LLMs to code open-text social survey responses at scale”\nWe compare the accuracy of six LLMs using a few-shot approach, three supervised learning algorithms (SVM, DistilRoBERTa, and a neural network trained on BERT embeddings), and a second human coder on the task of categorizing “most important issue” responses from the British Election Study Internet Panel into 50 categories. For the scenario where a researcher lacks existing training data, the accuracy of the highest-performing LLM (Claude-1.3: 93.9%) neared human performance (94.7%) and exceeded the highest-performing supervised approach trained on 1000 randomly sampled cases (neural network: 93.5%). In a scenario where previous data has been labeled but a researcher wants to label novel text, the best LLM’s (Claude-1.3: 80.9%) few-shot performance is only slightly behind the human (88.6%) and exceeds the best supervised model trained on 576,000 cases (DistilRoBERTa: 77.8%). PaLM-2, Llama-2, and the SVM all performed substantially worse than the best LLMs and supervised models across all metrics and scenarios. Our results suggest that LLMs may allow for greater use of open-ended survey questions in the future. Open access paper published in Research & Politics by Jonathan Mellon, Jack Bailey, Ralph Scott, James Breckwoldt, Marta Miori, Phillip Schmedeman and replication code/data (includes all prompts/API calls/local models etc).\nJonathan Mellon is an Associate Professor at West Point’s Department of Systems Engineering and co-director of the British Election Study. His research focuses on improving measurement and causal inference in social science. He studies electoral behavior, online citizen engagement, and measuring public opinion. He holds a DPhil in Sociology from Nuffield College, University of Oxford.\n\nThursday 7 March 2024, noon - 1pm\n\nMatheus Facure, Nubank\n“Why Banking has the Coolest Stats/Data Science Problems”\nMatheus Facure has a background in Economics and is a Staff Data Scientist at Nubank. He works mostly with credit underwriting and causal inference. He is the author of Causal Inference in Python (O’Reilly) and Causal Inference for the Brave and True (Online, Open Source).\n\nFriday 1 March 2024, noon - 1pm\n\nJacob Austin, Google DeepMind\n“Resolving Code Review Comments with Machine Learning”\nCode reviews are a critical part of the software development process, taking a significant amount of the code authors’ and the code reviewers’ time. As part of this process, the reviewer inspects the proposed code and asks the author for code changes through comments written in natural language. With machine learning, we have an opportunity to automate and streamline the code-review process, e.g., by proposing code changes based on a comment’s text.\nJacob Austin is a Senior Research Engineer at Google DeepMind, working on program synthesis and large language models. He was previously an AI Resident at Google Brain, and a Research Intern at NVIDIA with Anima Anandkumar. He holds a bachelor’s degree in computer science and mathematics from Columbia University. He studied machine learning and robotics at the Columbia Creative Machines Lab. He is also a pianist who plays a lot of chamber music, and has performed at Carnegie Hall, Music Mountain, Apple Hill, and Kinhaven.\n\nThursday 29 February 2024, noon - 1pm\n\nSky CH-Wang, Columbia University\n“Do Androids Know They’re Only Dreaming of Electric Sheep?”\nSky CH-Wang is a PhD Candidate in Computer Science at Columbia University advised by Zhou Yu and Smaranda Muresan. His research primarily revolves around Natural Language Processing (NLP). He is broadly interested in the area where NLP meets Computational Social Science (CSS). Here, his research primarily revolves around three major areas: (1) revealing and designing for social difference and inequality, (2) cross-cultural NLP, and (3) mechanistic interpretability. His research is supported by a NSF Graduate Research Fellowship.\n\nThursday 22 February 2024, noon - 1pm\n\nJessica Otis, George Mason University\n“By the Numbers: Numeracy, Religion, and the Quantitative Transformation of Early Modern England”\nDr Jessica Marie Otis is Assistant Professor of History and Director of Public Projects at the Roy Rosenzweig Center for History and New Media at George Mason University. She is the author of the new book By the Numbers: Numeracy, Religion, and the Quantitative Transformation of Early Modern England published by Oxford University Press.\n\nThursday 15 February 2024, noon - 1pm\n\nRichard Iannone, Posit, PBC\n“Using Great Tables to Make Presentable Tables in Python”\nRich is a software engineer at Posit, PBC (formerly RStudio). He focuses on making useful R and (more lately) Python packages for data analysis and presentation workflows.\n\nThursday 8 February 2024, noon - 1pm\n\nAndreas Florath, Deutsche Telekom\n“LLM Interactive Optimization of Open Source Python Libraries – Case Studies and Generalization”\nAndreas Florath is a Cloud Architect at Deutsche Telekom AG, with a focus on Edge-Cloud Continuum Architecture. His tenure of nearly 25 years at the company has seen him contribute significantly to the development of innovative technology solutions, particularly through his work with the Open Grid and EU Cloud Alliances. Andreas is proficient in complex system analysis and programming languages such as C, C++, and Python, which he leverages to directly implement and test architectures, especially in the realms of cloud federation and AI/ML applications within the telecommunications industry. His expertise is further underscored by his involvement in numerous projects and his commitment to advancing the field through research and collaboration.\n\nTuesday 6 February 2024, 6:10pm - 7pm\n\nBradley Congelio, Kutztown University of Pennsylvania\n“Introduction to NFL Analytics”\nBradley Congelio is an Assistant Professor in the College of Business at Kutztown University of Pennsylvania. His main area of instruction & research is in Data Analytics and Sport Analytics. He is the author of Introduction to NFL Analytics with R, which was published by CRC Press in December 2023. His research focuses on using big data, the R programming language, and analytics to explore the impact of professional stadiums on neighboring communities. He uses the proprietary Zillow ZTRAX database as well as the U.S. Census and other forms of data to create robust, applied, and useful insight into how best to protect those livings in areas where stadiums are proposed for construction. His work in sport analytics, specifically the NFL, has been featured on numerous media outlets, including the USA Today and Sports Illustrated.\n\nThursday 1 February 2024, noon - 1pm\n\nOliver Giesecke, Stanford University\n“AI at the Frontiers of Economic Research”\nOliver Giesecke is a research fellow at the Hoover Institution at Stanford University. Giesecke works on topics related to asset pricing and public finance. His recent work studies the finances of state and local governments across the United States. This includes the capital structure of state governments, the book and market equity position of city governments, and the status quo and trend of public pension obligations. For his work on city governments’ finances, he was awarded the NASDAQ OMX Award for the best paper on asset pricing. His work on pension obligations was instrumental to shaping state legislation. In addition, Giesecke has conducted a large-scale survey that elicits the retirement plan preferences of public sector employees across the United States. He is the author of the Stanford municipal finances dashboard which provides, for the first time, credit spreads and fiscal fundamentals for many state and local governments in the United States. The dashboard has received national media coverage in The Bond Buyer. Prior to his academic career, he has worked for Germany’s Federal Agency for Financial Market Stabilization (FMSA) and as a senior quantitative finance consultant. Giesecke received a Ph.D. in finance and economics from Columbia University, a Master’s in economics from the Graduate Institute in Geneva, Switzerland, and a BA from Frankfurt University, Germany.\n\nThursday 25 January 2024, noon - 1pm\n\nKristina Gligorić, Stanford University\n“In-class Data Analysis Replications: Teaching Students while Testing Science”\nKristina Gligorić is a Postdoctoral Scholar at Stanford University Computer Science Department, advised by Dan Jurafsky at the NLP group. Previously she obtained her Ph.D. in Computer Science at EPFL, where she was advised by Robert West. Her research focuses on developing computational approaches to solve burning societal issues, understand and improve human well-being, and promote social good. She leverages large-scale textual data and digital behavioral traces and tailors computational methods drawn from AI, NLP, and causal inference. Her work has been published in top computer science conferences (such as ACM CSCW, AAAI ICWSM, and TheWebConf) and broad audience journals (Nature Communications and Nature Medicine). She is a Swiss National Science Foundation Fellow and University of Chicago Rising star in Data Science. She received awards for her work, including EPFL Thesis Distinction, CSCW 2021 Best Paper Honorable Mention Award, ICWSM 2021 and 2023 Best Reviewer Award, and EPFL Best Teaching Assistant Award.\n\nThursday 18 January 2024, noon - 1pm\n\nGregory Zuckerman, Wall Street Journal\n“Renaissance Technologies, data, and Wall Street”\nGregory Zuckerman is a special writer at The Wall Street Journal and non-fiction author. His non-fiction books include “The Greatest Trade Ever: The Behind-the-Scenes Story of How John Paulson Defied Wall Street and Made Financial History”, “The Frackers: The Outrageous Inside Story of the New Billionaire Wildcatters”, two books for children, “The Man Who Solved the Market: How Jim Simons Launched the Quant Revolution” and “A Shot to Save the World: The Inside Story of the Life-or-Death Race for a COVID-19 Vaccine”. He has been awarded the Gerald Loeb Award, the highest honor in business journalism, three times."
  },
  {
    "objectID": "events-tdw.html#section-1",
    "href": "events-tdw.html#section-1",
    "title": "Toronto Data Workshop",
    "section": "2023",
    "text": "2023\n\nFriday 24 November 2023, noon - 1pm\n\nWendy Foster, Shopify\n“Socio-technical processes for data integrity”\n\nWendy Foster is Director of Engineering and Data, Optimize, at Shopify.\n\nFriday 17 November 2023, 1pm - 2pm\n\nChiara Alcantara, University of Waterloo\nFranklin Ramirez, University of Waterloo\nHelena Xu, University of Waterloo\n“Exploring Alternatives to REST for Accessing Public Data Sets”\nIn the past decade, governments, non-profits, and other civic institutions have increasingly been sharing data on a wide range of topics, supporting various sectors of society from private enterprise to academic research. However, using this data in a programmatic and reproducible fashion is hampered by the myriad nature of the APIs provided to access it. In this talk, we explore one potential alternative to the predominant REST paradigm: storing tabular data in the open Parquet file format on publicly accessible endpoints. By using this standardized file format and a simple backing infrastructure, new tools based on Apache Arrow, such as DuckDB and Polars, can be used to build reusable front ends and other cost-effective solutions. We present a case study based on Statistics Canada’s library of public data sets, showcasing how such a system can simplify initial selection and filtering through common SQL queries.\nChiara, Franklin, and Helena are second-year computer science students at the University of Waterloo who are interested in applying new software and data engineering tooling to increase the accessibility of public data sets.\n\nFriday 10 November 2023, noon - 1pm\n\nAlexander Coppock, Yale University\n“Research design in the social sciences”\nAlex Coppock is an Associate Professor (on term) of Political Science at Yale University. He is the author of Persuasion in Parallel: How Information Changes Minds about Politics and a member of the DeclareDesign team. He is a co-author of the recently published book Research Design in the Social Sciences: Declaration, Diagnosis, and Redesign.\n\nFriday 3 November 2023, noon - 1pm\n\nApoorva Lal, Netflix\n“Modern balancing methods for causal inference”\nData scientist on the experimentation team at Netflix working on observational causal inference with spatial and panel data.\n\nFriday 3 November 2023, 11am - noon\n\nJohn Yang, Princeton University\n“SWE-bench: Can Language Models Resolve Real-World GitHub Issues?”\nJohn Yang is a research assistant working at Princeton University, advised by Professor Karthik Narasimhan. He is interested in Language Grounding & Interaction, Benchmarks for LLMs, Software Engineering, and Code Generation.\n\nFriday 27 October 2023, 11am - noon\n\nJames Zou, Stanford University\n“Can large language models provide useful feedback on research papers? A large-scale empirical analysis”\nJames Zou is an Assistant Professor of Biomedical Data Science and, by courtesy, of Computer Science and Electrical Engineering at Stanford University. He works on making machine learning more reliable, human-compatible and statistically rigorous, and am especially interested in applications in human disease and health. He is a two-time Chan-Zuckerberg Investigator and the faculty director of the university-wide Stanford Data4Health hub. His research is supported by the Sloan Fellowship, the NSF CAREER Award, and Google, Amazon and Adobe AI awards.\n\nFriday 20 October 2023, noon - 1pm\n\nMarzieh Fadaee, Cohere for AI\n“Aya: An Open Science Initiative to Accelerate Multilingual AI Progress”\n\nMarzieh Fadaee is a Senior Research Scientist @ Cohere For AI. She holds a PhD from the Language Technology Lab at the University of Amsterdam, where she developed models to understand and utilize interesting phenomena in data, and was advised by Christof Monz and Arianna Bisazza. She received a B.Sc. from Sharif University majoring in Computer Engineering and M.Sc. from University of Tehran majoring in Artificial Intelligence.\n\nFriday 20 October 2023, 11am - noon\n\nYangjun Ruan, University of Toronto\n“Identifying the Risks of Language Model Agents with an Language Model Emulated Sandbox”\nYangjun Ruan is a Ph.D. Candidate in Computer Science at the University of Toronto, where he is advised by Chris Maddison and Jimmy Ba. Previously, he was a student researcher at Google Research and a research intern at Microsoft Research. In summer 2019, he was a visiting student at UCLA, where he worked with Cho-Jui Hsieh. He obtained a Bachelor degree in Information Engineering from Zhejiang University.\n\nFriday 13 October 2023, noon - 1pm\n\nTom Cardoso, The Globe and Mail\n“Secret Canada: An investigation into Canada’s freedom of information systems”\nTom Cardoso is a member of The Globe and Mail’s investigations team based in Toronto. Since the fall of 2021, he has been investigating Canada’s broken freedom of information systems.\n\nFriday 6 October 2023, noon - 1pm\n\nFabrizio Dell’Acqua, Harvard Business School\n“Experimental evidence on the effect of access to GPT-4 on performance”\nFabrizio Dell’Acqua is a postdoctoral research fellow and teaching fellow at Harvard Business School and the Laboratory for Innovation Science at Harvard (LISH). He received a Ph.D. in Management from Columbia Business School. His research focuses on the areas of automation, human/AI collaboration, and business ethics.\n\nFriday 29 September 2023, noon - 1pm\n\nNima Sarajpoor, Manulife\n\n“STUMPY: A powerful tool for modern time series analysis”\nNima Sarajpoor is a data scientist in Manulife, working in Fraud Detection. He has been contributing to the software STUMPY for about two years.\n\nFriday 22 September 2023, noon - 1pm\n\nSaloni Dattani, Our World in Data\n“Missing data in global health”\nSaloni Dattani is a researcher on global health at Our World in Data, and an editor at Works in Progress. She’s interested in everything related to global health, epidemiology and meta-science."
  },
  {
    "objectID": "events-tdw.html#section-2",
    "href": "events-tdw.html#section-2",
    "title": "Toronto Data Workshop",
    "section": "2022",
    "text": "2022\n\nFriday, 16 December 2022, noon - 1pm\n\nZane Schwartz, Investigative Journalism Foundation\nZane Schwartz is the editor-in-chief of the Investigative Journalism Foundation.\n\nFriday, 25 November 2022, noon - 1pm\n\nMarcel Fortin, U of T Map & Data Library\nLeanne Trimble, U of T Map & Data Library\nRecent additions to the Map and Data Library\nThe Map and Data Library’s data collections, software & support, with a focus on recently acquired datasets.\nLeanne Trimble is a Data & Statistics Librarian, and Marcel Fortin is Head, Map and Data Library.\n\nFriday, 18 November 2022, noon - 1pm\n\nLindsay Katz, University of Toronto\nA new, comprehensive database of all proceedings of the Australian Parliamentary Debates\nLindsay Katz holds a Masters of Statistics from the University of Toronto and a Bachelor of Arts and Science from the University of Guelph where she specialized in Mathematical Science and International Development. At Guelph she worked with Professor Ryan Briggs to explore lived poverty in Africa using Afrobarometer data. At Toronto she works with Professor Monica Alexander to research demographic variation in short-term migration patterns using Facebook data, and with Professor Rohan Alexander to digitize the Australian parliamentary debates from 1901 to present. As an interdisciplinary researcher, she is interested in using statistics to better understand social processes in the world.\n\nFriday, 4 November 2022, noon - 1pm\n\nMaitreyee Sidhaye, St. Michael’s Hospital, Unity Health Toronto\nMeggie Debnath, St. Michael’s Hospital, Unity Health Toronto\nThe Things We Learned from Deploying AI in Healthcare\nMaitreyee and Meggie are data scientists in the Data Science & Advanced Analytics (DSAA) unit at St. Michael’s Hospital in Toronto. DSAA is a multiteam unit in the hospital that provides data sciecne and machine learning solutions across a variety of problems: clinical prediction tools, staffing optimization, imaging, and more. DSAA works very closely with others in the hospital to collaborate on understanding problems and providing solutions. Maitreyee and Meggie will share their experiences and learnings from building and deploying machine learning tools in the hospital.\n\nFriday, 21 October 2022, noon - 1pm\n\nMeg Risdal, Kaggle (Google)\nMeg Risdal is a lead Product Manager for Kaggle (a Google company) where she works with software developers, designers, and researchers to create great experiences for people learning ML, ML practitioners, and ML researchers.\n\nFriday, 14 October 2022, noon - 1pm\n\nApril Wang, University of Michigan\nReimagining Tools for Collaborative Data Science\nApril Wang is a Ph.D. candidate at University of Michigan School of Information, advised by Dr. Steve Oney and Dr. Christopher Brooks. Her research in human-computer interaction (HCI) explores barriers in real-world data science programming practices, and reimagines the workflow and interfaces for collaborative data science environments.\n\nFriday, 7 October 2022, noon - 1pm\n\nRohan Alexander, University of Toronto\nRethinking data science\n\nFriday, 30 September 2022, noon - 1pm\n\nEmily Giambalvo, The Washington Post\nEnce Morse, The Washington Post\nHow the NFL blocks Black coaches\nEmily Giambalvo covers University of Maryland athletics for The Washington Post, where she has worked since June 2018. Emily grew up in South Carolina and graduated from the University of Georgia.\nClara Ence Morse is an Investigative Reporting Workshop intern with The Washington Post’s data desk. She is a student at Columbia University and the editor in chief of the Columbia Daily Spectator.\n\nThursday, 22 September 2022, 5pm - 6pm\n\nMelina Vidoni, Australian National University\nDr Vidoni is a Lecturer at the Australian National University in the CECS School of Computing, where she continues her domestic and international collaborations with Canada and Germany. Dr Vidoni’s main research interests are mining software repositories, technical debt, software development, and empirical software engineering when applied to data science and scientific software.\n\nFriday, 9 September 2022, noon - 1pm\n\nRyan Briggs, University of Guelph\nStatistical power in political science\nRyan Briggs is a social scientist at the University of Guelph\n\nFriday, 1 Apr 2022, noon - 1pm\n\nBrittany Witham, Geopolitica\nData science at a startup\nOriginally from Melbourne, Australia, Brittany received her B.A. in International Studies from the University of Saskatchewan and started her career in economic development, equipping her with comprehensive knowledge of foreign direct investment and international business early in her career.She went on to obtain an M.A. in European and Russian Affairs from the University of Toronto in 2018, where she first discovered the potential of programming for political science and became fascinated with artificial intelligence (AI). Over the past three years, Brittany has worked in many facets of the AI industry, from leading research and development of new AI products for video game developers to building automated data pipelines for business intelligence and managing software engineering and client engagement teams. In that time, she has honed technical skills in full-stack development, machine learning, and data engineering. She recently struck out on her own to launch an online global event monitoring tool and deliver novel solutions to clients in the political risk and social enterprise sectors. Brittany is a firm believer in the potential for data-driven technologies for geopolitics and is excited to contribute to the many discoveries to be made in this space.\n\nThursday, 24 March 2022, 5pm - 6pm\n\nEmi Tanaka, Monash University\nAn anthology of experimental designs\nDr. Emi Tanaka is an assistant professor in statistics at Monash University whose primary interest is to develop impactful statistical methods and tools that can readily be used by practitioners. Her research area include data visualisation, mixed models and experimental designs, motivated primarily by problems in bioinformatics and agricultural sciences. She is currently the President of the Statistical Society of Australia Victorian Branch and the recipient of the Distinguished Presenter’s Award from the Statistical Society of Australia for her delivery of a wide-range of R workshops.\n\nFriday, 18 March 2022, noon - 1pm\n\nMay Chan, University of Toronto, Library\nRamses Van Zon, University of Toronto, SciNet\n\nFriday, 11 March 2022, noon - 1pm\n\nIrena Papst, McMaster University\nUsing models to help guide pandemic response\nI’m a postdoctoral fellow in McMaster’s Mathematics & Statistics department, where I work on mathematical modelling, especially of infectious disease dynamics. I did my PhD in Cornell’s Center for Applied Mathematics. I care deeply about reproducible research, clear scientific communication, good teaching, and big salads.\n\nFriday, 4 March 2022, noon - 1pm\n\nMaria Kamenetsky, University of Wisconsin-Madison\nSpatial clustering\nI am a PhD candidate in Epidemiology at the University of Wisconsin-Madison, where I also completed my MS in Statistics. My research focuses on methods in spatial epidemiology, specifically working on statistical methods and applications in spatial cluster detection.\n\nFriday, 18 February 2022, noon - 1pm\n\nVincent Arel-Bundock, Université de Montréal\nWhat modelsummary taught me about R package development\nI am a political science professor at the Université de Montréal.\n\nFriday, 11 February 2022, noon - 1pm\n\nSilvia Canelón, University of Pennsylvania\nLessons Learned from EHR Research\nSilvia Canelón is a postdoctoral research scientist in the Department of Biostatistics, Epidemiology, and Informatics at the University of Pennsylvania where she applies biomedical informatics to population health research. She uses R to work on projects that develop novel data mining methods to extract pregnancy-related information from Electronic Health Records (EHR) and that study the relationship between environment and disease.\n\nFriday, 4 February 2022, noon - 1pm\n\nNick Huntington-Klein, Seattle University\nThe Influence of Hidden Researcher Decisions in Applied Microeconomics\nI am an economics professor at Seattle University, with research that focuses on higher education, econometrics, and metascience.\n\nFriday, 28 January 2022, noon - 1pm\n\nAshok Chaurasia, University of Waterloo\nMultiple Imputation: Old and New Combining Rules for Statistical Inference\nDr. Ashok Chaurasia is an Assistant Professor (of Statistics) in the School of Public Health Sciences at University of Waterloo. His background/training is in Statistics, with research interests in topics of Missing Data, Data Imputation, Model Selection, and Longitudinal Data Analysis Methodology."
  },
  {
    "objectID": "events-tdw.html#section-3",
    "href": "events-tdw.html#section-3",
    "title": "Toronto Data Workshop",
    "section": "2021",
    "text": "2021\n\nFriday, 10 December 2021, noon - 1pm\n\nNathan Taback, Departments of Statistical Sciences\n“Teaching data science”\nNathan Taback is the director of Data Science programs and an Associate Professor, Teaching Stream in the Department of Statistical Sciences, and Computer Science (cross-appointed) at the University of Toronto. He currently serves as a Special Advisor to the Dean of Arts and Science on Computational and Data Science Education.\n\nFriday, 3 December 2021, noon - 1pm\n\nLeanne Trimble, UofT Libraries\n“Data science and libraries”\nLeanne Trimble is a data librarian at the Map & Data Library, University of Toronto Libraries.\n\nFriday, 26 November 2021, noon - 1pm\n\nKieran Campbell, Lunenfeld Tanenbaum Research Institute\n“Data science and Biomedicine”\nDr Kieran Campbell is an investigator at the Lunenfeld-Tanenbaum Research Institute and an assistant professor at the Departments of Molecular Genetics and Statistical Sciences, University of Toronto. His research focusses on Bayesian models and machine learning for high dimensional biomedical data, including single-cell and cancer genomics. Recently, he has led efforts to develop statistical machine learning methodology to integrate single-cell RNA and DNA sequencing data to uncover the effects of tumour clonal identity on gene expression, as well as methods to automatically delineate the tumour microenvironment from single-cell RNA-sequencing data. Such findings can improve our understanding of cancer progression and of why certain tumours are resistant to therapies, leading to relapse.\n\nFriday, 19 November 2021, noon - 1pm\n\nRadu Craiu, Statistical Sciences, University of Toronto\n“Data science and statistical sciences”\nDr Radu V. Craiu is Professor and Chair of Statistical Sciences at the University of Toronto. His main research interests are in computational methods in statistics, especially, Markov chain Monte Carlo algorithms (MCMC), Bayesian inference, copula models, model selection procedures and statistical genetics.\n\nFriday, 12 November 2021, noon - 1pm\n\nAnn Glusker, Doe Library, University of California Berkeley\n“Supporting Big Data Research at the University of California, Berkeley”\nDr Ann Glusker is Librarian for Sociology, Demography, Public Policy, Psychology (fall 2021) & Quantitative Research at the Doe Library, University of California, Berkeley. She will discuss a recently released report ‘Supporting Big Data Research at the University of California, Berkeley’. This report provides insights on researcher practices and challenges in six thematic areas: data collection & processing; analysis: methods, tools, infrastructure; research outputs; collaboration; training; and balancing domain vs data science expertise.\n\nFriday, 5 November 2021, noon - 1pm\n\nYun William Yu, Math Department, UofT; UTSC Computer & Mathematical Sciences\n“Data science and math”\nYun William Yu is an assistant professor in the math department at UofT whose research focuses on algorithmic methods for computational biology and medical informatics.\n\nFriday, 29 October 2021, noon - 1pm\n\nJosh Speagle, Astronomy & Astrophysics, Dunlap Institute, Statistical Sciences\n“Data science and astronomy”\nJosh is a Banting & Dunlap Postdoctoral Fellow at the University of Toronto whose research focuses on using astrostatistics and “data science” to understand how galaxies like our own Milky Way form, behave, and evolve.\n\nFriday, 22 October 2021, noon - 1pm\n\nTegan Maharaj, Faculty of Information\n“Data science and information”\nI study AI systems and “what goes into” them, e.g. their real-world deployment context, and the effects that has on learning behaviour and generalization. I do that because I want to be able to use AI systems responsibly for problems I think are important, like impact and risk assessments for climate change, AI alignment, ecological management and other common-good problems. My website is: http://www.teganmaharaj.com/.\n\nFriday, 15 October 2021, noon - 1pm\n\nDrew Stommes, Department of Political Science, Yale University\n“On the reliability of published findings using the regression discontinuity design in political science”\nDrew Stommes is a doctoral candidate in the Department of Political Science at Yale University, where he researches democracy, political violence, and quantitative methods. He will talk about a recent working paper.\n\nFriday, 8 October 2021, noon - 1pm\n\nFedor Dokshin, Department of Sociology\n“Data science and sociology”\nFedor Dokshin is an Assistant Professor of Sociology at the University of Toronto. He is a computational social scientist with research interests in social networks, organizations, and energy and the environment. Across these domains, Fedor leverages data science methods and novel data sources to improve existing measurement strategies.\n\nFriday, 1 October 2021, noon - 1pm\n\n“The 2021 Canadian Election”\nDavid Andrews\nDaniel Rubenson\nJohnson Vo\nEric Zhu\nBrian Diep\nAshely (Jing Yuan) Zhang\nKristin (Xi Yu) Huang\nTanvir Hyder\n\nFriday, 24 September 2021, noon - 1pm\n\nKaren Chapple, Department of Geography and Planning/School of Cities\n“Data science and geography, planning, cities”\nKaren Chapple is the inaugural Director of the School of Cities and Professor of Geography and Planning at the University of Toronto. Her research uses data science methods to identify and predict gentrification and displacement in cities. She is Professor Emerita at the University of California, Berkeley, where she helped to launch the undergraduate data science program.\n\nFriday, 20 August 2021, noon - 1pm\n\nJessica Long, Simone Collier, Vinky Wang, Sophie Berkowitz, and Yun-Hsiang Chan, University of Toronto\n“Using statistical model to analyzing shark, lizard, and basketball movement data”\nJessica Long, Simone Collier, Vinky Wang, Sophie Berkowitz, and Yun-Hsiang Chan are undergraduate students at the University of Toronto.\n\nFriday, 13 August 2021, noon - 1pm\n\n“Independent Summer Statistics Community projects”\n‘Prospective Analytics’: Ashley Zhang, Eric Zhu, Muhammad Tsany and Sergio Zheng Zhou.\n‘Statistically Significant’: Aliza Lakho, Chloris Jiang, Janhavi Agarwal, and José Casas on whether young professionals should move to Toronto.\n‘Point Zero Five’: Pan Chen, Xiaoxuan Han, Yi Qin, and Yini Mao on the livability of Toronto for newcomers.\n\nFriday, 6 August 2021, 12:30pm - 1pm\n\nIjeamaka Anyene, Kaiser Permanente Division of Research\n“Taking the next step past standard charts”\nIjeamaka is a data analyst working in healthcare research. She specializes in using R and SAS for data analysis, epidemiological research, and data visualizations. She is also passionate about computational art, knowledge sharing / dissemination, and how to mix the two.\n\nFriday, 30 July 2021, noon - 1pm\n\nKeli Chiu, University of Toronto\n“Detecting sexist and racist text contents with explanations accompanied with GPT-3”\nKeli Chiu is a recent graduate of master in Information at the University of Toronto with the concentration in Human-Centred Data Science. Prior to pursuing her master in the information fields, she worked as a web developer and fell in love with data. Her research interests are natural language processing applications, text analysis and ethics in AI and machine learning. She received rstudio::global Diversity Scholarships in the year of 2021.\n\nFriday, 23 July 2021, noon - 1pm\n\nAnnie Collins and Rohan Alexander, University of Toronto\n“Reproducibility of COVID-19 pre-prints”\nAnnie Collins is an undergraduate student at the University of Toronto specializing in applied mathematics and statistics with a minor in history and philosophy of science. In her free time, she focusses her efforts on student governance, promoting women’s representation in STEM, and working with data in the non-profit and charitable sector.\nRohan Alexander is an assistant professor at the University of Toronto in Information and Statistical Sciences, and a faculty affiliate at the Schwartz Reisman Institute for Technology and Society. He holds a PhD in Economics from the Australian National University.\n\nFriday, 16 July 2021, noon - 1pm\n\nKamilah Ebrahim, University of Toronto\n“Trust in contact tracing apps”\nKamilah Ebrahim received a B.A. in Economics from the University of Waterloo in 2019 and is currently pursuing a Masters of Information in Human Centred Data Science at the University of Toronto. Kamilah is a 2020-21 Graduate Fellow at the University of Toronto Centre for Ethics focusing on the intersection between race, economics and data monopolies in Canada. Prior to joining the University of Toronto she held roles at the United Nation Economic and Social Commission for Asia and the Pacific (UN ESCAP), as well as the Canadian federal government.\n\nFriday, 2 July 2021, noon - 1pm\n\nZachary McCaw, Google\nZachary McCaw is a data scientist at Google.\n\nFriday, 25 June 2021, noon - 1pm\n\nLaura Derksen, University of Toronto Mississauga\n“The impact of student access to Wikipedia”\nLaura is the Amgen Canada Professor in Health System Strategy at the University of Toronto Mississauga and assistant professor in Strategic Management at the Rotman School of Management. Her research interests are development and global health education, information and networks.\n\nFriday, 18 June 2021, noon - 1pm\n\nJacob Matson, Simetric\n“From data ask to dashboard”\nJacob is VP of Finance & Operations at Simetric, Inc..\n\nFriday, 11 June 2021, noon - 1pm\n\nLaura Bronner, Data scientist\n“Quantitative editing”\nLaura is a data scientist, who most recently worked as the quantitative editor at FiveThirtyEight. More generally, she is a data scientist with an interest in causal inference, political science and quantitative text analysis. Before FiveThirtyEight, she was a Senior Analyst at the Analyst Institute, designing and analyzing field experiments for the 2018 election cycle. In September 2018, she completed a PhD in Political Science at the London School of Economics’ Department of Government.\n\nFriday, 4 June 2021, noon - 1pm\n\nHeather Krause, We All Count\n“Equity in Data (or, how not to accidentally use data like a racist, sexist, colonialist, etc)”\nHeather remains unconvinced. As a statistician with decades of global experience working on complex data problems and producing real-world knowledge, she has developed the Data Equity Framework to address the equity issues in data products and research projects. Her emphasis is on combining strong statistical analysis with clear and meaningful communication. She is currently working on implementing tools for equity and ethics in data. As the founder of two successful data science companies, she attacks the largest questions facing societies today, working with both civic and corporate organizations to improve outcomes and lives. Her relentless pursuit of clarity and realism in these projects pushed her beyond pure analysis to mastering the entire data ecosystem including award-winning work in data sourcing, modeling, and data storytelling, each incorporating bleeding edge theory and technologies. Heather is the founder of We All Count, a project for equity in data working with teams across the globe to embed a lens of ethics into their data products from funding to data collection to statistical analysis and algorithmic accountability. Her unique set of tools and contributions have been sought across a range of clients from MasterCard and Volkswagen to the United Nations, the Syrian Refugee Resettlement Secretariat, Airbnb, and the Bill and Melinda Gates Foundation. She is on the Data Advisory Board of the UNHCR.\n\nFriday, 28 May 2021, noon - 1pm\n\nSamantha Pierre, University of Toronto\n“And The Nominees Are… An Empirical Study of the Effects of a Tony Award Win and Nomination on a Show’s Success”\nSamantha is a fourth-year statistics student studying at the University of Toronto. Throughout the past year she has combined her love for theatre and statistics to analyze trends in the theatre community. She volunteers as a member of PAIR-CG to create a representational framework for the international theatre community. She currently works at WOMBO, an app developed by former U of T students, as head of music content.\n\nFriday, 21 May 2021, noon - 1pm\n\nDavid Shor, OpenLabs\n“Political data science”\nDavid is an American data scientist who tries to elect Democrats. He is known for analyzing political polls and currently serves as head of data science with OpenLabs, a progressive nonprofit, and also as a Senior Fellow with the Center for American Progress Action Fund.\n\nThursday, 22 April, 4:30-5:30pm\n\n“Panel on teaching data-focused topics”\nAimee Schwab-McCoy, Creighton University\nAshley Juavinett, UC San Diego\nChris Papalia, St. Andrew’s College\nSamantha-Jo Caetano, University of Toronto\nAimee Schwab-McCoy is an Assistant Professor of Statistics at Creighton University. Ashley Juavinett is a neuroscientist, an educator, and a writer, currently working as an Assistant Teaching Professor at UC San Diego. Chris Papalia is a Mathematics and Science Teacher and Head of House at St. Andrew’s College. Samantha-Jo Caetano is an Assistant Professor, Teaching Stream, at the University of Toronto.\n\nThursday, 15 April, 4:30-5:30pm\n\nEmily A. Sellars, Yale University\n“Missing data and mis-measurement in Mexico’s 1900 census and the Historical Archive of Localities (AHL)”\nEmily A. Sellars is an assistant professor in the Department of Political Science at Yale University. Before coming to Yale, she was an assistant professor in the Bush School of Government and Public Service at Texas A&M University and a postdoctoral scholar at the University of Chicago’s Harris School of Public Policy. She received her Ph. D. in Political Science and Agricultural and Applied Economics from the University of Wisconsin–Madison in 2015. Her research interests are at the intersection of political economy and development economics. Her research examines the political economy of emigration and population.\n\nThursday, 8 April, 4:30-5:30pm\n\n“Mining Process Models from Email Data”\nFaria Khandaker, University of Toronto\nFaria is a 2nd year student of the Information Systems and Design Concentration at the Faculty of Information and is one of the co-hosts of the Toronto Data Workshop. She holds an Honour’s Bachelor of Science Degree in Anthropology and Human Biology from the University of Toronto Scarborough. Since starting her masters, she became interested in research related to data-driven decision making within organizations. Under the supervision of Professor Arik Senderovich, she is researching topics related to the application of Machine Learning within the field of Process Mining and exploring various methodologies for gaining insights from email driven business processes.\nFaria will discuss mining process models from email data.\n\nThursday, 1 April, 4:30-5:30pm\n\nVik Pant, Natural Resources Canada\n“Supporting the Integration of Science and Policy through Data Science and Artificial Intelligence”\nDr. Vik Pant is the Chief Scientist and Chief Science Advisor of Natural Resources Canada (NRCan). He leads the Office of the Chief Scientist at NRCan and reports directly to the Deputy Minister. His office oversees the development and implementation of evidence-based science policy across NRCan sectors and agencies. His office also manages NRCan’s enterprise-wide technology strategy and portfolio of science products. He also runs the Digital Accelerator, which is an innovation platform for designing and launching AI-driven software products in NRCan. Vik is also the Founder of Synthetic Intelligence Forum, which is a leading community of practice focused on the industrial application of Artificial Intelligence (AI). He earned a doctorate from the Faculty of Information (iSchool) in the University of Toronto, a master’s degree in business administration with distinction from the University of London, a master’s degree in information technology from Harvard University, where he received the Dean’s List Academic Achievement Award, and an undergraduate degree in business administration from Villanova University. Vik serves as an Adjunct Professor in the Faculty of Information (iSchool) at the University of Toronto.\n\nThursday, 25 March, 4:30-5:30pm\n\nAlex Cookson, Muse\n“The power of great datasets”\nAlex Cookson is a Data Scientist at Muse, where he helps make the most of their data. In his spare time, you can find him participating in Tidy Tuesday or thinking up cool datasets to explore. And when he’s not doing that, he’s probably cycling around Toronto or doting on his two cats, Tom Tom and Ruby.\nAlex will explore the power of great datasets, and discuss the importance of interesting, fun datasets as a way to guide and motivate learning R.\n\nThursday, 18 March, 4:30-5:30pm\n\nSofia Ruiz-Suarez, National University of Comahue\n“Animal tracking data”\nSofia Ruiz-Suarez holds an undergraduate degree in mathematics from the University of Buenos Aires and now is a PhD candidate at the institute for Research on Biodiversity and Environment. She also teaches mathematics at the University of Comahue and leads R-Ladies at her local city. Her research is focused on Bayesian statistics with applications in animal behaviour and movement.\n\nMonday, 15 March, 4:00-5:00pm\n\nTodd Feathers, Freelance reporter\n“Major Universities are Using Race as a ”High Impact Predictor” of Student Success”\nJointly hosted with Maryclare Griffin, University of Massachusetts Amherst.\nTodd Feathers is a freelance journalist covering artificial intelligence, surveillance, and the technologies changing our world. He spent years at daily newspapers reporting on politics, criminal justice, and health care. On every beat, new tech is solving problems and creating them. His goal is to use data, scientific research, and inside sources to cut through the hype and examine what our gadgets and algorithms really do.Writing in Vice, OneZero, The Wall Street Journal, and others.\n\nThursday, 11 March, 4:30-5:30pm\n\nLucas Cherkewski, Canadian Digital Service\n“Using publicly-available data to better understand the government’s operations”\nLucas Cherkewski is a policy advisor at the Canadian Digital Service (CDS). He helps delivery teams improve government services. From that experience, he advises on structural changes to make better services the default. This work includes plenty of data-enabled research and analysis—Lucas is in a happy place when his work leads him to spend an afternoon poking around a dataset, trying to better understand government so he can help change it.\n\nThursday, 4 March, 4:30-5:30pm\n\nPetros Pechlivanoglou, The Hospital for Sick Children (SickKids) Research Institute\nSimulation and retrospective data for health economic decision making\nPetros Pechlivanoglou, PhD, is a Scientist at The Hospital for Sick Children (SickKids) Research Institute and an Assistant Professor at the University of Toronto, Institute of Health Policy Management and Evaluation. He studied economics in his native country, Greece, econometrics at the University of Groningen, the Netherlands and obtained a PhD in health econometrics from the same university. He completed a post-doctoral fellowship at the University of Toronto, within the Toronto Health Economics and Technology Assessment (THETA) Collaborative where he focused on methodological aspects around the application of decision analysis in health-care policy.\n\nThursday, 18 February, 4:30-5:30pm\n\nUniversity of Toronto DoSS toolkit launch\nSpecial guest Bethany White (Department of Statistical Sciences).\nAnnie Collins, Haoluan Chen, Isaac Ehrlich, Mariam Walaa, Marija Pejcinovska, Mathew Wankiewicz, Michael Chong, Paul Hodgetts, Rohan Alexander, Samantha-Jo Caetano, Shirley Deng, and Yena Joo, University of Toronto\nThe DoSS toolkit is a series of self-paced lessons that students can go through ahead of class, to achieve badges for various levels of accomplishment with R. Instructors can use the badges to work out the level of the class and either direct students to the toolkit to address deficiencies or cover missing aspects themselves.\n\nMonday, 15 February, Noon-1:00pm\n\nEmily Riederer, Capital One\nCausal design patterns for data analysts\nEmily is a Senior Analytics Manager at Capital One. Emily’s team focuses on reimagining analytical infrastructure by building data products, elevating business analysis with novel data sources and statistical methods, and providing consultation and training to partner teams.\n\nThursday, 11 February, 4:30-5:30pm\n\nGarrick Aden-Buie, R Studio\nUsing R Markdown in general and in some specific projects\nGarrick is a Data Science Educator at RStudio who lives in sunny St. Petersburg, Florida. His passion is combining creative coding with programming education, using code to build tools that teach coding to new and advanced R users alike. Like tidyexplain: a project that used ggplot2 and gganimate to reimagine database operations as colorful flying boxes instead of the typical Venn diagrams. Garrick has developed a number of open source addins and packages for RStudio—such regexplain, shrtcts and rsthemes—and is always easily distracted by projects that combine R Markdown and online learning or teaching.\n\nThursday, 4 February, 4:30-5:30pm\n\nKathy Ge, Uber\nHow data insights and experimentation help drive product design and intelligent recommendations on the Uber Eats platform\nKathy is a data scientist with Uber Eats primarily focused on the shopping experience including ranking and recommendations throughout the order flow. She received her M.Sc. in Computer Science and B.Sc in Computer Science and Statistics from the University of Toronto.\n\nThursday, 28 January, 4:30-5:30pm\n\nIrene Duah-Kessie, University of Toronto\nExploring algorithmic bias and fairness and its impact on health outcomes faced by racialized communities\nIrene Duah-Kessie is a graduate of the University of Toronto’s Master of Science in Sustainability Management program. Throughout her studies, Irene published her research on racial income inequality in Toronto with the Wellesley Institute and is currently a part of the Turtle Island Journal of Indigenous Health Editorial Team. Irene is a Project Manager at Across Boundaries leading an initiative to address food security and mental health challenges in Toronto’s Black community. She is also the founder of Rise In STEM, a grassroots organization that aims to increase access to STEM learning opportunities in Black and marginalized communities.\n\nWednesday, 20 January, 4:30-5:30pm\n\nZia Babar, University of Toronto\nDerivative data security\nZia Babar obtained his PhD from the University of Toronto where his research studies focused on the analysis and design of data-centered information systems for enabling enterprise transformation. He is engaged in a multi-year research engagement with IBM Research Labs and is a startup technical mentor at WeWork Labs. He is the organizer of technology meetup groups in both Toronto and Waterloo, and a course instructor at the Faculty of Information, University of Toronto.\n\nThursday, 14 January, 4:30-5:30pm\n\nAndrew Miles, University of Toronto\nCode, plots, and values.\nJointly hosted with Elizabeth Parke and the UTM Collaborative Digital Research Space.\nAndrew Miles is Assistant Professor of Sociology at the University of Toronto and Director of the Morality, Action, and Cognition Lab."
  },
  {
    "objectID": "events-tdw.html#section-4",
    "href": "events-tdw.html#section-4",
    "title": "Toronto Data Workshop",
    "section": "2020",
    "text": "2020\n\nThursday, 17 December 2020, 4-5pm\n\nLiza Bolton, University of Toronto\nMaria Tackett, Duke University\nNathalie Moon, University of Toronto\nTeon Brooks, Mozilla Firefox\n“Panel discussion on teaching data-focused topics”\nLiza Bolton is an Assistant Professor, Teaching Stream, at the University of Toronto. Maria Tackett is an Assistant Professor of the Practice in the Department of Statistical Science at Duke University. Nathalie Moon is an Assistant Professor, Teaching Stream, University of Toronto. Teon L. Brooks, holds a PhD in experimental psychology from NYU, and now works as a data scientist for Mozilla Firefox. He also serves as the technical advisor and President of BrainWaves, an NIH-funded project to teach experimentation and cognitive neuroscience to high school students in NYC, and has co-founded Computation in Education Labs (CIEL), a nonprofit that aims to further the mission of the BrainWaves project while focusing on data science and computational thinking.\n\nThursday, 10 December 2020, 4-5pm\n\nShabrina Mardevi, United Nations Population Fund and University of Toronto\nRomesh Silva, United Nations Population Fund\n“Population data estimation”\nShabrina is a Masters of Information student at the University of Toronto and a Population Data Estimation and Analysis Intern at the United Nations Population Fund. Romesh holds a PhD in Demography from the University of California, Berkeley, and is a Technical Specialist, Health & Social Inequalities, at the United Nations Population Fund.\n\nThursday, 3 December 2020, 5-6pm\n\nMonica Alexander, University of Toronto\n“Using Facebook advertising data to estimate migration”\nMonica Alexander is an Assistant Professor in Statistical Sciences and Sociology at the University of Toronto. She received her PhD in Demography from the University of California, Berkeley. Her research interests include statistical demography, mortality and health inequalities, and computational social science. Monica will talk about using Facebook advertising data to estimate migration.\n\nThursday, 19 November 2020, 4-5pm\n\nMichael Chong, University of Toronto\n“High-throughput Bayesian modelling workflow”\nMichael is a PhD student in the Department of Statistical Sciences at the University of Toronto building models for demographic estimation. Previously, he completed his BSc in Integrated Science at McMaster University.\nMichael will discuss lessons from a high-throughput Bayesian modelling workflow.\n\nThursday, 12 November 2020, 4-5pm\n\nKevin Armstrong, University of Toronto\n“Measuring poverty for NGOs”\nKevin Armstrong is a Masters of Information student at the University of Toronto, and a data consultant for ‘Women’s Integrated Sexual Health’ (WISH) - a three-year program delivering integrated health care in 16 countries in Africa and South Asia.\n\nMonday, 9 November 2020, 4-5pm\n\nTom Cardoso, Globe and Mail\n“Bias Behind Bars”\nTom Cardoso is a crime and justice reporter and data journalist for The Globe and Mail.\nTom will discuss his Bias Behind Bars series of articles which show Black and Indigenous inmates in Canada are more likely to get worse scores than white inmates, based solely on their race.\n\nThursday, 5 November 2020, 4-5pm\n\nAndrew Whitby, Industry data scientist\n“Censuses: The sum of the people”\nAndrew is a data scientist and economist currently looking for his next challenge. He is particularly interested in the economics of technology, creativity, innovation and growth. He wrote The Sum of the People: How the Census Has Shaped Nations from the Ancient World to the Modern Age which was published in March 2020. Previously, he worked as a Data Scientist at the World Bank, and at Nesta, the UK’s innovation think tank. His academic background combines economics, statistics and computer science. He completed his doctoral research in the Department of Economics at the University of Oxford.\n\nThursday, 29 October 2020, 4-5pm\n\nFei Chiang, McMaster University\n“Data currency and applications”\nFei Chiang is an Associate Professor in the Department of Computing and Software (Faculty of Engineering), the Director of the Data Science Research Group, and a Faculty Fellow at the IBM Centre for Advanced Studies. She served as an inaugural Associate Director of the MacData Institute. Her research interests and industrial experience is in data management, spanning data cleaning, data quality, data privacy, data fusion, and database systems.\n\nThursday, 22 October 2020, 4-5pm\n\nJeff Waldman, University of Toronto\nLeanne Trimble, University of Toronto\nLeslie Barnes, University of Toronto\nLisa Strug, University of Toronto\n“Panel discussion on data-focused resources at the University of Toronto”\nJeff Waldman is the Manager, Institutional Data Governance; Leslie Barnes is the Digital Scholarship Librarian at UTL; Leanne Trimble is the Data and Statistics Librarian at UTL; Lisa Strug is a Senior Scientist in the Program of Genetics and Genome Biology, Associate Director of The Centre for Applied Genomics, Professor of Statistical Sciences and Biostatistics at the University of Toronto, and Director of CANSSI Ontario.\n\nThursday, 8 October 2020, 4-5pm\n\nYim Register, University of Washington Data Lab\n“Self-advocacy within machine learning systems”\nYim Register (they/them) is a radical optimist, child advocate, and PhD student at the University of Washington Data Lab exploring what self-advocacy looks like within machine learning systems. They study how empowering novices with Data Science knowledge can impact their participation and joy in an AI-driven world! Their passion project right now is writing a book called Life Lessons from Algorithms, a book thatteaches how machine learning algorithms work through trauma recovery skills.\n\nThursday, 1 October 2020, 4-5pm\n\nFlorence Vallée-Dubois, Université de Montréal\n“Canadian demographics by riding (1991-2015)”\nFlorence Vallée-Dubois is a Ph.D. candidate at the department of political science of the University of Montreal. She is also a member of the Centre for the Study of Democratic Citizenship and Canada Research Chair in Electoral Democracy. Her research interests focus on Quebec and Canadian politics, political behaviour and quantitative methods. Her doctoral project focuses on the political behaviour and democratic representation of seniors in Canada.\n\nThursday, 24 September 2020, 4-5pm\n\nChelsea Parlett-Pelleriti, Chapman University\n“Talking to non-statisticians about statistics”\nChelsea is a PhD candidate and full-time instructional faculty at Chapman University where her research focuses on using novel statistical and Machine Learning methods (mostly Bayesian statistics, IRT models, and clustering) to behavioral data. As an instructor she teaches Python, R and Data Science, and loves using novel technology (like TikTok, Twitch, and flipped classes) to better engage and inspire students.\n\nThursday, 17 September 2020, 4-5pm\n\nAmber Simpson, Queen’s University\n“Cancer and AI”\nDr. Simpson is the Canada Research Chair in Biomedical Computing and Informatics and Associate Professor in the School of Computing (Faculty of Arts and Science) and Department of Biomedical and Molecular Sciences (Faculty of Health Sciences). She specializes in biomedical data science and computer-aided surgery. Her research group is focused on developing novel computational strategies for improving human health.\n\nThursday, 10 September 2020, 4-5pm\n\nA Mahfouz, University of Toronto\nDiego Mamanche Castellanos, University of Toronto\nHidaya Ismail, University of Toronto\nKe-Li Chiu, University of Toronto\nPaul Hodgetts, University of Toronto\n“arxivdl, aRianna, and cesR”\n\nTuesday, 8 September 2020, 3:30-4:30pm\n\nSophie Bennett, Industry data scientist\nUK A levels algorithm issues\n(Jointly hosted with Gillian Hadfield and the Schwartz Reisman Institute for Technology and Society.)\nSophie Bennett holds an undergraduate degree in Experimental Psychology from the University of Oxford and a PhD in Neuroscience from King’s College. She is the lead data scientist at Up Learn, a London-based online learning platform specialising in A levels. In this role, she conducts evaluations of course effectiveness and uses data to improve instruction and curriculum design. She is passionate about increasing the use of responsible evidence and statistics to guide social policy, and, in her spare time, enjoys working with publicly available datasets to explore London demographics, social issues and infrastructure.\n\nThursday, 3 September 2020, 4-5pm\n\nErik Drysdale, The Hospital for Sick Children\n“Using hospital data”\nErik works as a Machine Learning Specialist at the Hospital for Sick Children (SickKids) for the Goldenberg Lab and AI in Medicine (AIM) initiative. His professional responsibilities include the development and training of the machine learning models for various pediatric data science projects. His research interests are focused on the intersection of statistics and machine learning methods such as high-dimensional inference, survival analysis, and optimization methods.\n\nThursday, 20 August 2020, 4-5pm\n\nAije Egwaikhide, IBM\n“Preparing data for optical character recognition (OCR)”\nAije Egwaikhide holds an undergraduate degree in Economics and Statistics from the University of Manitoba, and a post-graduate degree in Business Analytics from St. Lawrence College, Kingston. She works at IBM where she is a Lead Data Scientist on the System Enablement group.\n\nThursday, 13 August 2020, 4-5pm\n\nRichard Iannone, R Studio\n“pointblank”\nRich is a Software Engineer at R Studio.\nRich will talk about pointblank, which is an R package that allows workflows involving nice and easy data validation in reproducible documents.\n\nThursday, 6 August 2020, 4-5pm\n\nSharla Gelfand, Freelance R Developer\n“Creativity in R”\nSharla is a freelance R developer specializing in enabling easy access to data and replacing manual, redundant processes with ones that are automated, reproducible, and repeatable.\n\nThursday, 30 July 2020, 4-5pm\n\nAlex Luscombe, University of Toronto, Criminology and Sociolegal Studies\nAlexander McClelland, Carleton University, Criminology and Criminal Justice\n“Policing the Pandemic”\nAlex Luscombe is a PhD student in the Centre for Criminology & Sociolegal Studies at the University of Toronto and a Junior Fellow at Massey College. Alexander McClelland is an Assistant Professor at the Institute of Criminology and Criminal Justice, Carleton University.\nPolicing the Pandemic is a project that was launched on 4 April, 2020, to track and visualize the massive and extraordinary expansions of police power in response to the COVID-19 Pandemic and the unequal patterns of enforcement that may arise as a result.\n\nThursday, 23 July 2020, 11am-noon\n\nMarta Kołczyńska, Institute of Political Studies of the Polish Academy of Sciences\n“Cleaning survey data to measure political trust”\nMarta is an Assistant Professor at the Institute of Political Studies of the Polish Academy of Sciences and a visiting researcher in the Probabilistic Machine Learning Group, Department of Computer Science, Aalto University. Her research interests include comparative analyses of political attitudes and behavior across nations and over time, as well as the methodology of comparative research, in particular cross-national surveys.\nMarta will talk about cleaning survey data, in particular a project in which she gathers political trust items from different cross-national survey datasets to model time trends, and the tools she has developed to facilitate this work.\n\nThursday, 16 July 2020, 4-5pm\n\nCasey Breen, University of California, Berkeley, Demography\n“CenSoc: A project to link US 1940 Census data with Social Security Administration mortality records”\nCasey is a PhD student in the Demography Department at Berkeley. He previously worked at the Institute for Social Research and Data Innovation, home of IPUMS.\n\nThursday, 9 July 2020, 4-5pm\n\nRoxanne Chui, University of Toronto, Faculty of Information\n“What do we have here among millions of observations? EDA for Tokyo AirBnB data and pattern discovery in listing prices using R”\nRoxanne is an emerging anthropological data science professional. She did her BSc program in Forensic anthropology and worked in the pharmaceutical industry before doing her Masters in data science. She is passionate about excavating context from data for predicting future patterns of human behaviour.\n\nThursday, 2 July 2020, 4-5pm\n\nHeather McBrien, University of Toronto, Department of Statistical Sciences\n“How the data that we collect can bias the results that we obtain and our knowledge of the problem”\nHeather just graduated from the Statistics BSc program at the University of Toronto, and is interested in modelling in population health research, particularly using novel data sources to answer questions where traditional data is lacking.\n\nThursday, 25 June 2020, 4-5pm\n\nA Mahfouz, University of Toronto, Information\n“Geographic data cleaning, extracting mappable data from Google Directions API results in Python”\nA is a Master of Information student at the University of Toronto with a background in geography. Their prior work has been largely concerned with data pipelines.\n\nThursday, 11 June 2020, 4-5pm\n\nHarrison Jones, Deloitte\n“Using R with actuarial data”\nHarrison is a Manager at Deloitte in Toronto, where he focuses on data analytics and machine learning in the property & casualty insurance, life insurance, health insurance, pensions, and the public sector.\n\nThursday, 4 June 2020, 4-5pm\n\nMarija Pejcinovska, University of Toronto, Department of Statistical Sciences\n“Estimating global maternal mortality”\nMarija is a second-year Ph.D. student in Statistics at the University of Toronto. Her research interests are in applied statistics, specifically the application of Bayesian methods to data and modeling challenges that arise in demography, public health, and certain areas of the social sciences.\nMarija will talk about a current project with the World Health Organization (WHO) focused on estimating global maternal mortality to share her R workflow and the different tools and packages she’s found helpful in the data processing stage. More specifically, she’ll be sharing a few ways of dealing with text and date data in R.\n\nThursday, 28 May 2020, 4-5pm\n\nShiro Kuriwaki, Harvard University, Government\n“Project-oriented workflow”\nShiro is a Ph.D. Candidate at the Department of Government, Harvard University. His research focuses on democratic representation in American Politics, for instance cast vote records, public opinion, survey methods, and applied statistics more generally. Shiro will bring together best practices for organizing data and code in the social sciences that experts have proposed with some of his own experience. He will propose a project-oriented workflow that adopts a minimal and consistent file organization structure within a single project, using RStudio Projects and GitHub. He will then discuss how to organize multiple projects that share common components, and propose the use of custom R packages to share code and Dataverse to share large datasets. He will use some of his own projects involving the Cooperative Congressional Election Study (CCES), one of the largest political surveys of American Politics, as a demonstration.\n\nThursday, 21 May 2020, 4-5pm\n\nRohan Alexander, University of Toronto, Faculty of Information\n“OCR with applications to the Kenyan census”\nRohan Alexander is a post-doctoral fellow at the Faculty of Information, University of Toronto. He holds a PhD in Economics from the Australian National University.\n\nFriday, 6 March 2020, noon\n\nFatemeh Nargesian, Computer Science, University of Rochester\n\nFriday, 28 February 2020, noon\n\nEugene Joh, St. Michael’s Hospital\n\nFriday, 14 February 2020, noon\n\nJosh Harris, KOHO\n\nFriday, 7 February 2020, noon\n\nKathy Chung, Records of Early English Drama, University of Toronto\n\nFriday, 31 January 2020, noon\n\nArik Senderovich, Information, University of Toronto\n\nFriday, 24 January 2020, noon\n\nSteven Pimentel, Business intelligence, University of Toronto"
  },
  {
    "objectID": "events-tdw.html#section-5",
    "href": "events-tdw.html#section-5",
    "title": "Toronto Data Workshop",
    "section": "2019",
    "text": "2019\n\nThursday, 21 November 2019, noon\n\nMichelle Alexopoulos, University of Toronto, Economics\nParaskevi Massara, University of Toronto, Medicine\n\nThursday, 7 November 2019, noon\n\nMaria D’Angelo, Delphia\nHareem Naveed, Munich Re\n\nThursday, 24 October 2019, noon\n\nSharla Gelfand, Freelance R and Shiny developer\n\nWednesday, 16 October 2019, noon\n\nLauren Kennedy, Columbia University\n\nThursday, 10 October 2019, noon\n\nHassan Teimoori, Deloitte, Omnia AI\nLudovic Rheault, University of Toronto, Political Science\n\nThursday, 26 September 2019, noon\n\nPeriklis Andritsos, ODAIA & University of Toronto, Information"
  },
  {
    "objectID": "bookshelf.html",
    "href": "bookshelf.html",
    "title": "Bookshelf",
    "section": "",
    "text": "Inspired by Patrick Collison’s version, this is an incomplete (so far) list of the books that Monica and I own in Toronto. I’ve been a bit more liberal than Patrick and included a section on books that I’ve read and would like to own. If you have recommendations, then please get in touch. I usually only buy books that I like, but books with a ⭐️are ones either Monica or I especially like."
  },
  {
    "objectID": "bookshelf.html#academic",
    "href": "bookshelf.html#academic",
    "title": "Bookshelf",
    "section": "Academic",
    "text": "Academic\n\nBryant, John, and Junni L. Zhang, ‘Bayesian Demographic Estimation and Forecasting’.\nChan, Ngai Hang, ‘Time Series’.\nClark, Greg, ‘The Son Also Rises’.\nDuflo, Esther, ‘Expérience, science et lutte contre la pauvreté’. Monica ‘borrowed’ this from the Berkeley Demography library before Duflo won the Nobel and now we’re not giving it back.\nFoster, Ghani, Jarmin, Kreuter, Lane, ‘Big Data and Social Science’.\nFrancois Chollet with JJ Allaire, ‘Deep Learning with R’.\n⭐️Friedman H, Jerome, Robert Tibshirani, and Trevor Hastie, ‘Elements of Statistical Learning’.\n⭐️Gelman, Andrew and Jennifer Hill, ‘Data Analysis Using Regression and Multilevel Hierarchical Models’. Ah, the money-maker! Thank you Gelman and Hill. I haven’t properly studied this book, just read it as needed, but Monica’s probably read it more than enough for both of us.\nGelman, Andrew, Jennifer Hill and Aki Vehtari, ‘Regression and Other Stories’.\nGelman, Andrew, John Carlin, Hal Stern, David Dunson, Aki Vehtari and Donald Rubin, ‘Bayesian Data Analysis’. Known as the old testament in our household.\nGirosi, Federico and Gary King, ‘Demographic Forecasting’.\nHealy, Kieran, ‘Data Visualization’.\nImai, Kosuke, ‘Quantitative Social Science’.\n⭐️McElreath, Richard, ‘Statistical Rethinking’. Known as the new testament in our household.\nMcLean, Ian, ‘Why Australia Prospered’.\nNeuman, Lawrence W, ‘Social Research Methods’.\nPetty, William, John Graunt and Charles Henry Hull, ‘The Economic Writings of Sir William Petty: Together with the observations upon the bills of mortality, more probably by Captain John Graunt’.\n⭐️Pitman, Jim, ‘Probability’. Monica loves this book. We own multiple copies.\nPreston, Samuel, Patrick Heuveline and Michel Guillot, ‘Demography’.\nSalganik, Matthew, ‘Bit by Bit: Social Research in the Digital Age’.\nSmith, David and Nathan Keyfitz, ‘Mathematical Demography’.\nStewart, James, ‘Calculus’.\nTaddy, Matt, ‘Business Data Science’.\nWachter, Ken, ‘Essential Demographic Methods’.\nWakerly, Dannis, William Mengenhall, Richard Scheaffer, ‘Mathematical Statistics with Applications’.\n⭐️Wickham, Hadley, and Grolemund, Garrett, ‘R for Data Science’. Some people convert to Catholicism when they get married, I converted to R. But my knowledge was piecemeal and going through this book addressed that.\n⭐️Witten, Daniela, Gareth James, Robert Tibshirani, and Trevor Hastie, ‘Introduction to Statistical Learning’. Going through this with Peter taught me a lot including: 1) if you want to learn something then buy a physical copy of a textbook, get a partner and commit to weekly chapter reviews; 2) learn statistics from statisticians; and 3) statistics is awesome and I want to learn more. I always thought that I was terrible at it because econometrics never came easily to me, but now I realise that maybe the econometrics-approach isn’t for me, but the statistics one is.\nWu, Changbao and Mary Thompson, ‘Sampling Theory and Practice’."
  },
  {
    "objectID": "bookshelf.html#non-fiction",
    "href": "bookshelf.html#non-fiction",
    "title": "Bookshelf",
    "section": "Non-fiction",
    "text": "Non-fiction\n\n‘150 Years of Stats Canada!’.\nAchatz, Grant and Nick Kokonas, ‘Life, on the Line’.\n⭐️Agassi, Andre, ‘Open’. Tennis is incidental to the aspects that make this book great, so try it even if you don’t particularly like the sport. I can’t remember who recommended it – seems like a Nick Crocker type of book?\nAslett, Don, ‘Is There Life After Housework’. Got given this at the Museum of Clean in Pocatello, Idaho. The museum was surprisingly good!\nCarson, Anne, ‘Autobiography of Red’. Present from Dad.\nClemens, Mark, ‘The Mountain’. Present from Dad.\nClinton, Bill, ‘My Life’.\nCrabb, Annabel, ‘Stop At Nothing’. This Turnbull bloke sounds like he’d be a great prime minister.\nDennison, CJ, ‘Yesterday’s Hobart Today’. Present from Helen.\nEdwards, John, ‘Keating: The Inside Story’.\nElliot, Francis and James Hanning, ‘Cameron’. I haven’t yet come across a good David Cameron biography, but this one was interesting. Focused on pre-parliament period.\nGaskell, Elizabeth, ‘The Life of Charlotte Brontë’.\n⭐️Halberstam, David, ‘The Best and the Brightest’. I read Plato’s Republic at an impressionable age and so I used to think that if we could just put the smart people in charge then the ‘right’ decisions would be made. This book cured me of that.\nHolden, Anthony, ‘King Charles III’.\nHumphrey, Luke, ‘Hansons Half-Marathon Method’.\nHumphrey, Luke, ‘Hansons Marathon Method’. Bought this; followed this; ran my best marathon.\nJohnson, E, Robert and Janet L Byron, ‘Berkeley Walks’.\nKelly, Paul, ‘Triumph and Demise’. Present from Dad.\nKudelka, ‘Hobart’.\nLawrence, TE, ‘Seven Pillars of Wisdom’. ‘Borrowed’ from Dad. .\nLepore, Jill, ‘If Then’. I’ve never been so disappointed by a book. So much potential.\nLevitt, Steven and Stephen Dubner, ‘Freakonomics’.\nLove, David, ‘Unfinished Business’.\nMarr, David, ‘Political Animal’.\nMcNamee, Thomas, ‘Alice Waters and Chez Panisse’.\nMears, Ashley, ‘Very Important People’.\nMegalogenis, George, ‘The Longest Decade’.\nMitchell, M. Waldrop, ‘The Dream Machine’.\nMoore, Charles, ‘Margaret Thatcher’. Present from Dad.\nNT News, ‘What a Croc: Legendary Front Pages From the NT News’. Present from Mark.\nObama, Barack, ‘A Promised Land’.\nObama, Barack, ‘Audacity of Hope’.\nPetzold, Charles, ‘Code: The Hidden Language of Computer Hardware and Software’.\nSchlesinger, Arthur, ‘A Thousand Days’.\nSimmons, Bill, ‘The Book of Basketball’.\nSorenson, Ted, ‘Kennedy’.\nStephanopoulos, George, ‘All Too Human’. I thought that politics was romantic until I read this book when I was 20.\nTrump, Donald and Tony Schwartz, ‘The Art of the Deal’. This book is terrible. Read that NYT article about how he really made his money instead.\nTucker, Ross and Jonathan Dugas, ‘Runner’s World: The Runner’s Body’.\n⭐️Vanhoenacker, Mark, ‘Skyfaring’. Read this on your next flight.\n⭐️Vaughan, Diane, ‘The Challenger Launch Decision’. One of the greatest non-fiction books ever.\nWormser, Baron, ‘The Road Washes Out in Spring’. Stayed at the author’s house, so bought some of his books and particularly liked this one about how his family lived off the grid in New England. Liked it even better after living in Amherst for a while.\nZ, Jay, ‘Decoded’."
  },
  {
    "objectID": "bookshelf.html#fiction",
    "href": "bookshelf.html#fiction",
    "title": "Bookshelf",
    "section": "Fiction",
    "text": "Fiction\n\nAesop, ‘Fables’.\nAmis, Kingsley, ‘Lucky Jim’.\nAmis, Kingsley, ‘The Crime of the Century’.\nBronte, Anne, ‘Agnes Grey’.\nBronte, Anne, ‘The Tenant of Wildfell Hall’.\nBronte, Charlotte, ‘Jane Eyre’.\nBronte, Charlotte, ‘Shirley’.\nBronte, Charlotte, ‘The Professor’.\nBronte, Charlotte, ‘Villette’.\nBronte, Emily, ‘Wuthering Heights’.\nCamus, Albert, ‘Le Chute’.\nChaucer, Geoffrey, ‘The Canterbury Tales’.\nChristie, Agatha, ‘Approximately 1,000,000 different titles’.\nCrichton, Michael, ‘The Andromeda Strain’.\nDavies, Robertson, ‘The Rebel Angels’.\nDeWitt, Helen, ‘Lightning Rods’.\nDeWitt, Helen, ‘Some Trick’.\n⭐️DeWitt, Helen, ‘The Last Samurai’. Nothing to do with the Tom Cruise movie.\nDickens, Charles, ‘Oliver Twist’.\nDoyle, Arthur Conan, ‘Various titles’.\nEliot, George, ‘Middlemarch’.\nEugenides, Jeffrey, ‘Middlesex’.\nFitzgerald, Scott, ‘Tender is the Night’.\nFlanagan, Richard, ‘The Sound of One Hand Clapping’. Present from Helen.\nFleming, Ian, ‘Moonraker’.\nFleming, Ian, ‘On Her Majesty’s Secret Service’.\nGalsworthy, ‘The Silver Spoon and Passers By’.\nGalsworthy, ‘The White Monkey and A Silent Wooing’.\nGalsworthy, John, ‘The Man of Property’.\nGalsworthy, John, ‘End of the Chapter’.\nGalsworthy, John, ‘Swan Song’.\nGalsworthy, John, ‘The Forsyth Saga’.\nGrisham, John, ‘The Pelican Brief’.\nHardy, Thomas, ‘Tess of the D’Urbervilles’.\nHaruf, Kent, ‘Plainsong’.\nIshiguro, Kazuo, ‘The Unconsoled’.\nle Carré, John, ‘A Perfect Spy’. Dad likes le Carré so I buy them when there’s a nice edition.\nMelchor, Fernanda, ‘Hurricane Season’.\nOndaatje, Michael, ‘The English Patient’.\nPotok, Chaim, ‘The Chosen’.\nRowling, K, J, ‘Harry Potter and the Half-Blood Prince’.\nRushdie, Salman, ‘The Moor’s Last Sigh’.\nSchulz, Charles, ‘Snoopy and the Red Baron’.\nSchulz, Charles, ‘Very Funny, Charlie Brown’.\nSeth, Vikram, ‘The Golden Gate’.\nShakespeare, William, ‘Complete Works’.\nSimson, Graeme, ‘The Rosie Project’.\nThackeray, William, ‘Vanity Fair’.\nTolstoy, Leo, ‘War and Peace’.\n⭐️Waugh, Evelyn, ‘Brideshead Revisited’.\nWaugh, Evelyn, ‘Officers and Gentlemen’.\nWinton, Tim, ‘Cloudstreet’. All Australians have a copy of this. I think it’s in the constitution.\nWinton, Tim, ‘Island Home’. Present from Helen. There may be a theme emerging.\nWinton, Tim, ‘The Shepherd’s Hut’. Present from Helen."
  },
  {
    "objectID": "bookshelf.html#food-and-drink",
    "href": "bookshelf.html#food-and-drink",
    "title": "Bookshelf",
    "section": "Food and drink",
    "text": "Food and drink\n\nAlexander, Stephanie, ‘The Cook’s Companion’. Again, all Australians own this.\nCountry Women’s Association of Tasmania, ‘The 21st Birthday Cookery Book’. My understanding, based on Helen, is that all Tasmanian women own this.\nEvans, Matthew, Nick Haddow and Ross O’Meara, ‘The Gourmet Farmer Goes Fishing’. Present from Helen.\nGrossi, Guy, ‘Italian’. Present from Angela.\nKeller, Thomas, ‘Bouchon Bakery’.\nOliver, Jamie, ‘Jamie’s 30-Minute Meals’.\nPant, Pushpesh, ‘India’.\nRedzepi, Rene and David Zilber, ‘The Noma Guide to Fermentation’.\nWomen’s Weekly, ‘Children’s Birthday Cake Book’."
  },
  {
    "objectID": "bookshelf.html#best-books-that-i-read-in",
    "href": "bookshelf.html#best-books-that-i-read-in",
    "title": "Bookshelf",
    "section": "Best books that I read in:",
    "text": "Best books that I read in:\n\n2023\n\nBouk, Dan, “Democracy’s Data” (🙏 Emily)\n\n2022\n\nKuang, R. F., “Babel, or the Necessity of Violence”\nEfron & Hastie, “Computer Age Statistical Inference” (🙏 Monica)\nLight, Singer & Willett, “By Design”\nMindell, “Digital Apollo” (🙏 Dan)\nMorange, “A History of Biology”\n\n2021\n\nAnother baby was born in 2021.\n\n2020\n\nThere was a pandemic.\n\n2019\n\nThe baby was born in 2019.\n\n2018:\n\nCarreyrou, John, ‘Bad Blood’.\nGreen, Joshua, ‘Devil’s Bargain’.\nChozick, Amy, ‘Chasing Hillary’.\nPetzinger, Thomas, ‘Hard Landing’.\nAgassi, Andre, ‘Open’.\nDeWitt, Helen, ‘Some Trick’.\nWaldrop, Mitchell M, ‘The Dream Machine’.\nPrice, David A, ‘The Pixar Touch’."
  },
  {
    "objectID": "teaching-sta302.html",
    "href": "teaching-sta302.html",
    "title": "Methods of Data Analysis I",
    "section": "",
    "text": "“Methods”, “Data”, “Analysis”—we consider such loaded words in this course! What is data? What does it mean to do analysis? And what methods? The very core of statistical sciences!\nThis course develops in students an appreciation for how our world becomes data, what to do in the face of overwhelming options for the analysis of that data, and how to do all this in a way that provides value to others. In addition, by the end of the course, students should be able to understand the mathematical aspects of linear regression and inference for regression models.\n\n\n\n\n2024\n\nSyllabus\nStudent evals\n\n\n\n\n\n\nCan I audit this course? Sure, but it is pointless, because the only way to learn this stuff is to do the work.\nWhy is there so much assessment? The only way to learn this stuff is to actually do the work, and students only do the work when they are assessed. It is unfortunate, but there is no way around it.\nHow difficult is the course? Of students that enrol, the median student drops the course. But the mode overall grade at the end of the course is an A+. The course is not difficult, but the hands-on-projects mean it is a great opportunity for you to do a lot of work. Past students have said that it set them up for success in grad school applications and job interviews.\nWhat is the format of the class? There are rarely lectures because those are not effective. You should read the relevant chapter and attempt the quiz before class. During class we will focus on examples, activities and discussion. We will also have industry guests discuss their experience.\nYou are asking about X, but you didn’t explicitly teach that. In fact you’re not teaching anything. We’re just doing activities in class? A key skill is being able to teach yourself what you need. In general, I will probably have directed you to the materials that you should go over before class and then during class we will do activities that highlight aspects that are easy to over look, but you’re welcome to ask for more pointers if I’ve not been clear enough.\n\n\n\n\nThe purpose of the course is to develop the core skills to do with methods of data analysis that are applicable across academia and industry. By the end of the course, you should be able to:\n\nEngage critically with ideas and readings in data analysis (demonstrated in all papers but also mini-essays and quizzes).\nConduct data analysis research in a reproducible and ethical way (demonstrated in all papers).\nClearly communicate what was done, what was found, and why in writing (demonstrated in all papers).\nUnderstand what constitutes ethical high-quality data analysis practice, especially reproducibility and respect for those that underpin our data (demonstrated in all papers and selected quizzes).\nRespectfully identify strengths and weaknesses in the data analysis research conducted by others (demonstrated in quizzes, and the peer review).\nDevelop the ability to appropriately choose and apply statistical models to real-world situations (demonstrated in the final paper)\nConduct all aspects of the typical data analysis workflow (demonstrated in all papers).\nReflect effectively on your own learning and professional development (demonstrated in some mini-essays and quizzes).\n\n\n\n\nTelling Stories with Data\n\n\n\nIn this course you will use R, Git and GitHub, and a little bit of Python and SQL."
  },
  {
    "objectID": "teaching-sta302.html#preamble",
    "href": "teaching-sta302.html#preamble",
    "title": "Methods of Data Analysis I",
    "section": "",
    "text": "“Methods”, “Data”, “Analysis”—we consider such loaded words in this course! What is data? What does it mean to do analysis? And what methods? The very core of statistical sciences!\nThis course develops in students an appreciation for how our world becomes data, what to do in the face of overwhelming options for the analysis of that data, and how to do all this in a way that provides value to others. In addition, by the end of the course, students should be able to understand the mathematical aspects of linear regression and inference for regression models.\n\n\n\n\n2024\n\nSyllabus\nStudent evals\n\n\n\n\n\n\nCan I audit this course? Sure, but it is pointless, because the only way to learn this stuff is to do the work.\nWhy is there so much assessment? The only way to learn this stuff is to actually do the work, and students only do the work when they are assessed. It is unfortunate, but there is no way around it.\nHow difficult is the course? Of students that enrol, the median student drops the course. But the mode overall grade at the end of the course is an A+. The course is not difficult, but the hands-on-projects mean it is a great opportunity for you to do a lot of work. Past students have said that it set them up for success in grad school applications and job interviews.\nWhat is the format of the class? There are rarely lectures because those are not effective. You should read the relevant chapter and attempt the quiz before class. During class we will focus on examples, activities and discussion. We will also have industry guests discuss their experience.\nYou are asking about X, but you didn’t explicitly teach that. In fact you’re not teaching anything. We’re just doing activities in class? A key skill is being able to teach yourself what you need. In general, I will probably have directed you to the materials that you should go over before class and then during class we will do activities that highlight aspects that are easy to over look, but you’re welcome to ask for more pointers if I’ve not been clear enough.\n\n\n\n\nThe purpose of the course is to develop the core skills to do with methods of data analysis that are applicable across academia and industry. By the end of the course, you should be able to:\n\nEngage critically with ideas and readings in data analysis (demonstrated in all papers but also mini-essays and quizzes).\nConduct data analysis research in a reproducible and ethical way (demonstrated in all papers).\nClearly communicate what was done, what was found, and why in writing (demonstrated in all papers).\nUnderstand what constitutes ethical high-quality data analysis practice, especially reproducibility and respect for those that underpin our data (demonstrated in all papers and selected quizzes).\nRespectfully identify strengths and weaknesses in the data analysis research conducted by others (demonstrated in quizzes, and the peer review).\nDevelop the ability to appropriately choose and apply statistical models to real-world situations (demonstrated in the final paper)\nConduct all aspects of the typical data analysis workflow (demonstrated in all papers).\nReflect effectively on your own learning and professional development (demonstrated in some mini-essays and quizzes).\n\n\n\n\nTelling Stories with Data\n\n\n\nIn this course you will use R, Git and GitHub, and a little bit of Python and SQL."
  },
  {
    "objectID": "teaching-sta302.html#content",
    "href": "teaching-sta302.html#content",
    "title": "Methods of Data Analysis I",
    "section": "Content",
    "text": "Content\nBefore class starts you should go through Chapter 1 “Telling stories with data” and Appendix A “R essentials”.\n\nWeek 1\n\nDrinking from a fire hose\n\n\n\nWeek 2\n\nReproducible workflows\nWriting research\nGuest: Anna Li.\n\n\n\nWeek 3\n\nStatic communication\nGuest: Annie Collins.\n\n\n\nWeek 4\n\nFarm data\nGather data\nGuest: Steven Coyne - “Who Owns This? The Ethics of Copyright”\n\n\n\nWeek 5\n\nHunt data\nGuest: Bradley Congelio - “NFL Analytics”\n\n\n\nWeek 6\n\nClean and prepare\nStore and share\nMissing data\nGuest: Dory Abelman - “Writing masterclass”\n\n\n\nReading Week\n\n\nWeek 7\n\nLinear models\nGuest: TBD\n\n\n\nWeek 8\n\nLinear models\nGuest: TBD\n\n\n\nWeek 9\n\nMid-term\n\n\n\nWeek 10\n\nGeneralized linear models\nGuest: TBD\n\n\n\nWeek 11\n\nPrediction\nWe will focus on trying to predict the upcoming US presidential election, with a view to students being able to write a final paper that could be submitted to the PS: Political Science & Politics special issue.\nGuest: TBD\n\n\n\nWeek 12\n\nMRP and\nGuest: TBD"
  },
  {
    "objectID": "teaching-sta302.html#assessment",
    "href": "teaching-sta302.html#assessment",
    "title": "Methods of Data Analysis I",
    "section": "Assessment",
    "text": "Assessment\n\nSummary\n\n\n\nItem\nWeight (%)\nDue date\nNotes\n\n\n\n\nQuiz\n7\nTuesdays, noon, Weeks 1-12\nOnly best seven out of twelve count.\n\n\nSQL quiz\n1\nTuesday, noon, Week 6\n\n\n\nPersonal website\n1\nTuesday, noon, Week 9\nCreate a personal website using Quarto and make it live via GitHub Pages. At a minimum, it must include a bio and a CV in PDF form.\n\n\nMini-essays\n6\nTuesdays, noon, Weeks 1-12\nOnly best three out of twelve count.\n\n\nTerm papers\n46\nTuesdays, noon, Weeks 3, 6, 9\nTerm Paper I: 23 January 2024\nTerm Paper II: 13 February 2024\nTerm Paper III: 12 March 2024\nYou must submit Term Paper I in order to pass the course.\nOnly best two of three term papers count.\nMarking starts, noon, on the Thursday after submission, and you can update until then i.e. submissions made by noon, Tuesday, Week 3 can be updated until noon, Thursday, Week 3 (this is to allow you to incorporate peer review comments). Please do not make any changes after marking starts.\nTerm Paper I: Donaldson Paper |\nTerm Paper II: Mawson Paper |\nTerm paper III: Pick one of Murrumbidgee Paper, Spadina Paper or Spofforth Paper |\n\n\nConduct peer review of Term/Final papers\n3\nWednesdays, noon, Weeks 3, 6, 9, 12\nConduct peer review for six other term/final papers, by creating a GitHub Issue or Pull Request. Papers will be distributed by a spreadsheet—add a link to the Issue/PR to a term paper that does not have four other entries. You will only have 24 hours to do this.\nStudents are not assigning grades to other students, but are instead getting the mark based on the quality of the feedback they provide to other students.\n\n\nMid-term\n6\nIn-class, Tuesday, 12 March 2024\nA mid-term consisting of a few questions.\n\n\nFinal paper\n30\nTuesday, noon, Week 12 (2 April 2024)\nYou must submit this paper.\nMarking starts, noon, Thursday 18 April and you can update until then i.e. submissions made by noon, Tuesday, Week 12 can be updated until noon, Thursday, 18 April (this is to allow you to incorporate peer review comments). Please do not make any changes after marking starts. |\nFinal paper.\n\n\n\nYou must submit Term Paper 1. You must submit the Final Paper. Beyond that, you have scope to pick an assessment schedule that works for you. I will take your best three of the twelve insight mini-essays for that six per cent, and your best seven of twelve quizzes for that seven per cent. I take your two best papers from the three term papers for that 46 per cent (23 per cent for each). You get up to three percentage points for conducting peer review of other student papers, (half a percentage point per review). There is 30 per cent allocated for the Final Paper.\nAdditional details:\n\nQuiz questions are drawn from those in the Quiz section that follows each chapter of Telling Stories with Data. Some of them are multiple choice, and you should expect to know the mark within a few days of submission. Please do them before coming to class.\nMini-essay questions are drawn from those in the Tutorial section that follows each chapter of Telling Stories with Data. The general expectation (although this differs from week to week) is about two pages of written content. You should expect to know the mark within a few days of submission.\nIn general term papers require a considerable amount of work, and are due after the material has been covered in quizzes and mini-essays (i.e. you would draw on knowledge tested in the quizzes, and potentially material could be re-used from the mini-essay material). In general, they require original work to some extent. Papers are taken from the Papers appendix of Telling Stories with Data and students have access to the grading rubrics before submission.\nIf you already have a website, please communicate with me about this early in the term so that I can let you know whether it can be used for the purposes of this submission.\nWhile they vary, a rough rubric for mini-essays is:\n\n0 - Any typos, grammatical errors, other table stakes issues for this level. Submission is too short. Other basic mistakes.\n0.25 - Tables/graphs not properly labeled, no references, other aspects that affect credibility.\n0.5 - Makes some interesting and relevant points, related to course material (including required materials), but lacking in terms of structure and story/argument.\n0.80 - Interesting submission that is well-structured, coherent, and credible.\n1 - As with 0.80, but exceptional in some way.\n\nOnly the best two of three term papers counts. This means each is worth 23 per cent."
  },
  {
    "objectID": "the_other_course.html",
    "href": "the_other_course.html",
    "title": "The Other Course",
    "section": "",
    "text": "This is a course that improves your skills in data science and gives you the space to write a paper within certain guardrails.\nThe course will be an enormous amount of work and cause you a large amount of stress because it is likely your first opportunity to do unstructured original research. This is unfortunate, but there’s little way around it. All I can tell you is that having done this course, it’ll be easier in the future. Pressure makes diamonds.\nThe purpose of this course is to write an original research paper, and in the process of that, to learn some useful skills. The paper will incorporate relevant literature, detailed data collection processes, be reproducible, use technically and statistically sound methodology, and present the outcomes in an informative manner.\nThe purpose of this course is to explore the technical aspects required to design and complete an end-to-end data science project, similar to those that students are likely to encounter in a professional environment. This course will require students to:\n\nGather data through scraping and the use of APIs.\nDesign supporting architecture to allow the data to gather over time.\nDevelop machine learning models describing the statistical relationship between variables within the dataset.\nTest the outcomes with new data to examine machine learning prediction veracity.\nPresent the outcomes of a highly specific concept in a meaningful manner to a non-technical audience.\n\nEssentially this course provides students with the freedom to conduct original research on a topic of interest to them within certain guidelines.\n\n\n\n\nCan I audit this course? No. It’s a reading course so the concept of auditing doesn’t make sense.\nCan I attend lectures? No. There are no lectures. Students update their GitHub repo on a weekly basis and we meet the next day to talk through things.\n\n\n\n\n\nCollect real-world data and design systems to allow for the dataset to continuously grow.\nRefine R skills.\nClean, manage, analyze, and make predictions with data towards the project goal.\nExplore and implement modelling options and explore relevant assumptions.\nExplore relationships within data\nDesign an interactive Shiny app or R Package for presentation of the results and to allow the model to be deployed and used by others.\nHave a high-quality project to show the culmination of learning.\n\n\n\n\nYou need to have taken ‘The Course’ or equivalent, such that you’ve taken courses such that you’ve covered everything through to (but not including) ‘Enrichment’ of Telling Stories with Data.\n\n\n\nThanks to the following who helped develop this course: Thomas William Rosenthal."
  },
  {
    "objectID": "the_other_course.html#preamble",
    "href": "the_other_course.html#preamble",
    "title": "The Other Course",
    "section": "",
    "text": "This is a course that improves your skills in data science and gives you the space to write a paper within certain guardrails.\nThe course will be an enormous amount of work and cause you a large amount of stress because it is likely your first opportunity to do unstructured original research. This is unfortunate, but there’s little way around it. All I can tell you is that having done this course, it’ll be easier in the future. Pressure makes diamonds.\nThe purpose of this course is to write an original research paper, and in the process of that, to learn some useful skills. The paper will incorporate relevant literature, detailed data collection processes, be reproducible, use technically and statistically sound methodology, and present the outcomes in an informative manner.\nThe purpose of this course is to explore the technical aspects required to design and complete an end-to-end data science project, similar to those that students are likely to encounter in a professional environment. This course will require students to:\n\nGather data through scraping and the use of APIs.\nDesign supporting architecture to allow the data to gather over time.\nDevelop machine learning models describing the statistical relationship between variables within the dataset.\nTest the outcomes with new data to examine machine learning prediction veracity.\nPresent the outcomes of a highly specific concept in a meaningful manner to a non-technical audience.\n\nEssentially this course provides students with the freedom to conduct original research on a topic of interest to them within certain guidelines.\n\n\n\n\nCan I audit this course? No. It’s a reading course so the concept of auditing doesn’t make sense.\nCan I attend lectures? No. There are no lectures. Students update their GitHub repo on a weekly basis and we meet the next day to talk through things.\n\n\n\n\n\nCollect real-world data and design systems to allow for the dataset to continuously grow.\nRefine R skills.\nClean, manage, analyze, and make predictions with data towards the project goal.\nExplore and implement modelling options and explore relevant assumptions.\nExplore relationships within data\nDesign an interactive Shiny app or R Package for presentation of the results and to allow the model to be deployed and used by others.\nHave a high-quality project to show the culmination of learning.\n\n\n\n\nYou need to have taken ‘The Course’ or equivalent, such that you’ve taken courses such that you’ve covered everything through to (but not including) ‘Enrichment’ of Telling Stories with Data.\n\n\n\nThanks to the following who helped develop this course: Thomas William Rosenthal."
  },
  {
    "objectID": "the_other_course.html#content",
    "href": "the_other_course.html#content",
    "title": "The Other Course",
    "section": "Content",
    "text": "Content\n\nWeek 1\nTasks:\n\nCreate research plan.\nConduct initial literature review.\nAddress any weaknesses in version control.\n\nReadings:\n\nKing, Gary, ‘How to Write a Publishable Paper as a Class Project’, https://gking.harvard.edu/papers.\nShapiro, Jesse, ‘Four Steps to an Applied Micro Paper’, https://www.brown.edu/Research/Shapiro/pdfs/foursteps.pdf.\nRiederer, Emily, ‘RMarkdown Driven Development (RmdDD)’, https://emilyriederer.netlify.com/post/rmarkdown-driven-development/.\nMiyakawa, Tsuyoshi, ‘No raw data, no science: another possible source of the reproducibility crisis’, Molecular Brain, https://doi.org/10.1186/s13041-020-0552-2.\nBryan, Jenny, Happy Git and GitHub for the useR, https://happygitwithr.com.\n\n\n\nWeek 2\nTasks:\n\nContinue literature review.\nIdentify relevant data.\nMake first Shiny app.\nSetup GitHub Repo, with appropriate folder structure and README.\n\nReadings:\n\nWickham, Hadley, 2020, Mastering Shiny, Chapters 2-5.\nJesse Shapiro, ‘Code and Data for the Social Sciences: A Practitioner’s Guide’, https://web.stanford.edu/~gentzkow/research/CodeAndData.xhtml\n\n\n\nWeek 3\nTasks:\n\nGather data:\n\nBuild initial webscrapers.\nBuild simulated dataset.\n\nFinalize literature review.\nBegin automating.\n\nReadings:\n\nCouch, Simon, ‘Running R Scripts on a Schedule with GitHub Actions’, https://blog.simonpcouch.com/blog/r-github-actions-commit/.\nWickham, Hadley, 2020, Mastering Shiny, Chapters 2-5\n\n\n\nWeek 4\nTasks:\n\nDevelop basic infrastructure for housing data.\nSet-up GitHub actions to automate gathering and cleaning.\nEstablish data pipeline to update analysis dataset.\nWrite tests against the simulated clean dataset.\n\n\n\nWeek 5\nTasks:\n\nMake second Shiny app.\nBuild first R package.\nBegin cleaning dataset toward passing tests.\nContinue automating data gathering.\n\nReadings:\n\nWickham, Hadley, 2020, Mastering Shiny, Chapters 8-9, https://mastering-shiny.org/action-feedback.html.\nWickham, Hadley and Jenny Bryan, R Packages, Chapter 2, https://r-pkgs.org/whole-game.html.\n\n\n\nWeek 6\nTasks:\n\nFinish any data pipeline/architecture development.\nPrepare data for modelling:\n\nEDA.\nFully incorporate all datasources.\nFurther cleaning if necessary.\n\nDetermine features of interest.\nContinue developing statistics skills.\nFamiliarize with tidymodels.\n\nReadings:\n\nKuhn, Max, and Julia Silge, 2021, Tidy Modeling with R, Chapters 1-5.\nWickham, Hadley, and Garrett Grolemund, 2017, R for Data Science, Chapters 2-8.\nGareth M. James, Daniela Witten, Trevor Hastie, and Robert Tibshirani, 2021, An Introduction to Statistical Learning, Second Edition, Chapters 1-4.\n\n\n\nWeek 7\nTasks:\n\nBuild model with tidymodels or alternative approach.\nBuild a second R package that is more involved.\nContinue developing statistics skills, including Bayesian methods.\n\nReadings:\n\nKuhn, Max and Julia Silge, 2021, Tidy Modeling with R, Chapters 6-7.\nWickham, Hadley, and Garrett Grolemund, 2017, R for Data Science, Chapters 22-25.\nGareth M. James, Daniela Witten, Trevor Hastie, and Robert Tibshirani, 2021, An Introduction to Statistical Learning, Second Edition, Chapters 5-7.\nMcElreath, Richard, 2020, Statistical Rethinking, Second edition, Chapters 1-4.\n\n\n\nWeek 8\nTasks:\n\nContinue machine learning development\n\nExplore, script, and compare different machine learning algorithm performance\nFinalize model development and evaluate results.\nBuild third Shiny app.\n\nReadings:\n\nKuhn, Max and Julia Silge, 2021, Tidy Modeling with R, Chapters 8-9.\nWickham, Hadley, 2020, Mastering Shiny, Chapters 10-12.\nMcElreath, Richard, 2020, Statistical Rethinking, Second edition, Chapters 5-6.\n\n\n\nWeek 9\nTasks:\n\nContinue improving model and pipeline, especially model evaluation and refinement with data augmentation.\nContinue developing statistics skills.\n\nReadings:\n\nKuhn, Max and Julia Silge, 2021, Tidy Modeling with R, Chapters 10-12.\nMcElreath, Richard, 2020, Statistical Rethinking, Second edition, Chapters 7-8.\n\n\n\nWeek 10\nTasks:\n\nBegin write-up of paper, especially data and model sections.\nBegin developing R package or Shiny app\n\nReadings:\n\nKuhn, Max and Julia Silge, 2021, Tidy Modeling with R, Chapters 13-14.\nMcElreath, Richard, 2020, Statistical Rethinking, Second edition, Chapters 11-13.\n\n\n\nWeek 11\nTasks:\n\nContinue write-up, especially results and discussion sections.\nFinish developing R package or Shiny app.\n\nReadings:\n\nWickham, Hadley, 2020, Mastering Shiny, Chapters 13-16.\nMcElreath, Richard, 2020, Statistical Rethinking, Second edition, Chapters 14-16.\n\n\n\nWeek 12\nFinalise all apsects.\nTasks:\n\nAs needed."
  },
  {
    "objectID": "the_other_course.html#assessment",
    "href": "the_other_course.html#assessment",
    "title": "The Other Course",
    "section": "Assessment",
    "text": "Assessment\n\nLearning diary\nTask: Each week you will read relevant papers and books, engaging with them by writing notes and completing exercises. You will use GitHub to manage these notes and exercises and email a link to me at the end of each week. Additionally, reflect on what went well, what has room for improvement, and consider ‘lessons learned’ during the week.\nDate: At the end of the week please send me a link to the GitHub repo that contains this diary.\nWeight: 15 per cent.\n\n\nPresentation I\nTask: 10-15-minute presentation on what you’ve learned about the literature and plans.\nDate: Roughly end of Week 4 (exact date determined by lab presentation cycle— the last Friday of the month).\nWeight: 15 per cent.\n\n\nPresentation II\nTask: 10-15-minute presentation on what you’ve learned about the data.\nDate: Roughly end of Week 8 (exact date determined by lab presentation cycle— the last Friday of the month).\nWeight: 15 per cent.\n\n\nPresentation III\nTask: 10-15-minute presentation on what you’ve learned about the model.\nDate: Roughly end of Week 12 (exact date determined by lab presentation cycle— the last Friday of the month).\nWeight: 15 per cent.\n\n\nFinal Paper\nTask: A fully reproducible paper and associated Shiny app or R Package. Toward the mid-term break we will have a meeting to discuss the topic of your final paper. It will be due on the last day of the exam period. This will be marked by me and reviewed by another professor.\nDate: Second last day of exam period.\nWeight: 40 per cent."
  },
  {
    "objectID": "teaching-inf3104.html",
    "href": "teaching-inf3104.html",
    "title": "Data science foundations",
    "section": "",
    "text": "Quantitative approaches have a common concern: How can others be confident that our statistical models have been brought to bear on appropriate datasets? This course focuses on the ‘data’ of data science. It develops in students an appreciation for the many ways in which dealing with a dataset can get out-of-hand, and establishes approaches to ensure data science is conducted in ways that engenders trusted findings. It touches on statistical modelling, but focuses on everything that comes before and after modelling, and in doing so ensures modelling and analysis are placed on a firmer foundation. In assessment, students will conduct end-to-end data science projects using real-world data, enabling them to fully understand potential pitfalls, and build a portfolio.\nThe purpose of this course is to develop students who appreciate, and can iterate on, the foundations of data science.\nThe focus of the learning will be on:\n\nactively reading and consider relevant literature;\nactively using the statistical programming language R in real-world conditions;\ngathering, cleaning, and preparing datasets; and\nchoosing and implementing statistical models and evaluating their estimates.\n\nEssentially this course provides students with everything that they need to know to be able to do the most exciting thing in the world: use data to tell convincing stories.\n\n\n\n\nCan I audit this course? Sure, but it is pointless, because the only way to learn this stuff is to do the work.\nWhat is a tutorial? You write a paper. Then you send it to your tutor. The next day you have a meeting, ‘tutorial’, where you discuss it with them.\nWhy is there so much assessment? The only way to learn this stuff is to actually do the work, and students only do the work when they are assessed. It is unfortunate, but there is no way around it.\nHow difficult is the course? Of students that enrol, the median student drops the course. But the mode overall grade at the end of the course is an A+. The course is not difficult, but the hands-on-projects mean it is a lot of work.\nWhat is the format of the class? There are rarely old-school lectures because those are not effective. You should read the relevant chapter before class. During class we will focus on tutorials and discussion. We will also have industry guests discuss their experience.\nHow mature is the course? This is the fifth iteration of this course. A lot of the materials are well-developed but there is still a long way to go. Your feedback is appreciated.\n\n\n\n\nThe purpose of the course is to develop the core skills of data science that are applicable across academia and industry. By the end of the course, you should be able to:\n\nEngage critically with ideas and readings in data science (demonstrated in all papers but also tutorials and quizzes).\nConduct research in data science in a reproducible and ethical way (demonstrated in all papers).\nClearly communicate what was done, what was found, and why in writing (demonstrated in all papers).\nUnderstand what constitutes ethical high-quality data science practice, especially reproducibility and respect for those that underpin our data (demonstrated in all papers and selected quizzes).\nRespectfully identify strengths and weaknesses in the data science research conducted by others (demonstrated in quizzes, and the peer review).\nDevelop the ability to appropriately choose and apply statistical models to real-world situations (demonstrated in the final paper)\nConduct all aspects of the typical data science workflow (demonstrated in all papers).\nReflect effectively on your own learning and professional development (demonstrated in some tutorials and quizzes).\n\n\n\n\n\nNone.\n\n\n\n\n\n2023\n\nSyllabus\nStudent evals\n\n\n\n\n\nTelling Stories with Data"
  },
  {
    "objectID": "teaching-inf3104.html#preamble",
    "href": "teaching-inf3104.html#preamble",
    "title": "Data science foundations",
    "section": "",
    "text": "Quantitative approaches have a common concern: How can others be confident that our statistical models have been brought to bear on appropriate datasets? This course focuses on the ‘data’ of data science. It develops in students an appreciation for the many ways in which dealing with a dataset can get out-of-hand, and establishes approaches to ensure data science is conducted in ways that engenders trusted findings. It touches on statistical modelling, but focuses on everything that comes before and after modelling, and in doing so ensures modelling and analysis are placed on a firmer foundation. In assessment, students will conduct end-to-end data science projects using real-world data, enabling them to fully understand potential pitfalls, and build a portfolio.\nThe purpose of this course is to develop students who appreciate, and can iterate on, the foundations of data science.\nThe focus of the learning will be on:\n\nactively reading and consider relevant literature;\nactively using the statistical programming language R in real-world conditions;\ngathering, cleaning, and preparing datasets; and\nchoosing and implementing statistical models and evaluating their estimates.\n\nEssentially this course provides students with everything that they need to know to be able to do the most exciting thing in the world: use data to tell convincing stories.\n\n\n\n\nCan I audit this course? Sure, but it is pointless, because the only way to learn this stuff is to do the work.\nWhat is a tutorial? You write a paper. Then you send it to your tutor. The next day you have a meeting, ‘tutorial’, where you discuss it with them.\nWhy is there so much assessment? The only way to learn this stuff is to actually do the work, and students only do the work when they are assessed. It is unfortunate, but there is no way around it.\nHow difficult is the course? Of students that enrol, the median student drops the course. But the mode overall grade at the end of the course is an A+. The course is not difficult, but the hands-on-projects mean it is a lot of work.\nWhat is the format of the class? There are rarely old-school lectures because those are not effective. You should read the relevant chapter before class. During class we will focus on tutorials and discussion. We will also have industry guests discuss their experience.\nHow mature is the course? This is the fifth iteration of this course. A lot of the materials are well-developed but there is still a long way to go. Your feedback is appreciated.\n\n\n\n\nThe purpose of the course is to develop the core skills of data science that are applicable across academia and industry. By the end of the course, you should be able to:\n\nEngage critically with ideas and readings in data science (demonstrated in all papers but also tutorials and quizzes).\nConduct research in data science in a reproducible and ethical way (demonstrated in all papers).\nClearly communicate what was done, what was found, and why in writing (demonstrated in all papers).\nUnderstand what constitutes ethical high-quality data science practice, especially reproducibility and respect for those that underpin our data (demonstrated in all papers and selected quizzes).\nRespectfully identify strengths and weaknesses in the data science research conducted by others (demonstrated in quizzes, and the peer review).\nDevelop the ability to appropriately choose and apply statistical models to real-world situations (demonstrated in the final paper)\nConduct all aspects of the typical data science workflow (demonstrated in all papers).\nReflect effectively on your own learning and professional development (demonstrated in some tutorials and quizzes).\n\n\n\n\n\nNone.\n\n\n\n\n\n2023\n\nSyllabus\nStudent evals\n\n\n\n\n\nTelling Stories with Data"
  },
  {
    "objectID": "teaching-inf3104.html#content",
    "href": "teaching-inf3104.html#content",
    "title": "Data science foundations",
    "section": "Content",
    "text": "Content\nBefore class starts you should go through Chapter 1 and Appendix A of Telling Stories with Data.\n\nWeek 1\n\nDrinking from a fire hose\n\n\n\nWeek 2\n\nReproducible workflows\nWriting research\n\n\n\nWeek 3\n\nStatic communication\nFarm data\n\n\n\nWeek 4\n\nGather data\nHunt data\n\n\n\nWeek 5\n\nClean and prepare\nStore and share\n\n\n\nWeek 6\n\nTechnical skills exam\n\n\n\nWeek 7\n\nLinear models\n\n\n\nWeek 8\n\nLinear models\n\n\n\nWeek 9\n\nGeneralized linear models\n\n\n\nWeek 10\n\nGeneralized linear models\n\n\n\nWeek 11\n\nPrediction\n\n\n\nWeek 12\n\nTBD"
  },
  {
    "objectID": "teaching-inf3104.html#assessment",
    "href": "teaching-inf3104.html#assessment",
    "title": "Data science foundations",
    "section": "Assessment",
    "text": "Assessment\n\n\n\nItem\nWeight (%)\nDue date\n\n\n\n\nQuiz\n6\nWednesdays, noon, Weeks 1-3, 5, 7, 8, 10, 11\n\n\nEssay\n6\nWednesday, noon, Weeks 1-3, 5, 7\n\n\nWebsite\n2\nWednesday, noon, Week 2\n\n\nPeer review\n6\nThursdays, noon, Weeks 1-5, 7, 9, 12\n\n\nTerm Paper 1 (Donaldson)\n20\nWednesday, noon, Week 4\n\n\nTechnical skills exam\n10\nWednesday, noon, Week 6\n\n\nTerm Paper 2 (Howrah)\n20\nWednesday, noon, Week 9\n\n\nFinal Paper\n30\nWednesday, noon, Week 12\n\n\n\nQuizzes are done individually and encourage you to engage with the material. Only the best five count. Quiz questions are drawn from those in the Quiz section that follows each chapter of Telling Stories with Data.\nEssays provide a chance to adjust to the expectations of the course in a forgivable way. Only the best three count. Essays are done in groups, which are randomly set, each week, by the instructor. You may not change groups, but the groups will be different every week. Everyone in the group gets the same mark, unless there is something egregious (as evidenced by essentially having made no GitHub contributions). Essays are drawn from those in the Tutorial section that follows each chapter of Telling Stories with Data. The general expectation (although this differs from week to week) is about two pages of written content.\nEssays rubric: An essay will receive with 0 or 1. You must have a well-organized GitHub repo based on the starter folder. You must follow the example Quarto document and have a relevant title, date, authors, abstract, link to the supporting GitHub repo, introduction, other relevant sections, cross-referenced and captioned table/s and graph/s, and references. There must not be any typos, grammatical errors, poor writing. Essays must make interesting and relevant points, grounded in the course material, and building off them in some way. Essays must be well-structured, coherent, and credible.\nThe website is designed to make sure that you have set-up GitHub correctly and are comfortable with basic commands.\nEssays, the website, and all papers have “peer review”. You submit by the deadline, then (complete and) receive peer review feedback within 24 hours. You then have through to the following Monday, 9am, to update and re-submit the underlying assessment if you’d like.\nThe technical exam will cover R, SQL, Git and GitHub, and Python. It will be in class.\nTerm Paper 1 is done individually.\nTerm Paper 2 may be done in groups of 1-3 people.\nThe Final Paper is done individually.\nSummary:\n\nWeek 1: Quiz, essay\nWeek 2: Quiz, essay\nWeek 3: Quiz, essay\nWeek 4: Term paper\nWeek 5: Quiz, essay\nWeek 6: Exam\nWeek 7: Quiz, essay\nWeek 8: Quiz\nReading Week: -\nWeek 9: Term paper\nWeek 10: Quiz\nWeek 11: Quiz\nWeek 12: Final paper\nExam Block: Final paper updated"
  },
  {
    "objectID": "teaching-inf3104.html#other",
    "href": "teaching-inf3104.html#other",
    "title": "Data science foundations",
    "section": "Other",
    "text": "Other\n\nChildren in the classroom\nBabies (bottle-feeding, nursing, etc) are welcome in class as often as necessary. You are welcome to take breaks to feed your infant or express milk as needed, either in the classroom or elsewhere including here. A list of baby change stations is also available here. Please communicate with me so that I can make sure that we have regular breaks to accommodate this.\nFor older children, I understand that unexpected disruptions in childcare can happen. You are welcome to bring your child to class in order to cover unforeseeable gaps in childcare.\n\n\nAccommodations with regard to assessment\nPlease do not reveal your personal or medical information to me. I understand that illness or personal emergencies can happen from time to time. The following accommodations to assessment requirements exist to provide for those situations.\nStraight-forward (will automatically apply to all students - there’s no need to ask for these):\n\nQuiz: Only best five quizzes count.\nTutorial: Only best three tutorials count.\nPapers #1-#4: Worst two are dropped.\n\nSo for those (with the exception of Paper #1), if you have a situation, then just don’t submit.\nSlightly more involved:\n\nPaper #1: You must submit something for Paper #1, even if it gets zero. If you have a medical reason that makes it impossible for you to submit Paper #1, then you are welcome to continue with the class, but then one of the remaining term papers (Papers #2 - #4), must be done individually to ensure fairness with the rest of the class.\nPeer review: No accommodation or late submission is possible for this because it would hold up the rest of the class. If you cannot submit then email me before the deadline and the weight will be shifted to the final paper.\nFinal paper: The final paper is a critical piece of assessment. It is also up against deadlines for submission of grades. Extensions for valid reasons may be granted for a maximum of three days, however this isn’t possible for all students (i.e. there are restrictions around graduating students). This means the exact extension needs to be at my discretion. To be considered, an extension request must be sent to rohan.alexander@utoronto.ca by the business day before the due date so there is time to get advice from a faculty/department/college advisor about your particular circumstance.\n\n\n\nRe-grading\nRequests to have your work re-graded will not be accepted within 24 hours of the release of grades. This is to give you a chance to reflect. Similarly, requests to have your work re-graded more than seven days after the release of the grades will not be accepted. This is to ensure the course runs smoothly.\nInside that 1-7 day period if you would like to request a re-grade, please email rohan.alexander@utoronto.ca. Please specify where the marking error was made in relation to the marking guide. The entire assessment will be re-marked and it is possible that your grade could reduce.\nPlenty of students get 0 on the first paper, but go on to get an A+ overall in the course. The nature of the work in this course requires students to adjust from what is expected in other courses, and the forgiving assessment weighting is designed to allow this.\n\n\nPlagiarism and integrity\nPlease do not plagiarize. In particular, be careful to acknowledge the source of code - if it is extensive then through proper citation and if it is just a couple of lines from Stack Overflow then in a comment immediately next to the code.\nYou are responsible for knowing the content of the University of Toronto’s Code of Behaviour on Academic Matters.\nAcademic offenses includes (but is not limited to) plagiarism, cheating, copying R code, communication/extra resources during closed book assessments, purchasing labor for assessments (of any kind). Academic offenses will be taken seriously and dealt with accordingly. If you have any questions about what is or is not permitted in this course, please contact me.\nPlease consult the University’s site on Academic Integrity. Please also see the definition of plagiarism in section B.I.1.(d) of the University’s Code of Behaviour on Academic Matters available here. Please read the Code. Please review Cite it Right and if you require further clarification, consult the site How Not to Plagiarize.\n\n\nLate policy\nYou are expected to manage your time effectively. If no extension has been granted and no accommodation applies, then the late submission of an assessment item carries a penalty of 10 percentage points per day to a maximum of one week after which it will no longer be accepted, e.g. a problem set submitted a day late that would have otherwise received 8/10 will receive 7/10, if that same problem set was submitted two days late then it would receive 6/10.\n\n\nWriting\nPapers and reports should be well-written, well-organized, and easy to follow. They should flow easily from one point to the next. They should have proper sentence structure, spelling, vocabulary, and grammar. Each point should be articulated clearly and completely without being overly verbose. Papers should demonstrate your understanding of the topics you are studying in the course and your confidence in using the terms, techniques and issues you have learned. As always, references must be properly included and cited. If you have concerns about your ability to do any of this then please make use of the writing support provided to the faculty, colleges and the SGS Graduate Centre for Academic Communication.\n\n\nMinimum submission requirement\nIf you are going to not be able to submit at least two term papers, and/or be unable to submit the final paper then it would be unfair on the other students to allow you to pass the course. Please ensure you and your college registrar or faculty/department advisor get in touch with me as early as possible if this may be the case for you.\n\n\nRelationship to PhD Student Learning Outcomes\n\nRead broadly across data science to understand the extent of knowledge\nConduct original research\nWork in an independent way\nCommunicate work and findings in written form\nBe especially aware of the limitations of the data, and methods, they are using."
  },
  {
    "objectID": "teaching-sta304.html",
    "href": "teaching-sta304.html",
    "title": "Surveys, sampling, and observational data",
    "section": "",
    "text": "The best thing about being a statistician, is that you get to play in everyone’s backyard.\nJohn Tukey\n\nSTA304 is an upper-level undergraduate course at the University of Toronto’s Department of Statistical Sciences.\nThe work of applied statisticians, regardless of their specific job title and area of application, is the most important and exciting work in the world right now. The ability to gather data, analyse it, and communicate your understanding of the underlying process is incredibly valuable. In this course you will learn and apply the essentials of this.\nWe focus on surveys, sampling and observational data. The very stuff of statistical science! We will approach these topics from a practical perspective. You will actually run surveys and learn how messy it is to put one together. You will learn how to think about sampling, how to implement it, and why the details matter. You will forecast an election. And you will conduct original research. More generally, you will learn how to obtain and analyse data and use it to make sensible claims about the world.\nTo work as an applied statistician requires you to be able to, as part of a small team:\n\nGather data in less-than-perfect settings.\nEfficiently prepare and clean data toward some purpose.\nAnalyse it in a reproducible, thorough, modern, and statistically-mature manner.\nCommunicate your analysis to stakeholders including colleagues and clients with and without formal statistical training.\n\nYou likely have some of these skills already. This course will further develop them. At the end of the course you will have a portfolio of work focused on surveying, sampling, and observational data, that you could show off to a potential employer.\nEach week you will read relevant papers and books, engage with them through discussion with each other, myself, and the TA. You will bring this all together and show off how much you have learnt through practical, on-going, assessment.\nIt is important to recognise that putting together everything that you have learnt to this point in this way will be difficult. It is not possible to cover everything that you will need to know. You should proactively identify and address aspects where you are weak through seeking additional information and resources. This course acts as a guide as to what is important, it does not contain everything that is important.\nThis course is different to many other courses at the University of Toronto. At the end of this course, you will have a portfolio of work that you could show off to a potential employer. You will have developed the skills to work successfully as an applied statistician or data scientist. And you will know how to fill gaps in your knowledge yourself. A lot of scholarships and jobs these days ask for GitHub and blog links etc to show off a portfolio of your work. This is the class that gives you a chance to develop these. It’s very important to having something to show that needs to go beyond what is done in a normal class.\n\n\n\nIn this course you will work in a self-directed, open-ended manner. Identify relevant areas of interest and then learn the skills that you need to explore those areas.\nTo successfully complete this course, you should expect to spend a large portion of your time reading and writing (both code and text). Deeply engage with the materials. Find a small study group and keep each other motivated and focused. At the start of the week, read the course notes, all compulsory materials and some recommended materials based on your interest. After doing that, but before the ‘lecture’ time you should complete the weekly quiz. During ‘lectures’ I’ll live-code, discuss materials in the course notes, talk about an experiment, and you’ll have a chance to discuss the materials with me.\nYou need to be more active in your learning in this course than others - read the notes and related materials - and then go out there and teach yourself more and apply it. You will not be spoon-fed in this course. Each week try to write reproducible, understandable, R code surrounded by beautifully crafted text that motivates, backgrounds, explains, discusses, and criticizes. Make steady progress toward the assessment.\nThis is not a ‘bird course’. Typically, after the term is finished, students say that the course is difficult but rewarding. The TAs and I are always available to answer any questions. Please come to office hours!\n\n\n\nThis webpage will provide almost all the guiding materials that you need and links to the relevant parts of the notes. The course notes are available here. Those contain notes and other material that you could go over. We’ll use Quercus really only for assessment submission and grading.\nA rough weekly flow for the course would be something like:\n\nRead the week’s course notes.\nRead/watch/listen to the required materials.\nAttend the lecture.\nAttend the lab.\nComplete the weekly quiz.\nMake progress on a paper.\n\n\n\n\nSuccessful past students have the following advice (completely unedited by me):\n\n“Start reading and writing on a weekly basis, watch some videos on R and RMD but more importantly learn how to use Google.”\n“It is not a wise idea to take this course if you did not take any other STA 300 level course before.”\n“Start early, find a group of people you trust enough to divide the work up fairly. Let people work to their strengths (people who know R should do the modelling, good writers should write most of the reports, etc.)”\n“Not to worry if you don’t do well on the first problem set—the nature of the course is to build up skills overtime, and it’s meant to be challenging in the beginning. In the end, it is worth it because you learn very valuable applicable skills on how to write professional reports.”\n“Work on your writing and direction following skills.”\n“Look at the rubric. There were times that I lost marks because I didn’t follow the rubric properly. Go to office hours, they are very useful as you can ask your own question and also get answers to questions other people ask and you didn’t think of. Also, do the assignments to the best of your ability. You will lose marks if you don’t put in effort and the only person you’re hurting is yourself.”\n“During lectures, focus more on the why the prof is doing what he’s doing. When he runs certain commands in R, figure out why that sequence of code gives what you want, because it’ll help adapt his code into your assignment code. just remembering what he’s doing in lecture becomes useless really quickly since the thought process matters more. also, start everything early.”\n“Do this course when you really want to learn something and have a lot of time to working on it.”\n“you need to be very skillful in RStudio and latex. Otherwise you would be struggling.”\n“Try to incorporate the feedback given and read a looottttttttt. Also start early on the problem sets because they tend to take a lot of time. Don’t give up!”\n“-Find a good group for problem sets”\n“If the assignments stay the same, I would tell students to approach this class from the perspective of ‘storytelling with statistics’ rather than a statistics course. You need to use R, and Markdown, and have a solid understanding of concepts like regression and sampling, but more importantly you need to be able to interpret results and write about them in a way coherent and professional way.”\n“do your readings”\n“Definitely get ready to write reports”\n“Do not take sta304 with Prof Rohan, it is pretty tough”\n“Start your work a bit earlier, make sure to follow the format expected and the rubric exactly.”\n“Read course material. Figure out WHY this paper/video is being shown to you and what you generally learn from it. Surround yourself with people dedicated to putting in the effort to understand material and who are thorough in their work so you can discuss content and/or work together.”\n“1. Be prepared to work extremely hard (8-11 hours a week). 2. Learn RStudio before course begins–STA130 is ideal preparation. 3. Start problem sets as soon as they are released.”\n“learn to code early and extensively use the office hours with the prof.”\n“This course requires lots of time dedicated and is not an”easy bird course” but is an incredibly rewarding course if one wants to learn how statistics is applied in the real world.”\n\n\n\n\nThank you to the following people for generously providing comments, references, suggestions, and thoughts that directly contributed to this outline: Bethany White, Dan Simpson, Jesse Gronsbell, Kelly Lyons, Lauren Kennedy, Monica Alexander and Uzair Mirza. Thank you especially to Samantha-Jo Caetano who influenced all aspects of this and co-taught the first version in Fall 2020.\n\n\n\n\n2020\n\nSyllabus\nStudent evals\n\n2022\n\nSyllabus\nStudent evals"
  },
  {
    "objectID": "teaching-sta304.html#preamble",
    "href": "teaching-sta304.html#preamble",
    "title": "Surveys, sampling, and observational data",
    "section": "",
    "text": "The best thing about being a statistician, is that you get to play in everyone’s backyard.\nJohn Tukey\n\nSTA304 is an upper-level undergraduate course at the University of Toronto’s Department of Statistical Sciences.\nThe work of applied statisticians, regardless of their specific job title and area of application, is the most important and exciting work in the world right now. The ability to gather data, analyse it, and communicate your understanding of the underlying process is incredibly valuable. In this course you will learn and apply the essentials of this.\nWe focus on surveys, sampling and observational data. The very stuff of statistical science! We will approach these topics from a practical perspective. You will actually run surveys and learn how messy it is to put one together. You will learn how to think about sampling, how to implement it, and why the details matter. You will forecast an election. And you will conduct original research. More generally, you will learn how to obtain and analyse data and use it to make sensible claims about the world.\nTo work as an applied statistician requires you to be able to, as part of a small team:\n\nGather data in less-than-perfect settings.\nEfficiently prepare and clean data toward some purpose.\nAnalyse it in a reproducible, thorough, modern, and statistically-mature manner.\nCommunicate your analysis to stakeholders including colleagues and clients with and without formal statistical training.\n\nYou likely have some of these skills already. This course will further develop them. At the end of the course you will have a portfolio of work focused on surveying, sampling, and observational data, that you could show off to a potential employer.\nEach week you will read relevant papers and books, engage with them through discussion with each other, myself, and the TA. You will bring this all together and show off how much you have learnt through practical, on-going, assessment.\nIt is important to recognise that putting together everything that you have learnt to this point in this way will be difficult. It is not possible to cover everything that you will need to know. You should proactively identify and address aspects where you are weak through seeking additional information and resources. This course acts as a guide as to what is important, it does not contain everything that is important.\nThis course is different to many other courses at the University of Toronto. At the end of this course, you will have a portfolio of work that you could show off to a potential employer. You will have developed the skills to work successfully as an applied statistician or data scientist. And you will know how to fill gaps in your knowledge yourself. A lot of scholarships and jobs these days ask for GitHub and blog links etc to show off a portfolio of your work. This is the class that gives you a chance to develop these. It’s very important to having something to show that needs to go beyond what is done in a normal class.\n\n\n\nIn this course you will work in a self-directed, open-ended manner. Identify relevant areas of interest and then learn the skills that you need to explore those areas.\nTo successfully complete this course, you should expect to spend a large portion of your time reading and writing (both code and text). Deeply engage with the materials. Find a small study group and keep each other motivated and focused. At the start of the week, read the course notes, all compulsory materials and some recommended materials based on your interest. After doing that, but before the ‘lecture’ time you should complete the weekly quiz. During ‘lectures’ I’ll live-code, discuss materials in the course notes, talk about an experiment, and you’ll have a chance to discuss the materials with me.\nYou need to be more active in your learning in this course than others - read the notes and related materials - and then go out there and teach yourself more and apply it. You will not be spoon-fed in this course. Each week try to write reproducible, understandable, R code surrounded by beautifully crafted text that motivates, backgrounds, explains, discusses, and criticizes. Make steady progress toward the assessment.\nThis is not a ‘bird course’. Typically, after the term is finished, students say that the course is difficult but rewarding. The TAs and I are always available to answer any questions. Please come to office hours!\n\n\n\nThis webpage will provide almost all the guiding materials that you need and links to the relevant parts of the notes. The course notes are available here. Those contain notes and other material that you could go over. We’ll use Quercus really only for assessment submission and grading.\nA rough weekly flow for the course would be something like:\n\nRead the week’s course notes.\nRead/watch/listen to the required materials.\nAttend the lecture.\nAttend the lab.\nComplete the weekly quiz.\nMake progress on a paper.\n\n\n\n\nSuccessful past students have the following advice (completely unedited by me):\n\n“Start reading and writing on a weekly basis, watch some videos on R and RMD but more importantly learn how to use Google.”\n“It is not a wise idea to take this course if you did not take any other STA 300 level course before.”\n“Start early, find a group of people you trust enough to divide the work up fairly. Let people work to their strengths (people who know R should do the modelling, good writers should write most of the reports, etc.)”\n“Not to worry if you don’t do well on the first problem set—the nature of the course is to build up skills overtime, and it’s meant to be challenging in the beginning. In the end, it is worth it because you learn very valuable applicable skills on how to write professional reports.”\n“Work on your writing and direction following skills.”\n“Look at the rubric. There were times that I lost marks because I didn’t follow the rubric properly. Go to office hours, they are very useful as you can ask your own question and also get answers to questions other people ask and you didn’t think of. Also, do the assignments to the best of your ability. You will lose marks if you don’t put in effort and the only person you’re hurting is yourself.”\n“During lectures, focus more on the why the prof is doing what he’s doing. When he runs certain commands in R, figure out why that sequence of code gives what you want, because it’ll help adapt his code into your assignment code. just remembering what he’s doing in lecture becomes useless really quickly since the thought process matters more. also, start everything early.”\n“Do this course when you really want to learn something and have a lot of time to working on it.”\n“you need to be very skillful in RStudio and latex. Otherwise you would be struggling.”\n“Try to incorporate the feedback given and read a looottttttttt. Also start early on the problem sets because they tend to take a lot of time. Don’t give up!”\n“-Find a good group for problem sets”\n“If the assignments stay the same, I would tell students to approach this class from the perspective of ‘storytelling with statistics’ rather than a statistics course. You need to use R, and Markdown, and have a solid understanding of concepts like regression and sampling, but more importantly you need to be able to interpret results and write about them in a way coherent and professional way.”\n“do your readings”\n“Definitely get ready to write reports”\n“Do not take sta304 with Prof Rohan, it is pretty tough”\n“Start your work a bit earlier, make sure to follow the format expected and the rubric exactly.”\n“Read course material. Figure out WHY this paper/video is being shown to you and what you generally learn from it. Surround yourself with people dedicated to putting in the effort to understand material and who are thorough in their work so you can discuss content and/or work together.”\n“1. Be prepared to work extremely hard (8-11 hours a week). 2. Learn RStudio before course begins–STA130 is ideal preparation. 3. Start problem sets as soon as they are released.”\n“learn to code early and extensively use the office hours with the prof.”\n“This course requires lots of time dedicated and is not an”easy bird course” but is an incredibly rewarding course if one wants to learn how statistics is applied in the real world.”\n\n\n\n\nThank you to the following people for generously providing comments, references, suggestions, and thoughts that directly contributed to this outline: Bethany White, Dan Simpson, Jesse Gronsbell, Kelly Lyons, Lauren Kennedy, Monica Alexander and Uzair Mirza. Thank you especially to Samantha-Jo Caetano who influenced all aspects of this and co-taught the first version in Fall 2020.\n\n\n\n\n2020\n\nSyllabus\nStudent evals\n\n2022\n\nSyllabus\nStudent evals"
  },
  {
    "objectID": "teaching-sta304.html#content",
    "href": "teaching-sta304.html#content",
    "title": "Surveys, sampling, and observational data",
    "section": "Content",
    "text": "Content\n(Exact coverage will change based on how the class progresses.)\n\nWeek 1\n\nContent:\n\nIntroduction\nSeveral end-to-end worked examples\n\n\n\n\nWeek 2\n\nContent:\n\nR essentials\nReproducible workflows\n\n\n\n\nWeek 3\n\nContent:\n\nWriting\nStatic communication\n\n(Optional) R bootcamp (please allow six hours)\n\n\n\nWeek 4\n\nContent:\n\nInteractive communication\n\n(Optional) Git and GitHub bootcamp (please allow six hours)\n\n\n\nWeek 5\n\nContent:\n\nFarm data\nGather data\n\n\n\n\nWeek 6\n\nContent:\n\nHunt data\n\n\n\n\n\n\nWeek 7\n\nContent:\n\nClean and prepare data\nStore and share data\n\nGuest lecture: Chris Henry, Bank of Canada\n\nChristopher Henry (Chris) is a Senior Economist at the Bank of Canada. He serves as lead economist for the consumer survey research program on the Currency Department’s Economic Research and Analysis team. Chris first joined the Bank as a Research Assistant in 2012, and recently rejoined in 2021 after completing his PhD in Economics. In his role, Chris contributes to the design, implementation, and analysis of a range of surveys that measure the use of cash and alternative methods of payment. He holds an PhD in Economics from Université Clermont Auvergne (France), and an MSc in Mathematics from McMaster University.\n\n\n\n\nWeek 8\n\nContent:\n\nEDA\n\n\n\n\nWeek 9\n\nContent:\n\nLinear Models\n\n\n\n\nWeek 10\n\nContent:\n\nCausality\n\n\n\n\nWeek 11\n\nContent:\n\nMRP\n\nGuest lecture: Sue Ince and Alex Ince-Cushman\n\nAlex Ince-Cushman is Chief Executive Officer of Branch Energy. He holds a Ph.D. in nuclear fusion from MIT and an undergraduate degree from the University of Toronto.\nSue Ince has many years experience in research and forecasting in North America and the U.K., with a statistics background and very strong quantitative and research design skills. Sue was the principal and co-founder of Epic Consulting and directed large-scale projects for major corporations and small and medium enterprises. Sue specialized in segmentation, multivariate analysis and data mining integrating primary and secondary research with client’s internal sales and marketing data. Her career spans from the age of slide rules, log tables and graph paper to digital computing. In every age and whatever the analytical tools available it is critical to be able to communicate the meaning of data to an audience who often have poor statistical literacy. Good data storytelling is a very important skill for statisticians. Sue holds an B.Sc. Social Sciences; Combined Honours: Economics & Politics, Statistics, University of Southampton, England and was active professional member of Canada’s Market Research community before her retirement. Sue presented papers at research conferences in Canada and in Europe, lectured on Quantitative Methods in the Federated Press Market Courses and guest lectured at George Brown and Seneca Colleges on market research.\n\n\n\n\nWeek 12\n\nContent:\n\nTBD based on class progress and interest."
  },
  {
    "objectID": "teaching-sta304.html#assessment",
    "href": "teaching-sta304.html#assessment",
    "title": "Surveys, sampling, and observational data",
    "section": "Assessment",
    "text": "Assessment\n\nSummary\n\n\n\n\n\n\n\n\nItem\nWeight (%)\nDue date\n\n\n\n\nQuiz\n20\nWeekly before the lecture\n\n\nTutorial\n20\nWeekly the day before the tutorial\n\n\nPaper 1\n25\nEnd of Week 3\n\n\nPaper 2\n25\nEnd of Week 6\n\n\nPaper 3\n25\nEnd of Week 8\n\n\nPaper 4\n25\nEnd of Week 10\n\n\nFinal Paper (initial submission)\n1\nMiddle of Week 12\n\n\nFinal Paper (peer review)\n4\nEnd of Week 12\n\n\nFinal Paper\n25\nTwo weeks after that\n\n\n\nYou must submit Paper 1. And you must submit the Final Paper.\nBeyond that, you have scope to pick an assessment schedule that works for you. We will take your best 3 of the 11 tutorials, or your best 8 of 11 quizzes for that 20 per cent—whichever results in a better grade for you (i.e. you can choose to do either quizzes or tutorials). And we will take your two best papers from Papers 1-4 for that 50 per cent (25 per cent for each). The remainder is made up of 1 per cent for submitting a draft of the Final Paper, 4 per cent for peer reviewing other people’s drafts of the Final Paper, and 25 per cent for the Final Paper.\nAdditional details:\n\nQuiz questions are drawn from those in the Quiz section that follows each chapter of Telling Stories with Data. Almost all of them are multiple choice, and you should expect to know the mark within two days of submission.\nTutorial questions are drawn from those in the Tutorial section that follows each chapter of Telling Stories with Data. The general expectation (although this differs from week to week) is about two pages of written content, which the tutor will read, discuss with you, and then provide a mark. You should expect to know the mark within three days of the tutorial.\nIn general papers require a considerable amount of work, and are due after the material has been covered in quizzes and tutorials (i.e. you would draw on knowledge tested in the quizzes, and potentially material could be re-used from the tutorial material). In general, they require original work to some extent. Papers are taken from the Papers appendix of Telling Stories with Data and students have access to the grading rubrics before submission.\n\n\n\nQuiz\n\nYou should choose to do either tutorials or quizzes.\nDue date: Weekly before the lecture.\nWeight: 20 per cent. Only best eight out of eleven count and only if that is better for you than counting tutorials.\nTask: Please complete a weekly quiz in Quercus.\n\n\n\nTutorial\n\nYou should choose to do either tutorials or quizzes.\nDue date: Weekly the day before the tutorial.\nWeight: 20 per cent. Only best three out of eleven count and only if that is better for you than counting quizzes.\nTask: Please complete a tutorial question and submit it via Quercus.\nRubric:\n\n0 - Any typos, major grammatical errors, other table stakes issues for this level.\n0.25 - Grammatical errors, if relevant: tables/graphs not properly labeled, no references, other aspects that affect credibility. Too short.\n0.6 - Makes some interesting and relevant points, related to course material (including required materials), but lacking in terms of structure and story/argument.\n0.80 - Interesting paper that is well-structured, coherent, and credible.\n1 - As with 0.80, but exceptional in some way.\n\n\n\n\nPaper #1\n\nYou must submit this paper.\nTask: ‘Mandatory Minimums’ (details will be added to Quercus).\nDue date: End of Week 3.\nWeight: 25 per cent (for Papers #1-#4 the best two of four count).\n\n\n\nPaper #2\n\nDue date: End of Week 6.\nTask: ‘The Short List’ (details will be added to Quercus).\nWeight: 25 per cent (for Papers #1-#4 the best two of four counts).\n\n\n\nPaper #3\n\nDue date: End of Week 8.\nTask: TBA (details will be added to Quercus).\nWeight: 25 per cent (for Papers #1-#4 the best two of four counts).\n\n\n\nPaper #4\n\nDue date: End of Week 10.\nTask: TBA (details will be added to Quercus).\nWeight: 25 per cent (for Papers #1-#4 the best two of four counts).\n\n\n\nFinal Paper\n\nTask: TBA (details will be added to Quercus).\nYou must submit this paper.\nDue dates:\n\nInitial submission: Middle of Week 12.\nPeer review: End of Week 12.\nFinal Paper: Two weeks after that.\n\nWeight: 30 per cent\n\nInitial submission: 1 per cent\nPeer review: 4 per cent\nFinal Paper: 25 per cent"
  },
  {
    "objectID": "teaching-sta304.html#other",
    "href": "teaching-sta304.html#other",
    "title": "Surveys, sampling, and observational data",
    "section": "Other",
    "text": "Other\n\nLearning objectives\n\nDesign a survey or sample that appropriately gathers information of interest.\nCarry out a variety of statistical analyses in R to make inference on the data collected from a survey/sample.\nIdentify and implement different sampling techniques and different study designs and the trade-offs involved in each.\nIdentify sources of bias within a study and comment on a study’s design, including weaknesses, strengths, and appropriate analyses.\nClearly communicate results of statistical analyses to technical and non-technical audiences.\n\n\n\nCommunication\nIf you have a question, there is a decent chance that others have the same question or, at least, will benefit from the answer. Please post all questions to Piazza so that everyone in the course can benefit from your questions and our answers. You are encouraged to post answers to the questions of other students, where appropriate. Of course, if you have a concern of a personal nature then please email the TAs or me and you should begin your subject line with the course code ‘STA304’, and then an appropriate subject.\nEmails and the message board are not checked or responded to by either the TA or me after hours or on the weekend.\nPlease be polite. We continue to be in a pandemic.\n\n\nAccommodations with regard to assessment\nYou do not need to reveal your personal or medical information to me. I understand that illness or personal emergencies can happen from time to time. The following accommodations to assessment requirements exist to provide for those situations.\nStraight-forward (will automatically apply to all students - there’s no need to ask for these):\n\nQuiz: Worst three quizzes are dropped.\nTutorial: Worst eight tutorials are dropped.\nPapers #1-#4: Worst two are dropped.\n\nSo for those (with the exception of Paper #1), if you have a situation, then just don’t submit.\nSlightly more involved:\n\nPaper #1: I’m open to a day without penalty to account for situations. Beyond that it begins to slow down the class. You must submit something for Paper #1.\nPeer review: No accommodation or late submission is possible for this because it would hold up the rest of the class. If you cannot submit then email me before the deadline and the weight will be shifted to the final paper.\nFinal paper: The final paper is a critical piece of assessment. It’s also up against deadlines for submission of grades. Extensions for valid reasons may be granted for a maximum of three days, however this isn’t possible for all students (i.e. there may be restrictions around graduating students). Hence, the exact extension needs to be at my discretion. To be considered, an extension request must be sent to rohan.alexander@utoronto.ca by the business day before the due date so there is time to get advice from the Department and your college about your particular circumstance.\n\n\n\nMinimum submission requirement\nIf you are going to not be able to submit at least two problem sets, and/or be unable to submit the final paper then it would be unfair on the other students to allow you to pass the course. Please ensure you and your registrar get in touch with me as early as possible if this may be the case for you.\n\n\nRe-grading\nRequests to have your work re-graded will not be accepted within 24 hours of the release of grades. This is to give you a chance to reflect. Similarly, requests to have your work re-graded more than seven days after the release of the grades will not be accepted. This is to ensure the course runs smoothly.\nInside that 1-7 day period if you would like to request a re-grade, please email rohan.alexander@utoronto.ca with a subject line that starts with ‘STA304’. You must specify where the marking error was made in relation to the marking guide. Your entire assessment will be re-marked and it is possible that your grade could reduce.\n\n\nPlagiarism and integrity\nPlease do not plagiarize. In particular, be careful to acknowledge the source of code - if it’s extensive then through proper citation and if it’s just a couple of lines from Stack Overflow then in a comment immediately next to the code.\nYou are responsible for knowing the content of the University of Toronto’s Code of Behaviour on Academic Matters.\nAcademic offenses include (but are not limited to) plagiarism, cheating, copying R code, communication/extra resources during closed book assessments, purchasing labour for assessments (of any kind). Academic offenses will be taken seriously and dealt with accordingly. If you have any questions about what is or is not permitted in this course, please contact me.\nPlease consult the University’s site on Academic Integrity http://academicintegrity.utoronto.ca/. Please also see the definition of plagiarism in section B.I.1.(d) of the University’s Code of Behaviour on Academic Matters http://www.governingcouncil.utoronto.ca/Assets/Governing+Council+Digital+Assets/Policies/PDF/ppjun011995.pdf. Please read the Code. Please review Cite it Right and if you require further clarification, consult the site How Not to Plagiarize http://advice.writing.utoronto.ca/wp-content/uploads/sites/2/how-not-to-plagiarize.pdf.\n\n\nLate policy\nYou are expected to manage your time effectively. If no extension has been granted and no accommodation applies, then the late submission of an assessment item carries a penalty of 10 percentage points per day to a maximum of one week after which it will no longer be accepted, e.g. a problem set submitted a day late that would have otherwise received 8/10 will receive 7/10, if that same problem set was submitted two days late then it would receive 6/10.\n\n\nWriting\nPapers and reports should be well-written, well-organized, and easy to follow. They should flow easily from one point to the next. They should have proper sentence structure, spelling, vocabulary, and grammar. Each point should be articulated clearly and completely without being overly verbose. Papers should demonstrate your understanding of the topics you are studying in the course and your confidence in using the terms, techniques, and issues you have learned. As always, references must be properly included and cited. If you have concerns about your ability to do any of this then please make use of the writing support provided to the faculty, colleges and the SGS Graduate Centre for Academic Communication.\n\n\nAccessibility needs\nStudents with diverse learning styles and needs are welcome in this course. In particular, if you have a disability/health consideration that may require accommodations, please feel free to approach me and/or Accessibility Services at 416 978 8060 or visit studentlife.utoronto.ca/as.\n\n\nIntellectual Property Statement\nCourse material that has been created by your instructor is the intellectual property of your instructor and is made available to you for your personal use in this course. Sharing, posting, selling, or using this material outside of your personal use in this course is not permitted under any circumstances and is considered an infringement of intellectual property rights."
  }
]